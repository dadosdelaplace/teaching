<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>TÉCNICAS DE ANÁLISIS MULTIVARIANTE</title>
    <meta charset="utf-8" />
    <meta name="author" content="Profesor: Javier Álvarez Liébana" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Click para copiar código","success":"Código copiado","error":"Ctrl+C para copiar"})</script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <link href="libs/animate.css-xaringan/animate.fade.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# TÉCNICAS DE ANÁLISIS MULTIVARIANTE
## Máster propio (NTIC) en «Big Data y Business Analytics»
### Profesor: Javier Álvarez Liébana
### Facultad de Estudios Estadísticos (UCM)
### 22/04/2022 - 23/04/2022 (actualizado: 2022-04-20)

---





class: inverse center middle

# ATAJOS DE LAS DIAPOSITIVAS



`$$\\[2.7in]$$`

.left[Pulsa &lt;kbd-black&gt;O&lt;/kbd-black&gt; para ver el panel de diapositivas]
.left[Pulsa &lt;kbd-black&gt;H&lt;/kbd-black&gt; para ver otros atajos]

---


# Material de las clases


.pull-left[

- **Diapositivas** del curso:
&lt;https://dadosdelaplace.github.io/teaching/pca-clustering&gt;

- **Scripts** del curso:
&lt;https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/scripts&gt;

- **Evaluaciones**:
&lt;https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/eval&gt;

- **Bibliografía**: &lt;https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/biblio&gt;

- **Manual** de R: &lt;https://dadosdelaplace.github.io/courses-intro-R/&gt;

- **Curso de dataviz** en R: &lt;https://dadosdelaplace.github.io/curso-dataviz-ECI-2022&gt;


]

.pull-right[

&lt;img src="./img/portada_master.jpg" width="83%" style="display: block; margin: auto 0 auto auto;" /&gt;

]

---

# Me presento: la turra

.pull-left[

&lt;img src="./img/me.jpeg" width="80%" style="display: block; margin: auto auto auto 0;" /&gt;

]

.pull-right[

* **Javier Álvarez Liébana**, nacido en 1989 en Carabanchel Bajo (Madrid)

* Licenciado (UCM) en **Matemáticas** (Erasmus en Bologna mediante). **Máster (UCM) en Ingeniería Matemática** (2013-2014)

&amp;nbsp;

* **Doctorado en estadística** por la Universidad de Granada


* Encargado de la **visualización y análisis de datos covid** de la Consejería de Salud del **Principado de Asturias**

]

&amp;nbsp;

Intentando la **divulgación** por `Twitter (@dadosdelaplace)` e `Instagram (@javieralvarezliebana)`. Tengo una newsletter: &lt;https://cartasdelaplace.substack.com/&gt;

---

# Objetivos


.pull-left[

El propósito de estas clases será el tratamiento de **datos multidimensionales**, con tres objetivos principales:

- **Reducción de la dimensión**: ¿todas las variables aportan información? ¿Todas son necesarias? ¿Podemos transformar las variables para mantener la información de los datos pero reducir la dimensionalidad de los mismos?

- **Visualización**: ¿cuántas dimensiones podemos incluir en un gráfico 2D? ¿Cómo visualizar datos multidimensionales?

- **Encontrar patrones**: ¿cómo agrupar (clusterizar) los elementos en función de sus diferencias y similitudes?

]

.pull-right[

&lt;img src="./img/portada_master.jpg" width="120%" style="display: block; margin: auto auto auto 0;" /&gt;

]

&amp;nbsp;

📚 Estas **diapositivas** han sido elaboradas con el propio `R` haciendo uso del paquete `{xaringan}`
y `{xaringanExtra}`.


---


# Requisitos

Para el presente curso los únicos **requisitos** serán:

1. **Conexión a internet** (para la descarga de algunos datos y paquetes).

2. **Instalar R**: será nuestro **lenguaje**, nuestro **castellano** para poder «comunicarnos con el ordenador. La descarga la haremos (gratuitamente) desde &lt;https://cran.r-project.org/&gt;

3. **Instalar R Studio**. De la misma manera que podemos escribir castellano en un ordenador, en un Word, en un papel o en un tuit, podemos usar distintos IDE (entornos de desarrollo integrados, nuestro Office), para que el trabajo sea más cómodo. Nuestro **Word** para nosotros será **RStudio**.

.left[
  &lt;img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/cran-R.jpg" alt = "cran-R" align = "left" width = "500" style = "margin-top: 5vh"&gt;
]

.right[
  &lt;img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/R-studio.jpg" alt = "RStudio" align = "right" width = "500" style = "margin-top: 5vh;"&gt;
]


---

class: inverse center middle

# POR SI ACASO...¿POR QUÉ R Y NO EXCEL?


---

# R vs excel

![](./img/meme_barco.jpg)

---

# Incel vs excel

&lt;img src="./img/incel.jpg" width="85%" style="display: block; margin: auto;" /&gt;

---

# Datos: de la celda a la tabla

&lt;img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/celdas.jpg" alt = "celdas" align = "center" width = "850" style = "margin-top: 1vh;"&gt;


* **Celda**: un **dato individual** de un tipo concreto.
* **Variable**: **concatenación de datos** del mismo tipo.
* **Matriz**: **concatenación de variables** del **mismo tipo** y longitud.
* **Tabla**: **concatenación de variables** de **distinto tipo** pero igual longitud.

---

class: inverse center middle

# BLOQUE I. Selección de variables: PCA

&amp;nbsp;


### [¿Por qué es un paso importante en el análisis de datos multidimensional?](#intro-PCA)

### [Teoría: análisis de componentes principales](#teoria-PCA)

### [Práctica: PCA en R (visualización)](#practica-PCA)

---

name: intro-PCA
class: center, middle

# ¿Por qué es un paso importante en el análisis de datos multidimensional?

### **¿Qué es el análisis multivariante?**

---

# Breve historia de la estadística

.pull-left[

## Origen

* Del (neo)latín «statisticum collegium»: consejo de **Estado**.
* Del alemán «statistik» (ciencia del **Estado**, intoducido por G. Achenwall).


## Primeros usos: elaboración de censos

Los **primeros usos** documentados de la estadística fueron la elaboración de **censos** por parte de **mesopotámicos, chinos y egipcios**, con tres fines:

* Cobrar **impuestos** (un saludo, Willyrex).
* Reparto de **tierras** y optimización de su uso.
* **Reclutamiento de soldados**.

]

.pull-right[

## Estadística en la guerra

Según Tucídides, conceptos estadísticos como la **moda** datan del **siglo V a.C.**: para asaltar la muralla de la ciudad de Platea, ponían a contar a varios soldados el número de ladrillos vistos en la muralla, quedándose con el **conteo más repetido (la moda, el más frecuente)**, permitiendo el cálculo de la altura de la muralla.

&lt;img src="./img/peloponeso.jpg" width="70%" style="display: block; margin: auto;" /&gt;

]

---

# ¿Qué han hecho los romanos por nosotros?

.pull-left[

Precisamente por el tamaño de su Imperio, fueron los **romanos** quienes hicieron un uso más intenso de la estadística:

* **Censos** (elaborados por la censura, que elaboraba no solo el censo sino la supervisión de la moralidad pública).
* Primeras **tablas de natalidad/mortalidad**
* Primeros **catastros** (registros oficiales de propiedades, primeros impuestos)

&lt;img src="./img/catastro.jpg" width="60%" style="display: block; margin: auto;" /&gt;

]

.pull-right[

&lt;img src="https://www.publico.es/tremending/wp-content/uploads/2019/02/lifeofbrian3.jpg" width="95%" style="display: block; margin: auto auto auto 0;" /&gt;

]

---

# Breve historia de la estadística

.pull-left[

## **ÁRABES**

Autores de los **primeros tratados de estadística**, como el manuscrito de **Al-Kindi (801-873)**, que usó la distribución de **frecuencias de palabras** para el desarrollo de métodos de cifrado y descifrado de **mensajes encriptados**.

]

.pull-right[

## **MÉXICO**

Ya en el **año 1116, el rey Xólotl** implementó un **censo** que consistía en la **estimación de piedras**, tirando cada súbdito una a un montón (Nepohualco).
]


&amp;nbsp;

.pull-left[

## **INGLATERRA**

Desde el siglo XII se realiza la **Prueba del Pyx**, considerado uno de los **primeros controles de calidad**: se extre una de las monedas acuñadas y se deposita en una caja, para un año después comprobar su calidad y pureza.

]

.pull-right[

## **ITALIA**

En paralelo al **auge de los primeros «sistemas financieros» en Italia**, «La Nuova Crónica» de G. Villani fue considerado durante mucho tiempo el primer tratado de estadística (hasta el descubrimiento de los trabajos de Al-Kindi).

]

---

# Navegación y astronomía

Y es de aquella época medieval, en la que la navegación y la astronomía empezaban a tomar relevancia científica, cuando aparece la que se considera la primera gráfica (aunque no propiamente estadística) &lt;sup&gt;1&lt;/sup&gt;, representando el **movimiento cíclico de los planetas** (entre los siglos X y XI)

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/dataviz_historico_1.png" alt="Gráfica extraída de Beniger y Robyn (1978)" width="60%" /&gt;
&lt;p class="caption"&gt;Gráfica extraída de Beniger y Robyn (1978)&lt;/p&gt;
&lt;/div&gt;

[1] [📚 «Quantitative Graphics in Statistics: A Brief History» de James R. Beniger y Dorothy L. Robyn. The American Statistician (1978)](https://www.jstor.org/stable/2683467)

 
---

# Navegación y astronomía

Con una motivación similar, en torno a 1360 el matemático **Nicole Oresme** diseñó el **primer gráfico de barras**&lt;sup&gt;1&lt;/sup&gt; (no estadístico), con la idea de **visualizar a la vez dos magnitudes físicas teóricas** (pero...aún sin representar datos).


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/dataviz_historico_2.jpeg" alt="Gráfica extraída de Friendly y Valero-Mora (2010), de «Tractatus De Latitudinibus Formarum»" width="30%" /&gt;
&lt;p class="caption"&gt;Gráfica extraída de Friendly y Valero-Mora (2010), de «Tractatus De Latitudinibus Formarum»&lt;/p&gt;
&lt;/div&gt;

[1] [📚 «The First (Known) Statistical Graph: Michael Florent van Langren and the 'Secret' of Longitude» de M. Friendly y P. M. Valero-Mora. The American Statistician (2010)](https://www.researchgate.net/publication/227369016_The_First_Known_Statistical_Graph_Michael_Florent_van_Langren_and_the_Secret_of_Longitude)

 
---

# Primer gráfico estadístico

La mayoría de expertos, como Tufte &lt;sup&gt;1,2&lt;/sup&gt;, consideran **este gráfico** casi longitudinal como la **primera visualización de datos** de la historia, hecha por **van Langren** en 1644, representando la **distancia (en longitud) entre Toledo y Roma** (un poco mal medida ya que la distancia real es de 16.5º).

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/longitud_dataviz.jpg" alt="Gráfica original extraída de Friendly y Valero-Mora (2010)" width="45%" /&gt;
&lt;p class="caption"&gt;Gráfica original extraída de Friendly y Valero-Mora (2010)&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/dataviz_historico_3.jpeg" alt="Adaptación extraída de Friendly y Valero-Mora (2010)" width="45%" /&gt;
&lt;p class="caption"&gt;Adaptación extraída de Friendly y Valero-Mora (2010)&lt;/p&gt;
&lt;/div&gt;

[1] [📚 «Visual explanations: images and quantities, evidence and narrative» de E. Tufte](https://archive.org/details/visualexplanatio00tuft)

[2] [📚 «PowerPoint is evil» de E. Tufte](https://www.wired.com/2003/09/ppt2/)

---


# Navegación y astronomía

.pull-left[

### T. Brahe

Uno de los primeros usos «modernos» de la estadística fue en la **navegación y la astronomía**, siendo Tycho Brahe de los primeros en utilizar la estadística para **reducir los errores** observacionales.
]

.pull-right[

### E. Wright

Fue el primero en usar en 1599 lo que hoy llamamos **mediana** en su libro «Certaine errors in navigation», aplicada a la navegación.

]

.pull-left[

### G. Galileo

Aunque la fama se la llevó **Gauss**, fue el primero en plantear una idea similar a la que hoy llamamos **método de mínimos cuadrados**: los valores más probables serían aquellos que minimizaran los errores.

]

.pull-right[
### C. F. Gauss y A. M. Legendre

El **método de los mínimos cuadrados**, en el que basan modelos actuales como la regresión, fue desarrollado por **Legendre y Gauss** (el último lo aplicó a la detección más probable del planeta enano Ceres).

]

---

# Demografía, epidemiología y fisiología

.pull-left[

### J. Graunt

Autor de «Natural and Political Observations Made upon the Bills of Mortality» (1662), uno de los primeros trabajos en los que ya se hablaba de **exceso de mortalidad** a partir de las primeras tablas de natalidad y mortalidad, **estimando la población de Londres**.
]

.pull-right[

### G. Neumann

Las **fakes news** ya existían en el siglo XVII: Gaspar Neumann también un precursor en el **análisis estadístico de tablas de mortalidad**, para desmentir bulos (ejemplo: desmontó la creencia de que en los años acabados en siete morían más personas).
]

&amp;nbsp;

Son precisamente las tablas de Graunt las que usó **Christiaan Huygens** (pionero en teoría de probabilidad con su «De ratiociniis in ludo aleae» en 1656) para generar la **primera gráfica de densidad** de una distribución continua, visualizando la **esperanza de vida** (en función de la edad).


---

# Primer gráfico de densidad


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://omeka.lehigh.edu/files/fullsize/65fc32c11a768f1d3263a99caca28dff.jpg" alt="Primera función de densidad, extraída de https://omeka.lehigh.edu/exhibits/show/data_visualization/vital_statistics/huygen" width="50%" /&gt;
&lt;p class="caption"&gt;Primera función de densidad, extraída de https://omeka.lehigh.edu/exhibits/show/data_visualization/vital_statistics/huygen&lt;/p&gt;
&lt;/div&gt;

---

# El gran boom: los gráficos de Playfair

La figura que cambió el dataviz fue, sin lugar a dudas, el economista y político **William Playfair (1759-1823)**. En 1786 publicó el **«Atlas político y comercial»**&lt;sup&gt;1,2&lt;/sup&gt; con 44 gráficas (43 series temporales y el **diagrama de barras más famoso**, aunque no el primero).

.pull-left[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/playfair_1.jpg" alt="Gráficas de Playfair, extraídas de Funkhouser y Walker (1935)" width="85%" /&gt;
&lt;p class="caption"&gt;Gráficas de Playfair, extraídas de Funkhouser y Walker (1935)&lt;/p&gt;
&lt;/div&gt;

]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/playfair_2.jpg" alt="Gráficas de Playfair, extraídas de Funkhouser y Walker (1935)" width="35%" /&gt;
&lt;p class="caption"&gt;Gráficas de Playfair, extraídas de Funkhouser y Walker (1935)&lt;/p&gt;
&lt;/div&gt;

]

[1] [📚 «Atlas político y comercial» de William Playfair (1786)](https://www.amazon.es/Playfairs-Commercial-Political-Statistical-Breviary/dp/0521855543)

[2] [📚 «Playfair and his charts» de H. Gray Funkhouser and  Helen M. Walker (1935)](https://www.jstor.org/stable/45366440)

---

# Primer gráfico de barras

Playfair es además el **autor del gráfico de barras más famoso** (aunque no fue el primero, pero sí el que sentó un precedente, quien lo hizo _mainstream_).

.pull-left[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/playfair_5.jpg" alt="Gráficas de Playfair de importaciones (barras grises) y exportaciones (negras) de Escocia en 1781, extraídas de la wikipedia." width="95%" /&gt;
&lt;p class="caption"&gt;Gráficas de Playfair de importaciones (barras grises) y exportaciones (negras) de Escocia en 1781, extraídas de la wikipedia.&lt;/p&gt;
&lt;/div&gt;

]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/playfair_6.jpg" alt="Primer diagrama de barras (Philippe Buache y Guillaume de L’Isle), visualizando los niveles del Sena desde 1732 hasta 1766, extraída de https://friendly.github.io/HistDataVis" width="95%" /&gt;
&lt;p class="caption"&gt;Primer diagrama de barras (Philippe Buache y Guillaume de L’Isle), visualizando los niveles del Sena desde 1732 hasta 1766, extraída de https://friendly.github.io/HistDataVis&lt;/p&gt;
&lt;/div&gt;

]


---


# Epidemiología y bioestadística

.pull-left[

### F. Galton

Primo de Charles Darwin, inventor de los **silbatos para perretes**, de los mapas de predicción meteorológica y la persona que acuñó el concepto de **regresión** (y el de eugenesia :/).

&lt;img src="https://www.bogleheads.org/w/images/thumb/9/95/Screen_Shot_2012-01-03_at_7.36.29_AM.png/600px-Screen_Shot_2012-01-03_at_7.36.29_AM.png" width="93%" style="display: block; margin: auto;" /&gt;


]

.pull-right[

&lt;img src="./img/galton_1.jpg" width="58%" style="display: block; margin: auto;" /&gt;

&lt;img src="./img/galton_2.png" width="58%" style="display: block; margin: auto;" /&gt;

]



---

# Epidemiología y bioestadística

.pull-left[

### John Snow

Se le considera uno de los pioneros de la **epidemiología moderna** y la **estadística espacial**: aunque los **diagramas de Voronoi** tardarían años en ser formalizados, John Snow aplicó el mismo concepto para mitigar la **epidemia de cólera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street.

]

.pull-right[

&lt;img src="https://media.revistagq.com/photos/5cc84a91c46d3a2b7435d7cf/2:3/w_1799,h_2699,c_limit/pelo%20jon%20snow.jpg" width="100%" style="display: block; margin: auto;" /&gt;

]

---

# El boom de la estadística: epidemiología y bioestadística


.pull-left[

#### John Snow

Se le considera uno de los pioneros de la **epidemiología moderna** y la **estadística espacial**: aunque los **diagramas de Voronoi** tardarían años en ser formalizados, John Snow aplicó el mismo concepto para mitigar la **epidemia de cólera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street&lt;sup&gt;1&lt;/sup&gt;.


[1] [📚 «El mapa fantasma», Steven Johnson, sobre la historia de John Snow](https://capitanswing.com/libros/el-mapa-fantasma/)


]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://s1.eestatic.com/2016/04/22/reportajes/reportajes_119248513_3987143_854x640.jpg" alt="John Snow, el epidemiólogo" width="100%" /&gt;
&lt;p class="caption"&gt;John Snow, el epidemiólogo&lt;/p&gt;
&lt;/div&gt;

]

---

# Primeros usos de la estadística espacial

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/snow_mapa.jpg" alt="Mapa de Londres, mostrando los casos de cólera del 19 de agosto al 30 de septiembre de 1854, extraído de https://friendly.github.io/HistDataVis." width="77%" /&gt;
&lt;p class="caption"&gt;Mapa de Londres, mostrando los casos de cólera del 19 de agosto al 30 de septiembre de 1854, extraído de https://friendly.github.io/HistDataVis.&lt;/p&gt;
&lt;/div&gt;

---

# ¿Qué es el análisis multidimensional?

Hasta la década de los 60, la mayoría de la estadística que se realizaba era

* **estadística unidimensional**: extraer información de una sola variable (rentas, impuestos, exportaciones, etc).

* **estadística bidimensional**: desde que Galton acuñó la regresión, los grandes estadísticos de principios de siglo se centraron en el **análisis bidimensional**, analizando la dependencia entre una variable `\(X\)` y otra variable `\(Y\)`, con herramientas como los **coeficientes de correlación** de Pearson, Spearman o Kendall&lt;sup&gt;1&lt;/sup&gt;


[1] [📚 Kendall, M. (1938). «A New Measure of Rank Correlation». Biometrika 30 (1–2): 81-89. doi:10.1093/biomet/30.1-2.81](https://doi.org/10.1093/biomet/30.1-2.81)

--

&amp;nbsp;

## Wishart, contigo empezó todo

En 1928, Wishart publicó su famoso artículo &lt;sup&gt;2&lt;/sup&gt; en el que, se demostraba y desarrollaba explícitamente la función de distribución de una 
**distribución normal multivariante**, trabajo más tarde extendido y formalizado por Fisher.

[2] [📚 Wishart, J. (1928). «The generalised product moment distribution in samples from a normal multivariate population». Biometrika. 20A (1–2): 32–52. doi:10.1093/biomet/20A.1-2.32](https://doi.org/10.1093/biomet/20A.1-2.32)


---
 
# ¿Qué es el análisis multidimensional?
 
Aunque el verdadero boom no llegó hasta la **década de los 60**, con la publicación del libro «An Introduction to Multivariate Statistical Analysis»&lt;sup&gt;1&lt;/sup&gt; de Anderson (1958), proporcionando todo un marco teórico con el que poder trabajar.

[1] [📚 Anderson, T.W. (1958). «An Introduction to Multivariate Analysis». New York: Wiley ISBN 0471026409](http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/301_03_Anderson_An-Introduction-to-Multivariate-Statistical-Analysis-2003.pdf)

--

&amp;nbsp;

## **Definición**

&gt; El Análisis Multivariante es la rama de la estadística que estudia las relaciones (CONJUNTAMENTE) entre conjuntos de variables dependientes y los individuos para los cuales se han medido dichas variables (Kendall)


## **Notación**

* `\(n\)` tamaño muestral (número de individuos --&gt; filas).

* `\(\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)\)` conjunto de `\(p\)` variables (--&gt; columnas) medidas para cada individuo `\(i=1,\ldots,n\)`.

* Nuestros datos estarán en forma de tabla o matriz `\(\boldsymbol{X}\)` de `\(n\)` filas y `\(p\)` columnas (con `\(p \ll n\)`)

---

# Ejemplo de distribución bidimensional: normal bivariante

.pull-left[ 

Veamos un ejemplo sencillo con algo que seguramente nos sea familiar: la **distribución Normal o campana de Gauss** `\(X \sim \mathcal{N}\left(\mu, \sigma \right)\)`, cuya función de densidad es

`$$f(x) = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}}, \quad \mu \in\mathbb{R},~\sigma &gt;0$$`

&amp;nbsp;

La normal univariante depende de **dos parámetros**:

* **esperanza o media** `\(\mu = {\rm E} [X]\)` 
* **varianza** (unidimensional) `\(\sigma^2 := {\rm Var} [X] = {\rm E} [\left(X - \mu \right)^2] = {\rm E} [X^2] - \mu^2\)`


]


.pull-right[


```r
# Generamos muestra normal
rnorm(n = 10000, mean = 0, sd = 1)
```



&lt;img src="index_files/figure-html/unnamed-chunk-23-1.png" width="80%" /&gt;
  

]

---

# Normal bivariante

### **¿Y si medimos para cada individuo DOS variables?**

Si tenemos `\(\boldsymbol{X} = \left(X_1, X_2 \right)\)`, ¿qué estadísticos tenemos ahora a nuestra disposición?

* **Medidas marginales** (cada variable por separado):
  - medias `\(\mu_1:= {\rm E} [X_1]\)` y `\(\mu_2:= {\rm E} [X_2]\)`
  - varianzas `\(\sigma_{1}^{2}:=\sigma_{1, 1}^{2} = \sigma_{X_1, X_1}^2\)` y `\(\sigma_{2}^{2}:=\sigma_{2, 2}^{2} = \sigma_{X_2, X_2}^2\)`.

--

&amp;nbsp;

### **Covarianza**

La varianza `\({\rm Var} [X] := \sigma_{X}^2 = {\rm E} [ \left( X - \mu \right)^2 ]\)` es una medida de dispersión que nos **cuantifica** la relación de una variable consigo misma. ¿Y si en lugar de medir `\(X_1\)` vs `\(X_1\)` medimos `\(X_1\)` vs `\(X_2\)`?

Definiremos la **covarianza** como una especie de varianza en la que cambiamos una de las `\(X\)` por otra variable

`$${\rm Cov} [X_1, X_2] := \sigma_{1,2} =  {\rm E} [ \left( X_1 - \mu_1 \right) \left( X_2 - \mu_2 \right) ] = {\rm E}[X_1 * X_2] - \mu_1 * \mu_2 = \sigma_{2,1}$$`

---

# Normal bivariante

### **Matriz de covarianzas**

Desde un punto de vista teórico, dada una variable aleatoria bidimensional `\(\boldsymbol{X} = \left(X_1, X_2 \right)^{T}\)`, con vector de medias `\(\boldsymbol{\mu} = \left(\mu_1, \mu_2 \right)^{T}\)` definiremos la **matriz de varianzas y covarianzas** `\(\Sigma\)` de la siguiente manera:

`$$\boldsymbol{\Sigma} := \begin{pmatrix} \sigma_{1,1}^2 &amp; \sigma_{1,2} \\ \sigma_{2,1} &amp; \sigma_{2,2}^2 \end{pmatrix} = \begin{pmatrix} \sigma_{1}^2 &amp; \sigma_{1,2} \\ \sigma_{1,2} &amp; \sigma_{2}^2 \end{pmatrix}, \quad \left| \boldsymbol{\Sigma} \right| = \sigma_{1}^{2}  \sigma_{2}^{2} - \sigma_{1,2}^{2} &gt; 0$$`

--

Se puede expresar **matricialmente** como

`\(\begin{eqnarray}\boldsymbol{\Sigma} = {\rm E} \left[\left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T}\left(\boldsymbol{X} - \boldsymbol{\mu} \right) \right] &amp;=&amp; {\rm E} \left[\left( X_1  - \mu_1, X_2 - \mu_2 \right)^{T} \begin{pmatrix} X_1  - \mu_1 \\ X_2 - \mu_2 \end{pmatrix} \right] \\ &amp;=&amp; \begin{pmatrix} {\rm E} \left[ \left(X_1  - \mu_1 \right)^2 \right] &amp; {\rm E} \left[\left(X_1  - \mu_1 \right)\left(X_2  - \mu_2 \right) \right] \\ {\rm E} \left[\left(X_2  - \mu_2 \right)\left(X_1  - \mu_1 \right) \right] &amp; {\rm E} \left[\left(X_2  - \mu_2 \right)^2\right] \end{pmatrix} \end{eqnarray}\)`

**IMPORTANTE**: es una **matriz simétrica** (nos da igual medir `\(X\)` vs `\(Y\)`, que `\(Y\)` vs `\(X\)`).

---

# Normal multivariante

### **Normal univariante**

`$$X \sim \mathcal{N} \left(\mu, \sigma^2 \right), \quad \boldsymbol{\Sigma} = \sigma^2, \quad f(x) =  \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}} = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-\frac{1}{2} (x-\mu ) \boldsymbol{\Sigma}^{-1} (x-\mu )}$$`


### **Normal bivariante**

`$$\boldsymbol{X} = \left(X_1, X_2 \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, x_2) = \frac{1}{2\pi \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$`

--

### **Normal multivariante (caso general)** 

Multivariante de `\(p \ll n\)` variables

`$$\boldsymbol{X} = \left(X_1, \ldots, X_p \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, \ldots, x_p) = \frac{1}{\left(2\pi \right)^{p/2} \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$`

`$$\boldsymbol{\Sigma} = \left(\Sigma_{i,j} \right)_{i,j=1,\ldots,p}, \quad \Sigma_{i,j}:= {\rm Cov} [X_i, X_j ] = {\rm E}[(X_i-\mu_i) (X_j - \mu_j)]$$`

---

# Versión muestral

Lo anterior nos permite conocer la **formulación teórica (poblacional)**: ¿cómo calculamos la varianza y covarianza cuando tenemos una muestra `\(\boldsymbol{X}\)` de `\(n\)` individuos y `\(p\)` observaciones medidas?


`$$\boldsymbol{X} = \begin{pmatrix} x_{1, 1} &amp; \ldots &amp; x_{1, p} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n, 1} &amp; \ldots &amp; x_{n, p} \end{pmatrix} \quad \text{muestra}$$`

#### **p = 2**

* **Varianzas muestrales**: `\(s_{x_1}^{2} := s_{1}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 1} - \overline{x}_1 \right)^2\)` y `\(s_{x_2}^{2} := s_{2}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 2} - \overline{x}_2 \right)^2\)`, donde `\(\overline{x}_1\)` y `\(\overline{x}_2\)` son sus medias muestrales.

* **Covarianza muestral**: `\(s_{x_1, x_2}^{2} := s_{1, 2} = s_{2, 1}^2 = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \left(x_{i, 1} - \overline{x}_1 \right)\left(x_{j, 2} - \overline{x}_2 \right)\)`

--

#### **Estimadores insesgados**

Seguramente dichos valores los hallas visto divididos por `\(n-1\)` en lugar de `\(n\)`: los valores muestrales son estimadores de los valores poblacionales, y de aquí en adelante usaremos **estimadores insesgados**, estimadores `\(T\)` del valor población `\(U\)` tal que `\({\rm E}[T] = U\)`

* Estimador insesgado de `\(\mu_{x}\)`: `\(\overline{x}\)` tal que `\({\rm E}[\overline{x}] = \mu\)`

* Estimador insesgado de `\(\sigma_{x}^2\)`: la **cuasivarianza** `\(S_{x}^2 = \frac{n}{n-1} s_{x}^{2}\)` tal que `\({\rm E}[\sigma_{x}^2] = S_{x}^2\)`

* Estimador insesgado de `\(\sigma_{x, y}\)`: la **cuasicovarianza** `\(S_{x, y} = \frac{n}{n-1} s_{x, y}\)` tal que `\({\rm E}[\sigma_{x, y}] = S_{x, y}\)`

---

# Matriz de covarianzas (versión muestral)


En un **caso general**, dada una muestra `\(\boldsymbol{X}\)` de `\(n\)` individuos y `\(p\)` variables

`$$S_{x_{k}}^2 := S_{k}^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_{i, k} - \overline{x}_k \right)^2 \quad \text{(cuasi) var. muestrales (marginales)}$$`


`$$S_{x_{k}, x_{l}} := S_{k, l} = \frac{1}{n-1} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_{i, k} - \overline{x}_k \right)\left(x_{j, l} - \overline{x}_l \right) \quad \text{(cuasi) covarianzas}$$`

Así, la **matriz de (cuasi) covarianzas empíricas** quedará como

`$$S := \frac{1}{n-1} \left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T} \left(\boldsymbol{X} - \boldsymbol{\mu} \right) =_{\boldsymbol{\mu} = 0} \frac{1}{n-1} \boldsymbol{X}^{T} \boldsymbol{X} = \begin{pmatrix} S_{1,1} &amp;  \ldots &amp; S_{1, p} \\ \vdots &amp; \ddots &amp; \vdots \\ S_{p,1} &amp; \ldots &amp; S_{p, p} \end{pmatrix}$$`

&amp;nbsp;

--

Las **covarianzas (y varianzas)** tienen un **«problema»**: **dependen de la magnitud** de los datos, proporcionando una medida que solo nos sirve para ser comparada con otra covariana, pero que **no nos proporciona una escala absoluta** para poder cuantificar.

---

# Matriz de correlaciones (versión muestral)


Para resolverlo, tenemos la **correlación (de Pearson)** 

`$$\rho_{k, l} := r_{k, l} = \frac{s_{k, l}}{\sqrt{s_{k}^2} \sqrt{s_{l}^2}} = \frac{S_{k, l}}{\sqrt{S_{k}^2} \sqrt{S_{l}^2}}$$`

tal que siempre `\(-1 \leq r_{k, l} \leq 1\)`.

&amp;nbsp;

--

De esta forma la **matriz de correlaciones** se puede expresar como

`$$R := \left(r_{k, l} \right)_{k,l=1,\ldots,p} = D^{-1/2} S D^{-1/2}, \quad D = diag(S) = \begin{pmatrix} S_{1,1}^2 &amp; \ldots &amp; 0 \\ \vdots  &amp; \ddots &amp; \vdots \\  0 &amp; \ldots &amp; S_{p, p}^2 \end{pmatrix}$$`


---


name: teoria-PCA
class: center, middle

# Teoría: análisis de componentes principales

---

# Objetivo: ¿reducir dimensión?

.pull-left[

El **objetivo «mainstream»** del **análisis de componentes principales** (PCA en inglés) suele ser el de **reducir la dimensión** de nuestros datos: pasar de un conjunto de `\(n\)` individuos y `\(p\)` variables a otro de `\(k &lt; p\)` variables (para los mismos `\(n\)` individuos).

&amp;nbsp;

Esta reducción de la dimensión se suele hacer con **3 objetivos** principalmente:

* **Mejora computacional** de los algoritmos al tener un dataset más reducido.

* **Permitir la visualización** en 2 o 3 dimensiones de conjuntos `\(n\)`-dimensionales.

* **«Reflotar» patrones** subyacentes en los datos.

]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://miro.medium.com/max/959/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg" alt="Extraída de https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa" width="100%" /&gt;
&lt;p class="caption"&gt;Extraída de https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa&lt;/p&gt;
&lt;/div&gt;

]

---


# Objetivo: ¿reducir dimensión?

.pull-left[

¿Entonces? ¿No tiene sentido aplicar componentes principales o técnicas de reducción de la dimensión en **datos bidimensionales**?

&amp;nbsp;

Empecemos por un sencillo ejemplo, visualizando la **longitud y anchura de pétalo** del famoso conjunto de datos `iris`

&amp;nbsp;

**¿Cuáles podrían ser los objetivos?** ¿Tiene sentido en este ejemplo aplicar **técnicas de reducción de la dimensión** como las componentes principales?

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-25-1.png" width="100%" /&gt;

]

---

# Objetivo: maximizar la información


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png" alt="Gráfica extraída de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" width="80%" /&gt;
&lt;p class="caption"&gt;Gráfica extraída de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c&lt;/p&gt;
&lt;/div&gt;

Como veremos, el **objetivo real** será **maximizar la información obtenido al menor coste posible**, y eso hace que siga siendo útil, aunque no reduzcamos dimensiones, hacerlo en el caso bidimensional: una **clave** de las componentes principales es que las **componentes resultantes** serán **ortogonales** (perpendiculares), es decir, **linealmente independientes**.

&amp;nbsp;

Las **componentes principales** pueden ser una herramienta muy útil para atajar problemas de **colinealidad** (variables altamente correladas entre sí, interfiriendo entre ellas)


---


# Idea principal

La **idea subyacente** tras el cálculo de las componentes principales se puede resumir de forma **geométrica**: para un conjunto de puntos `\(p\)`-dimensionales, encontrar un **nuevo sistema de coordenadas** de dimensión `\(k \leq p\)` en el que expresar los datos, de forma que las **nuevas variables sean linealmente independientes**.

En el **caso bidimensional**, el resultado de aplicar componentes principales será una especie de «rotación» de los datos


&lt;img src="https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png" width="75%" style="display: block; margin: auto;" /&gt;

📚 **«Principal Component Analysis»**. Hervé and Lynne (2010) &lt;http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf&gt;


---

# Idea principal: caso bidimensional


En el **caso bidimensional**, la idea será buscar esa **elipse** en torno a la cual tenemos los datos, de forma que la dirección que marca el **eje mayor** será la **primera componente** (la que tiene mayor rango --&gt; mayor varianza) y la dirección que marca el **eje menor** será la **segunda componente**.


.pull-left[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/pca_words_1.jpg" alt="Gráfica extraída de Hervé and Lynne (2010)" width="77%" /&gt;
&lt;p class="caption"&gt;Gráfica extraída de Hervé and Lynne (2010)&lt;/p&gt;
&lt;/div&gt;

]


.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/pca_words_23.jpg" alt="Gráfica extraída de Hervé and Lynne (2010)" width="58%" /&gt;
&lt;p class="caption"&gt;Gráfica extraída de Hervé and Lynne (2010)&lt;/p&gt;
&lt;/div&gt;

]

📚 **«Principal Component Analysis»**. Hervé and Lynne (2010) &lt;http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf&gt;

---

# Caso inicial bidimensional


.pull-left[

Vamos a empezar por un **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del pétalo.




```
&gt; # A tibble: 150 × 2
&gt;    Petal.Length Petal.Width
&gt;           &lt;dbl&gt;       &lt;dbl&gt;
&gt;  1          1.4         0.2
&gt;  2          1.4         0.2
&gt;  3          1.3         0.2
&gt;  4          1.5         0.2
&gt;  5          1.4         0.2
&gt;  6          1.7         0.4
&gt;  7          1.4         0.3
&gt;  8          1.5         0.2
&gt;  9          1.4         0.2
&gt; 10          1.5         0.1
&gt; # … with 140 more rows
```

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-32-1.png" width="100%" /&gt;

]

---

# Caso bidimensional

.pull-left[

1. Encontrar las **direcciónes de máxima varianza**. Dichas direcciones vendrán determinadas por **dos vectores** `\(\left\lbrace \boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2 \right\rbrace\)` perpendiculares entre sí y que serán **combinación lineal de las variables** originales.

`$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$`


]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/pca_iris_1.jpg" alt="Direcciones de máxima varianza" width="85%" /&gt;
&lt;p class="caption"&gt;Direcciones de máxima varianza&lt;/p&gt;
&lt;/div&gt;

]

---

# Caso bidimensional

.pull-left[


1. Encontrar las **direcciónes de máxima varianza**. Dichas direcciones vendrán determinadas por **dos vectores** `\(\left\lbrace \Phi_1, \Phi_2 \right\rbrace\)` perpendiculares entre sí y que serán **combinación lineal de las variables** originales.
`$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$`

2. Dado un registro `\(\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)\)` (que puede entenderse como un vector `\(\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i\)`), lo que haremos será obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
`$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$`

]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/pca_iris_2.jpg" alt="Proyección ortogonal" width="78%" /&gt;
&lt;p class="caption"&gt;Proyección ortogonal&lt;/p&gt;
&lt;/div&gt;

]

---


# Caso bidimensional

.pull-left[


1. Encontrar las **direcciónes de máxima varianza**. Dichas direcciones vendrán determinadas por **dos vectores** `\(\left\lbrace \Phi_1, \Phi_2 \right\rbrace\)` perpendiculares entre sí y que serán **combinación lineal de las variables** originales.
`$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$`

2. Dado un registro `\(\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)\)` (que puede entenderse como un vector `\(\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i\)`), lo que haremos será obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
`$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$`

3. Las **nuevas direcciones** las seleccionaremos  **ortonormales** (módulo unitario):
`$$x_{i, 1}'  =  \langle \boldsymbol{x}_i, \Phi_1 \rangle =  \left(x_{i, 1}, x_{i, 2} \right) \left(z_{1, 1}, z_{2, 1} \right)^{T} = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{1}^{T}, \quad x_{i, 2}' = \langle \boldsymbol{x}_i, \Phi_2 \rangle = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{2}^{T}$$`

]

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="./img/pca_iris_2.jpg" alt="Proyección ortogonal" width="78%" /&gt;
&lt;p class="caption"&gt;Proyección ortogonal&lt;/p&gt;
&lt;/div&gt;

]

---

# Idea general


Nuestros datos originales `\(\boldsymbol{X}\)`  (dimensiones `\(n \times p\)`) serán reconvertidos en un conjunto `\(\boldsymbol{X}'\)` de dimensiones `\(n \times k\)`, con `\(k \leq p\)`, tal que 

`$$\boldsymbol{X}' = \boldsymbol{X} \boldsymbol{\Phi}^{T}$$`

--

tal que `\(\boldsymbol{\Phi}^{T}\)` es una matriz `\(p \times k\)` que contiene por columnas las `\(k\)` **direcciones principales**

`$$\boldsymbol{\Phi}^{T} = \begin{pmatrix} z_{1,1} &amp; z_{2,1} &amp; \ldots &amp; z_{k,1} \\ z_{1,2} &amp; z_{2,2} &amp; \ldots &amp; z_{k,2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{1,p} &amp; z_{2,p} &amp; \ldots &amp; z_{k,p} \end{pmatrix}$$`

--

bajo la condición de que sean **direcciones ortonormales**

`$$\Phi \Phi^{T} = \begin{pmatrix} 1 &amp; \ldots &amp; 0 \\  \vdots &amp;  \ddots &amp; \vdots \\ 0  &amp; \ldots &amp; 1 \end{pmatrix}$$`

tal que dichas direcciones **maximicen la varianza**.

---

# Primera componente

Por ejemplo, para la **primera componente** el objetivo es encontrar, de entre todas las direcciones  `\(\boldsymbol{u}_1\)` posibles, la dirección `\(\boldsymbol{\Phi}_1\)` que **maximice la varianza de nuestros datos cuando los proyectamos sobre dicha dirección**

`$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} &amp; x_{1, 2} &amp; \ldots &amp; x_{1, p} \\ x_{2,1} &amp; x_{2, 1} &amp; \ldots &amp; x_{2, p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots
\\ x_{n,1} &amp; x_{n, 2} &amp; \ldots &amp; x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ z_{1,2} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ x_{2,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$`


--

&amp;nbsp;

Dicha dirección por tanto saldrá de un proceso de **optimización**

`$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$`

---

# Primera componente

Dicha dirección por tanto saldrá de un proceso de **optimización**

`$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$`

--

Si **centramos los datos** (restamos su media para tener media nula)


`$$\begin{eqnarray}\boldsymbol{\Phi}_1 &amp;=&amp; \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} {\rm Var} \left( \boldsymbol{X}\boldsymbol{u}_{1}^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left({\rm E} \left[\left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right)^{T}\left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)\right] \right) \\ &amp;=&amp; \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} \left({\rm E} \left[ \boldsymbol{u}_{1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{u}_1^{T} \right] \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 {\rm E} \left[\boldsymbol{X}^{T} \boldsymbol{X} \right] \boldsymbol{u}_{1}^{T}  \right) \\ &amp;=&amp; \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_{1}^{T}  \right)\end{eqnarray}$$`

--

Si **estandarizamos los datos** (restamos su media y dividimos entre su desviación típica, teniendo **datos con media cero y varianza unitaria** para que todos los datos ponderen por igual)

`$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right)$$`

---

# Primera componente

Para **encontrar esa dirección `\(\boldsymbol{u}_1\)`** que nos maximiza la varianza de los proyectados en ella, sujeto a la restrcción de que `\(\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1\)`, se puede usar la técnica de los **multiplicadores de Lagrange** que nos dice que

`$$\boldsymbol{\Phi}_1 =  \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right), \quad \lambda \in \mathbb{R}$$`

--

Eso es equivalente a encontrar el valor que nos **iguale la derivada a cero**

`$$\frac{\partial}{\partial \boldsymbol{u}_1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right) =   R \boldsymbol{u}_1  - \lambda \boldsymbol{u}_1^{T} = \left(R - \lambda \boldsymbol{Id}_{p} \right) \boldsymbol{u}_1^{T}  =  0$$`
--

Esto es lo mismo que decir que `\(R \boldsymbol{u}_1^{T}  = \lambda \boldsymbol{u}_1^{T}\)`, es decir, la dirección que buscamos `\(\boldsymbol{u}_{1}^{T}\)` es un **autovector de la matriz de covarianzas** (tras **estandarizar** los datos).

---

# Paréntesis: autovectores y autovalores

En álgebra matricial, dada una matriz `\(\boldsymbol{A}\)` cuadrada de tamaño `\(p \times p\)`, decimos que `\(v\)` es su **autovector** y `\(\lambda\)` su **autovalor asociado** si y solo sí

`$$A v= \lambda v, \quad v = \left(v_1, \ldots, v_p \right) \neq 0$$`

Esto es equivalente a decir que 

`$$A v - \lambda v = 0 \rightarrow (A - \lambda I_{p}) v = 0$$`

donde `\(I_p\)` es la matriz identidad de tamaño `\(p \times p\)`. Dicha ecuación tiene solución si y solo sí

`$$\left| A - \lambda I_{p} \right| = 0$$`

Además, por el **Teorema Fundamental del Algebra** sabemos que dicho determinante puede expresarse como un polinomio de grado `\(p\)` (conocido como **polinomio característico**)

`$$\left| A - \lambda I_{p} \right| = \left(\lambda_1 - \lambda \right)\left(\lambda_2 - \lambda \right) \ldots \left(\lambda_p - \lambda \right) = p (\lambda)$$`

Además el determinante `\(\left| A \right|\)` será el producto de todos sus autovalores.

---

# Primera componente

Recapitulando, para obtener la **primera componente** `\(\boldsymbol{\Phi}_1\)`, debemos de 

* **Estandarizar** nuestros datos
* Calcular la **matriz de (cuasi)covarianzas** `\(\boldsymbol{S}\)`
* Calcula sus **autovectores** tal que `\(S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}\)` (normalizados a módulo 1).

--

Además si es un autovector de la matriz de covarianzas tenemos entonces que la **varianza maximizada**, la **proporción de información** que **explica dicha componente**, será

`$$\boldsymbol{\Phi}_{1} \left(  S \boldsymbol{\Phi}_{1}^{T} \right) = \boldsymbol{\Phi}_{1} \left( R \boldsymbol{\Phi}_{1}^{T} \right) =\boldsymbol{\Phi}_{1}\left(  \lambda_1 \boldsymbol{\Phi}_{1}^{T} \right) =  \lambda_1 \boldsymbol{\Phi}_{1} \boldsymbol{\Phi}_{1}^{T} =_{\text{ortonormales}} \lambda_1$$` 

--

Así que obtener la dirección (de todos los autovalores) que mayor información captura nos fijaremos en aquella que tenga **asociada el autovalor más grande**.


`$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1, p}  \\ \vdots  &amp; \ddots &amp; \vdots
\\ x_{n,1}  &amp; \ldots &amp; x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$`

donde `\(S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}\)`, siendo `\(\lambda_1\)` el mayor de los autovalores de la matriz de (cuasi)covarianzas `\(S\)`, y `\(\boldsymbol{\Phi}_{1}^{T}\)` su autovector asociado. El **resto de las componentes** se obtendrán de forma similar, siendo ortogonales a cada una de las direcciones obtenidas.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Dados unos datos `\(\boldsymbol{X}\)` de `\(n\)` individuos y `\(p\)` variables, el objetivo es encontrar nuevas **direcciones ortonormales** `\(\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace\)`, con `\(1 \leq k \leq p\)`, como combinación lineal de las variables originales.

* Los **datos son estandarizados**  tal que

`$$\begin{pmatrix} \frac{x_{1,1} - \overline{x}_1}{S_{1}} &amp; \frac{x_{1,2} - \overline{x}_2}{S_{2}} &amp; \ldots &amp; \frac{x_{1,p} - \overline{x}_p}{S_{p}} \\  \frac{x_{2,1} - \overline{x}_1}{S_{1}} &amp; \frac{x_{2,2} - \overline{x}_2}{S_{2}} &amp; \ldots &amp; \frac{x_{2,p} - \overline{x}_p}{S_{p}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{x_{n,1} - \overline{x}_1}{S_{1}} &amp; \frac{x_{n,2} - \overline{x}_2}{S_{2}} &amp; \ldots &amp; \frac{x_{n,p} - \overline{x}_p}{S_{p}} \end{pmatrix}$$`

* Calcular la **matriz `\(S\)` de (cuasi)covarianzas** de dichos datos estandarizados.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Calculamos los `\(p\)` **autovectores** `\(\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_p \right\rbrace\)`, y sus **autovalores asociados** `\(\left\lbrace \lambda_1, \ldots, \lambda_p \right\rbrace\)`, de la matriz `\(S\)`, tal que `\(S  \boldsymbol{\Phi}_k = \lambda_k  \boldsymbol{\Phi}_k\)`.

* Seleccionamos las primeras `\(k \leq p\)` componentes `\(\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace\)` asociadas a los primeros `\(\left\lbrace \lambda_1, \ldots, \lambda_k \right\rbrace\)` autovalores.

* La **varianza (información) capturada** por la dirección `\(k\)`-ésima será igual a `\(\lambda_k\)`.

* Las nuevas coordenadas serán

`$$\boldsymbol{X}^{'} = \boldsymbol{X} \boldsymbol{\Phi}^{T} = \begin{pmatrix} x_{1,1} &amp; x_{1, 2} &amp; \ldots &amp; x_{1, p} \\ x_{2,1} &amp; x_{2, 1} &amp; \ldots &amp; x_{2, p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots
\\ x_{n,1} &amp; x_{n, 2} &amp; \ldots &amp; x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} &amp; z_{2,1} &amp; \ldots &amp; z_{k,1} \\ z_{1,2} &amp; z_{2,2} &amp; \ldots &amp; z_{k,2}  \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ z_{1,p} &amp; z_{2,p} &amp; \ldots &amp; z_{k,p} \end{pmatrix} = \begin{pmatrix} \boldsymbol{x}_1 \boldsymbol{\Phi}_1^{T} &amp; \boldsymbol{x}_1 \boldsymbol{\Phi}_2^{T} &amp; \ldots &amp; \boldsymbol{x}_1 \boldsymbol{\Phi}_k^{T} \\ \boldsymbol{x}_2 \boldsymbol{\Phi}_1^{T} &amp; \boldsymbol{x}_2 \boldsymbol{\Phi}_2^{T} &amp; \ldots &amp; \boldsymbol{x}_2 \boldsymbol{\Phi}_k^{T} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \boldsymbol{x}_n \boldsymbol{\Phi}_1^{T} &amp; \boldsymbol{x}_n \boldsymbol{\Phi}_2^{T} &amp; \ldots &amp; \boldsymbol{x}_n \boldsymbol{\Phi}_k^{T}\end{pmatrix}$$`

---

# Glosario 

* **Autovectores**: nos indican la **dirección de la componente**

* **Loadings**: serán los **coeficientes** de dichos autovectores (los que generan la combinación lineal de las variables originales) nos indican el **peso que tiene cada variable original en dicha componente**. Si por ejemplo `\(\boldsymbol{\Phi}_1 = \left(0.95, 0.15, 0.273 \right)\)`, significa que la primera componente (la que más varianza captura) será `\(0.95* \boldsymbol{X}_1 + 0.15 * \boldsymbol{X}_2 + 0.273 * \boldsymbol{X}_3\)`, estando dominada por la variable original `\(\boldsymbol{X}_1\)` (un peso de 0.95).


* **Signo de los loadings**: nos indica el **sentido de la relación entre la componente y la variable original** (correlación positiva/negativa). Si alguno de ellos fuese 0 significaría que la nueva componente está incorrelada respecto a dicha variable original.


* **Scores**: para cada observación `\(i\)`, las nuevas coordenadas `\(\left(x_{i, 1}^{'}, \ldots, x_{i, k}^{'} \right)\)`, calculadas tras **proyectar la observación original en las direcciones principales** `\(\boldsymbol{x}_{i} \boldsymbol{\Phi}^{T}\)`.


* **Truncamiento o número de componentes**: para seleccionar el número `\(k\)` de componentes a seleccionar el método más sencillo es **fijar de antemano** un **umbral varianza explicada** que queremos conservar (por ejemplo, `\(95%\)`), de forma que nos quedemos con el primer número `\(k\)` tal que `\(\sum_{j=1}^{k} \lambda_k &gt; 0.95\)` (**varianza explicada acumulada**, teniendo los autovalores ordenados de mayor a menor).


---


name: practica-PCA
class: center, middle

# Práctica: PCA en R

---

# PCA en R: caso «manual»


Volvemos a nuestro **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del pétalo.


```r
iris_bi &lt;- 
  tibble(iris) %&gt;%
  select(contains("Petal"))
iris_bi
```

```
&gt; # A tibble: 150 × 2
&gt;    Petal.Length Petal.Width
&gt;           &lt;dbl&gt;       &lt;dbl&gt;
&gt;  1          1.4         0.2
&gt;  2          1.4         0.2
&gt;  3          1.3         0.2
&gt;  4          1.5         0.2
&gt;  5          1.4         0.2
&gt;  6          1.7         0.4
&gt;  7          1.4         0.3
&gt;  8          1.5         0.2
&gt;  9          1.4         0.2
&gt; 10          1.5         0.1
&gt; # … with 140 more rows
```

---

# PCA en R: caso «manual»


**Primer paso**: **estandarizar** los datos.


```r
iris_bi_std &lt;-
  iris_bi %&gt;%
  mutate(Petal.Length = (Petal.Length - mean(Petal.Length)) /  sd(Petal.Length),
         Petal.Width = (Petal.Width - mean(Petal.Width)) / sd(Petal.Width))

iris_bi_std
```

```
&gt; # A tibble: 150 × 2
&gt;    Petal.Length Petal.Width
&gt;           &lt;dbl&gt;       &lt;dbl&gt;
&gt;  1        -1.34       -1.31
&gt;  2        -1.34       -1.31
&gt;  3        -1.39       -1.31
&gt;  4        -1.28       -1.31
&gt;  5        -1.34       -1.31
&gt;  6        -1.17       -1.05
&gt;  7        -1.34       -1.18
&gt;  8        -1.28       -1.31
&gt;  9        -1.34       -1.31
&gt; 10        -1.28       -1.44
&gt; # … with 140 more rows
```

---

# PCA en R: caso «manual»

.pull-left[
&lt;img src="index_files/figure-html/unnamed-chunk-38-1.png" width="100%" /&gt;

]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-39-1.png" width="100%" /&gt;

]

---


# PCA en R: caso «manual»

**Segundo paso**: calcular la **matriz de covarianzas**


```r
cov_mat &lt;- cov(iris_bi_std)
cov_mat
```

```
&gt;              Petal.Length Petal.Width
&gt; Petal.Length    1.0000000   0.9628654
&gt; Petal.Width     0.9628654   1.0000000
```

Al estar estandarizados los datos, es equivalente a calcular la matriz de correlaciones


```r
library(corrr)
iris_bi_std %&gt;% correlate(diagonal = 1)
```

```
&gt; # A tibble: 2 × 3
&gt;   term         Petal.Length Petal.Width
&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;
&gt; 1 Petal.Length        1           0.963
&gt; 2 Petal.Width         0.963       1
```

---

# PCA en R: caso «manual»

**Tercer paso**: calcular los **autovalores y autovectores** de la matriz de covarianzas


```r
autoelementos &lt;- eigen(cov_mat)
autoelementos
```

```
&gt; eigen() decomposition
&gt; $values
&gt; [1] 1.96286543 0.03713457
&gt; 
&gt; $vectors
&gt;           [,1]       [,2]
&gt; [1,] 0.7071068 -0.7071068
&gt; [2,] 0.7071068  0.7071068
```

**IMPORTANTE** al tener las **variables estandarizadas**, la **suma de los autovalores** es `\(p\)` (ya que será la suma de las varianzas de las variables que tenemos).

---

# PCA en R: caso «manual»

**Cuarto paso**: **ordenar** autovectores segun autovalores (de mayor a menor)


```r
order_lambda &lt;-
  order(autoelementos$values, decreasing = TRUE)
lambda &lt;- autoelementos$values[order_lambda]
PC &lt;- autoelementos$vectors[, order_lambda]
lambda # autovalores ordenadores
```

```
&gt; [1] 1.96286543 0.03713457
```

La **varianza capturada** por `\(\boldsymbol{\Phi}_1\)` es `\(1.963\)` y `\(0.037\)` para la segunda componente `\(\boldsymbol{\Phi}_2\)`.


```r
PC # autovectores asociados --&gt; direcciones principales
```

```
&gt;           [,1]       [,2]
&gt; [1,] 0.7071068 -0.7071068
&gt; [2,] 0.7071068  0.7071068
```

---

# PCA en R: caso «manual»


**Quinto paso**: calcular la **varianza explicada acumulada** por cada componente (una vez ordenadas)


```r
cumsum(lambda) / sum(lambda)
```

```
&gt; [1] 0.9814327 1.0000000
```

La **primera componente captura el 98.14% de la información (de la varianza)** y la segunda el 1.86% restante.

---

# PCA en R: caso «manual»

* **Sexto paso**: proyectar en las nuevas componentes para obtener las **nuevas coordenadas** (¡respecto a la nueva base, a las nuevas componentes!)
 


```r
iris_pca &lt;- iris_bi_std * t(PC)
names(iris_pca) &lt;- c("PC_1", "PC_2")
iris_pca
```

```
&gt;            PC_1          PC_2
&gt; 1   -0.94451904 -0.9270538645
&gt; 2    0.94451904 -0.9270538645
&gt; 3   -0.98457498 -0.9270538645
&gt; 4   -0.90446310  0.9270538645
&gt; 5   -0.94451904 -0.9270538645
&gt; 6    0.82435122 -0.7415194019
&gt; 7   -0.94451904 -0.8342866332
&gt; 8   -0.90446310  0.9270538645
&gt; 9   -0.94451904 -0.9270538645
&gt; 10   0.90446310 -1.0198210958
&gt; 11  -0.90446310 -0.9270538645
&gt; 12  -0.86440716  0.9270538645
&gt; 13  -0.94451904 -1.0198210958
&gt; 14   1.06468686 -1.0198210958
&gt; 15  -1.02463092 -0.9270538645
&gt; 16  -0.90446310  0.7415194019
&gt; 17  -0.98457498 -0.7415194019
&gt; 18   0.94451904 -0.8342866332
&gt; 19  -0.82435122 -0.8342866332
&gt; 20  -0.90446310  0.8342866332
&gt; 21  -0.82435122 -0.9270538645
&gt; 22   0.90446310 -0.7415194019
&gt; 23  -1.10474279 -0.9270538645
&gt; 24  -0.82435122  0.6487521707
&gt; 25  -0.74423934 -0.9270538645
&gt; 26   0.86440716 -0.9270538645
&gt; 27  -0.86440716 -0.7415194019
&gt; 28  -0.90446310  0.9270538645
&gt; 29  -0.94451904 -0.9270538645
&gt; 30   0.86440716 -0.9270538645
&gt; 31  -0.86440716 -0.9270538645
&gt; 32  -0.90446310  0.7415194019
&gt; 33  -0.90446310 -1.0198210958
&gt; 34   0.94451904 -0.9270538645
&gt; 35  -0.90446310 -0.9270538645
&gt; 36  -1.02463092  0.9270538645
&gt; 37  -0.98457498 -0.9270538645
&gt; 38   0.94451904 -1.0198210958
&gt; 39  -0.98457498 -0.9270538645
&gt; 40  -0.90446310  0.9270538645
&gt; 41  -0.98457498 -0.8342866332
&gt; 42   0.98457498 -0.8342866332
&gt; 43  -0.98457498 -0.9270538645
&gt; 44  -0.86440716  0.5559849394
&gt; 45  -0.74423934 -0.7415194019
&gt; 46   0.94451904 -0.8342866332
&gt; 47  -0.86440716 -0.9270538645
&gt; 48  -0.94451904  0.9270538645
&gt; 49  -0.90446310 -0.9270538645
&gt; 50   0.94451904 -0.9270538645
&gt; 51   0.37732694  0.1861529107
&gt; 52   0.29721507 -0.2789201420
&gt; 53   0.45743882  0.2789201420
&gt; 54  -0.09693537  0.0933856795
&gt; 55   0.33727101  0.2789201420
&gt; 56   0.29721507 -0.0933856795
&gt; 57   0.37732694  0.3716873733
&gt; 58   0.18345620 -0.1849160143
&gt; 59   0.33727101  0.0933856795
&gt; 60   0.05687943 -0.1861529107
&gt; 61  -0.10334432 -0.1849160143
&gt; 62  -0.17704725  0.2789201420
&gt; 63   0.09693537 -0.1849160143
&gt; 64   0.37732694 -0.1861529107
&gt; 65  -0.06328838  0.0933856795
&gt; 66  -0.25715913  0.1861529107
&gt; 67   0.29721507  0.2789201420
&gt; 68   0.13699131  0.1849160143
&gt; 69   0.29721507  0.2789201420
&gt; 70  -0.05687943 -0.0921487831
&gt; 71   0.41738288  0.5572218358
&gt; 72   0.09693537 -0.0933856795
&gt; 73   0.45743882  0.2789201420
&gt; 74  -0.37732694  0.0006184482
&gt; 75   0.21710319  0.0933856795
&gt; 76   0.25715913 -0.1861529107
&gt; 77   0.41738288  0.1861529107
&gt; 78  -0.49749476  0.4644546046
&gt; 79   0.29721507  0.2789201420
&gt; 80  -0.10334432  0.1849160143
&gt; 81   0.01682349 -0.0921487831
&gt; 82   0.02323244 -0.1849160143
&gt; 83   0.05687943  0.0006184482
&gt; 84   0.53755070 -0.3716873733
&gt; 85   0.29721507  0.2789201420
&gt; 86  -0.29721507  0.3716873733
&gt; 87   0.37732694  0.2789201420
&gt; 88   0.25715913 -0.0933856795
&gt; 89   0.13699131  0.0933856795
&gt; 90  -0.09693537  0.0933856795
&gt; 91   0.25715913  0.0006184482
&gt; 92   0.33727101 -0.1861529107
&gt; 93   0.09693537  0.0006184482
&gt; 94   0.18345620 -0.1849160143
&gt; 95   0.17704725  0.0933856795
&gt; 96   0.17704725 -0.0006184482
&gt; 97   0.17704725  0.0933856795
&gt; 98  -0.21710319  0.0933856795
&gt; 99  -0.30362402 -0.0921487831
&gt; 100  0.13699131 -0.0933856795
&gt; 101  0.89805415  1.2065924547
&gt; 102 -0.53755070  0.6499890671
&gt; 103  0.85799821  0.8355235296
&gt; 104  0.73783039 -0.5572218358
&gt; 105  0.81794227  0.9282907609
&gt; 106 -1.13838978  0.8355235296
&gt; 107  0.29721507  0.4644546046
&gt; 108  1.01822197 -0.5572218358
&gt; 109  0.81794227  0.5572218358
&gt; 110 -0.93811009  1.2065924547
&gt; 111  0.53755070  0.7427562984
&gt; 112  0.61766258 -0.6499890671
&gt; 113  0.69777446  0.8355235296
&gt; 114 -0.49749476  0.7427562984
&gt; 115  0.53755070  1.1138252234
&gt; 116  0.61766258 -1.0210579922
&gt; 117  0.69777446  0.5572218358
&gt; 118 -1.17844572  0.9282907609
&gt; 119  1.25855760  1.0210579922
&gt; 120  0.49749476 -0.2789201420
&gt; 121  0.77788633  1.0210579922
&gt; 122 -0.45743882  0.7427562984
&gt; 123  1.17844572  0.7427562984
&gt; 124  0.45743882 -0.5572218358
&gt; 125  0.77788633  0.8355235296
&gt; 126 -0.89805415  0.5572218358
&gt; 127  0.41738288  0.5572218358
&gt; 128  0.45743882 -0.5572218358
&gt; 129  0.73783039  0.8355235296
&gt; 130 -0.81794227  0.3716873733
&gt; 131  0.93811009  0.6499890671
&gt; 132  1.05827790 -0.7427562984
&gt; 133  0.73783039  0.9282907609
&gt; 134 -0.53755070  0.2789201420
&gt; 135  0.73783039  0.1861529107
&gt; 136  0.93811009 -1.0210579922
&gt; 137  0.73783039  1.1138252234
&gt; 138 -0.69777446  0.5572218358
&gt; 139  0.41738288  0.5572218358
&gt; 140  0.65771852 -0.8355235296
&gt; 141  0.73783039  1.1138252234
&gt; 142 -0.53755070  1.0210579922
&gt; 143  0.53755070  0.6499890671
&gt; 144  0.85799821 -1.0210579922
&gt; 145  0.77788633  1.2065924547
&gt; 146 -0.57760664  1.0210579922
&gt; 147  0.49749476  0.6499890671
&gt; 148  0.57760664 -0.7427562984
&gt; 149  0.65771852  1.0210579922
&gt; 150 -0.53755070  0.5572218358
```

---

# PCA en R: con prcomp

Dentro de los paquete básicos cargados por `R` tenemos `prcomp` que nos permite realizar los cálculos anteriores de manera automática (`scale. = TRUE` debe ser indicado si los han datos no entran estandarizados previamente).


```r
pca &lt;- prcomp(iris_bi, scale. = TRUE)
pca
```

```
&gt; Standard deviations (1, .., p=2):
&gt; [1] 1.4010230 0.1927033
&gt; 
&gt; Rotation (n x k) = (2 x 2):
&gt;                    PC1        PC2
&gt; Petal.Length 0.7071068 -0.7071068
&gt; Petal.Width  0.7071068  0.7071068
```

* **Rotation**: la matriz cuyas columnas son las componentes principales `\(\boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2\)` (recuerda que dijimos que estábamos «rotando» los datos).

* **Standard deviations**: dado que cada `\(\lambda_j = {\rm Var} \left(\boldsymbol{\Phi}_j \right)\)` representa la varianza de las componentes principales, lo que nos proporciona la salida es `\(\sqrt{\lambda_j}\)`, para cada `\(j=1,\ldots,p\)`


```r
pca$sdev^2 # autovalores
```

```
&gt; [1] 1.96286543 0.03713457
```

---

# PCA en R: con prcomp


```r
pca &lt;- prcomp(iris_bi, scale. = TRUE)
pca
```

```
&gt; Standard deviations (1, .., p=2):
&gt; [1] 1.4010230 0.1927033
&gt; 
&gt; Rotation (n x k) = (2 x 2):
&gt;                    PC1        PC2
&gt; Petal.Length 0.7071068 -0.7071068
&gt; Petal.Width  0.7071068  0.7071068
```

La **primera componente** viene definida como

`$$\boldsymbol{\Phi}_1 = 0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$` 

La **segunda componente** viene definida como 

`$$\boldsymbol{\Phi}_2 = -0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$`


---

# PCA en R: con prcomp

.pull-left[

En `pca$x` quedan guardados los **scores** o nuevas coordenadas de nuestros datos


```r
as_tibble(pca$x)
```

```
&gt; # A tibble: 150 × 2
&gt;      PC1     PC2
&gt;    &lt;dbl&gt;   &lt;dbl&gt;
&gt;  1 -1.87  0.0175
&gt;  2 -1.87  0.0175
&gt;  3 -1.91  0.0575
&gt;  4 -1.83 -0.0226
&gt;  5 -1.87  0.0175
&gt;  6 -1.57  0.0828
&gt;  7 -1.78  0.110 
&gt;  8 -1.83 -0.0226
&gt;  9 -1.87  0.0175
&gt; 10 -1.92 -0.115 
&gt; # … with 140 more rows
```

]

.pull-right[

También podemos calcularlas nosotros mismos **proyectando los datos en las nuevas componentes**


```r
as_tibble(as.matrix(iris_bi_std) %*%
            pca$rotation)
```

```
&gt; # A tibble: 150 × 2
&gt;      PC1     PC2
&gt;    &lt;dbl&gt;   &lt;dbl&gt;
&gt;  1 -1.87  0.0175
&gt;  2 -1.87  0.0175
&gt;  3 -1.91  0.0575
&gt;  4 -1.83 -0.0226
&gt;  5 -1.87  0.0175
&gt;  6 -1.57  0.0828
&gt;  7 -1.78  0.110 
&gt;  8 -1.83 -0.0226
&gt;  9 -1.87  0.0175
&gt; 10 -1.92 -0.115 
&gt; # … with 140 more rows
```

]

---

# Visualizando la transformación

.pull-left[

&lt;img src="index_files/figure-html/unnamed-chunk-52-1.png" width="100%" /&gt;

]

.pull-right[


&lt;img src="index_files/figure-html/unnamed-chunk-53-1.png" width="100%" /&gt;

]

---

# Visualizando la transformación


Si ahora pintamos los datos **codificando el color en función de la especie** podemos darnos cuenta de por qué la primera componente es la que captura prácticamente toda la información.

.pull-left[

&lt;img src="index_files/figure-html/unnamed-chunk-54-1.png" width="100%" /&gt;

]

.pull-right[


&lt;img src="index_files/figure-html/unnamed-chunk-55-1.png" width="100%" /&gt;

]


---

# PCA en R: con factominer y factoextra

Ahora que controlamos un poco cómo se calculan y qué significan, vamos a ampliar al dataset entero de iris `{iris}` con sus **4 variables numéricas**


```r
iris_full &lt;- iris %&gt;% select(-Species)

# Covarianza y correlación sin estandarizar antes
library(corrr)
cov(iris_full)
```

```
&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width
&gt; Sepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707
&gt; Sepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394
&gt; Petal.Length    1.2743154  -0.3296564    3.1162779   1.2956094
&gt; Petal.Width     0.5162707  -0.1216394    1.2956094   0.5810063
```

```r
iris_full %&gt;% correlate(diagonal = 1) %&gt;% fashion()
```

```
&gt;           term Sepal.Length Sepal.Width Petal.Length Petal.Width
&gt; 1 Sepal.Length         1.00        -.12          .87         .82
&gt; 2  Sepal.Width         -.12        1.00         -.43        -.37
&gt; 3 Petal.Length          .87        -.43         1.00         .96
&gt; 4  Petal.Width          .82        -.37          .96        1.00
```

---

# PCA en R: con factominer y factoextra

.pull-left[

Las correlaciones también podemos **visualizarlas** con el paquete `{corrplot}`

Las variables con mayor correlación (positiva además) es entre la longitud y la anchura del pétalo.

]

.pull-right[


```r
library(corrplot)
corrplot(cor(iris_full), type = "upper", tl.col = "black")
```

&lt;img src="index_files/figure-html/unnamed-chunk-57-1.png" width="100%" /&gt;
]

---


# PCA en R: con factominer y factoextra

Con `{FactoMineR}` podemos calcular con `PCA()` de forma muy sencilla, indicándole que de momento no queremos gráficos, que queremos tantas componentes como variables (luego ya decidiremos con cual nos quedamos) y que estandarice los datos (`scale.unit = TRUE`).


```r
library(FactoMineR)
library(factoextra)
pca_fit &lt;-
  PCA(iris_full, scale.unit = TRUE,
      ncp = ncol(iris_full), graph = FALSE)
```

--

Para mostrar los autovalores basta con `pca_fit$eig` (ya nos los da ordenados y con la varianza explicada, tanto componente a componente como acumulada). También se obtienen con `get_eig(pca_fit)`


```r
pca_fit$eig # Alternativa: get_eig(pca_fit)
```

```
&gt;        eigenvalue percentage of variance cumulative percentage of variance
&gt; comp 1 2.91849782             72.9624454                          72.96245
&gt; comp 2 0.91403047             22.8507618                          95.81321
&gt; comp 3 0.14675688              3.6689219                          99.48213
&gt; comp 4 0.02071484              0.5178709                         100.00000
```

---

# PCA en R: con factominer y factoextra

En `pca_fit$svd$V` se guardan los **autovectores o componentes principales (de nuevo por columnas)** asociados a los autovalores que ya tenemos ordenados


```r
pca_fit$svd$V
```

```
&gt;            [,1]       [,2]       [,3]       [,4]
&gt; [1,]  0.5210659 0.37741762 -0.7195664 -0.2612863
&gt; [2,] -0.2693474 0.92329566  0.2443818  0.1235096
&gt; [3,]  0.5804131 0.02449161  0.1421264  0.8014492
&gt; [4,]  0.5648565 0.06694199  0.6342727 -0.5235971
```

Además con `pca_fit$var$contrib` nos muestra en **porcentaje lo que aporta cada variable** a la varianza explicada por cada componente (la suma de cada columna es el 100%)


```r
pca_fit$var$contrib
```

```
&gt;                  Dim.1       Dim.2     Dim.3     Dim.4
&gt; Sepal.Length 27.150969 14.24440565 51.777574  6.827052
&gt; Sepal.Width   7.254804 85.24748749  5.972245  1.525463
&gt; Petal.Length 33.687936  0.05998389  2.019990 64.232089
&gt; Petal.Width  31.906291  0.44812296 40.230191 27.415396
```

---

# PCA en R: con factominer y factoextra


Así que darían expresadas los **loadings de las nuevas componentes principales** en función de las variables originales (estandarizadas `\(^*\)`)


|  Phi_1| Phi_2|  Phi_3|  Phi_4|
|------:|-----:|------:|------:|
|  0.521| 0.377| -0.720| -0.261|
| -0.269| 0.923|  0.244|  0.124|
|  0.580| 0.024|  0.142|  0.801|
|  0.565| 0.067|  0.634| -0.524|


`$$\boldsymbol{\Phi}_1 = 0.521 * Sepal.Length^* - 0.269 * Sepal.Width^* + 0.580 * Petal.Length^*  + 0.565 * Petal.Width^*$$`

`$$\boldsymbol{\Phi}_2 = 0.377 * Sepal.Length^*  + 0.923 * Sepal.Width^*  + 0.024 * Petal.Length^*  + 0.067 * Petal.Width^*$$`

`$$\boldsymbol{\Phi}_3 = -0.719 * Sepal.Length^*  + 0.244 * Sepal.Width^*  + 0.142 * Petal.Length^*  + 0.634 * Petal.Width^*$$`

`$$\boldsymbol{\Phi}_4 = -0.261 * Sepal.Length^*  + 0.124 * Sepal.Width^*  + 0.801 * Petal.Length^*  - 0.524 * Petal.Width^*$$`

---


# PCA en R: con factominer y factoextra

En `pca_fit$ind$coord` tenemos guardados los **scores**, las **nuevas coordenadas de los datos** (los **datos proyectados** en las nuevas direcciones).


```r
pca_scores &lt;- as_tibble(pca_fit$ind$coord)
names(pca_scores) &lt;- c("PC_1", "PC_2", "PC_3", "PC_4")
pca_scores # Nuevas coordenadas
```

```
&gt; # A tibble: 150 × 4
&gt;     PC_1    PC_2    PC_3     PC_4
&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
&gt;  1 -2.26  0.480  -0.128  -0.0242 
&gt;  2 -2.08 -0.674  -0.235  -0.103  
&gt;  3 -2.36 -0.342   0.0442 -0.0284 
&gt;  4 -2.30 -0.597   0.0913  0.0660 
&gt;  5 -2.39  0.647   0.0157  0.0359 
&gt;  6 -2.08  1.49    0.0270 -0.00661
&gt;  7 -2.44  0.0476  0.335   0.0368 
&gt;  8 -2.23  0.223  -0.0887  0.0246 
&gt;  9 -2.33 -1.12    0.145   0.0269 
&gt; 10 -2.18 -0.469  -0.254   0.0399 
&gt; # … with 140 more rows
```

---

# PCA en R: con factominer y factoextra


Podemos también calcular las **covarianzas entre cada componente principal y las variables originales**, tal que `\({\rm Cov}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \lambda_i z_{i,j}\)`, donde `\(z_{i,j}\)` es el coeficiente `\(j\)`-ésimo de la componente `\(i\)`-ésima (el peso de la variable `\(\boldsymbol{x}_j\)` en la componente `\(\boldsymbol{\Phi}_i\)`)

La correlación será calculada como `\({\rm Cor}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \frac{{\rm Cov}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j )}{s_{\boldsymbol{\Phi}_i} s_{\boldsymbol{x}_j}} = \frac{\lambda_i z_{i,j}}{\sqrt{\lambda_i} s_{\boldsymbol{x}_j}} = \frac{\sqrt{\lambda_i}}{s_{x_j}} z_{i,j}\)`

Dichas correlaciones las tenemos guardadas en `pca_fit$var$cor` y representan las **coordenadas de cada variable en cada componente** 


```r
pca_fit$var$cor
```

```
&gt;                   Dim.1      Dim.2       Dim.3       Dim.4
&gt; Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602
&gt; Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631
&gt; Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978
&gt; Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950
```

---


# PCA en R: con factominer y factoextra

.pull-left[

En `pca_fit$var$cos2` tenemos las **correlaciones al cuadrado**, que expresan la **proporción de varianza de cada variable explicada por cada componente**


```r
round(pca_fit$var$cos2, 3)
```

```
&gt;              Dim.1 Dim.2 Dim.3 Dim.4
&gt; Sepal.Length 0.792 0.130 0.076 0.001
&gt; Sepal.Width  0.212 0.779 0.009 0.000
&gt; Petal.Length 0.983 0.001 0.003 0.013
&gt; Petal.Width  0.931 0.004 0.059 0.006
```

Con `fviz_pca_var` podemos **visualizar de forma bidimensional** como se relacionan las variables originales con las dos componentes que mayor cantidad de varianza capturan.

La **primera componente captura** sobre todo las **dos variables del pétalo** (dichas variables prácticamente están sobre la horizontal de la primera componente). La **segunda componente** captura ligeramente el sépalo, aunque longitud del sépalo es la peor variable representada de todas.

]

.pull-right[


```r
col &lt;- c("#00AFBB", "#E7B800", "#FC4E07")
fviz_pca_var(pca_fit, col.var = "cos2",
             gradient.cols = col,
             repel = TRUE) +
  theme_minimal() + 
  labs(title = "Coordenadas de las variables",
       color = "Prop. var. explicada")
```

&lt;img src="index_files/figure-html/unnamed-chunk-66-1.png" width="93%" /&gt;
]



---


# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_cos2()` podemos mostrar el **porcentaje de la varianza de las variables que es explicada** por las componentes que le indiquemos en `axes`

Así podemos apreciar que todas las 
**dos primeras componentes ya son capaces de capturar** al menos el 75% de la varianza de todas y cada una de las variables, rozando el 100% en las variables `Sepal.Width` y `Petal.Length`

]

.pull-right[


```r
fviz_cos2(pca_fit, choice = "var",
          axes = 1:2)
```

&lt;img src="index_files/figure-html/unnamed-chunk-67-1.png" width="93%" /&gt;
]

---

# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_eig()` podemos visualizar la varianza explicada por cada componente


```r
fviz_eig(pca_fit,
         barfill = "darkolivegreen",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-69-1.png" width="100%" /&gt;

]

---

# PCA en R: con factominer y factoextra



.pull-left[

También podemos visualizar la **varianza acumulada** de forma manual


```r
cumvar &lt;- as_tibble(pca_fit$eig)
names(cumvar) &lt;- c("lambda", "var", "cumvar")

ggplot(cumvar, aes(x = 1:4, y = cumvar)) +
  geom_col(fill = "pink") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-71-1.png" width="100%" /&gt;

]

---

# PCA en R: con factominer y factoextra

.pull-left[

Por último `fviz_pca_biplot()` nos permite visualizar en las dos dimensiones que más varianza capturan, e incluso nos permite **visualizar clústers** de observaciones con las elipses definidas por las matrices de covarianza de cada uno de los grupos.


```r
fviz_pca_biplot(pca_fit,
                col.ind = iris$Species,
                palette = "jco",
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE,
                legend.title = "Especies")
```

Observamos de nuevo como la componente determinante es la primera, que nos discrimina perfectamente la especie de Setosa.

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-73-1.png" width="100%" /&gt;

]

---

# PCA con tidymodels

Por último, ahora que entendemos las componentes principales vamos a calcularlas haciendo uso de `{tidymodels}`, un conjunto de paquetes y herramientas para obtener un **único flujo de trabajo en el preprocesamiento, modelización y evaluación** de modelos.



&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://www.tmwr.org/premade/data-science-model.svg" alt="Infografía extraída de https://www.tmwr.org/dimensionality.html" width="75%" /&gt;
&lt;p class="caption"&gt;Infografía extraída de https://www.tmwr.org/dimensionality.html&lt;/p&gt;
&lt;/div&gt;


```r
library(tidymodels)
```

---


# Filosofía tidymodels

La idea detrás de `{tidymodels}` es **tratar por separado** la depuración y preparación de los datos, el modelo o paradigma de aprendizaje que se quiere aplicar, la optimización de los parámetros de dicho modelo, el ajuste, la evaluación y la predicción correspondiente, **creando un flujo de trabajo muy flexible**. La **filosofía es la misma que hay detrás de cocinar un plato**:

1. Primero **escribimos la RECETA**, una lista de pasos e instrucciones.
2. Después **preparamos HERRAMIENTAS y utensilios para cocinar** (nuestro modelo).
3. Con la **RECETA + HERRAMIENTAS** podemos cocinar el plato muchas veces, con **distintos lotes de ingredientes (datos)**.

&amp;nbsp;

&lt;img src="https://www.tmwr.org/premade/recipes-process.svg" width="85%" style="display: block; margin: auto;" /&gt;


---

# Ejemplo tidymodels: algoritmo knn

.pull-left[

Los datos utilizados forman parte de un cojunto de reservas de hotel elaborado por [Antonio et al., 2019](https://www.sciencedirect.com/science/article/pii/S2352340918315191?via%3Dihub). El **conjunto utilizado tiene 50 000 registros de reservas**, con algunas de las siguientes [variables](https://linkinghub.elsevier.com/retrieve/pii/S2352340918315191)

* `hotel`: nombre del hotel.
* `lead_time`: lapso de tiempo entre la reserva y la estancia.
* `children`: si la reserva tiene niños o no.
* `meal`: régimen de comidas.
* `country`: país de origen.
* `average_daily_rate`: tarifa media diaria.
* `arrival_date`: fecha de llegada.

]

.pull-right[

El **objetivo es predecir si una serva incluye niños/as o no**.


```r
hoteles_raw &lt;- read_csv("./DATOS/hotels.csv")
glimpse(hoteles_raw)
```

```
&gt; Rows: 50,000
&gt; Columns: 23
&gt; $ hotel                          &lt;chr&gt; "City_Hotel", "City_Hotel", "Resort_Hot…
&gt; $ lead_time                      &lt;dbl&gt; 217, 2, 95, 143, 136, 67, 47, 56, 80, 6…
&gt; $ stays_in_weekend_nights        &lt;dbl&gt; 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, …
&gt; $ stays_in_week_nights           &lt;dbl&gt; 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, …
&gt; $ adults                         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, …
&gt; $ children                       &lt;chr&gt; "none", "none", "none", "none", "none",…
&gt; $ meal                           &lt;chr&gt; "BB", "BB", "BB", "HB", "HB", "SC", "BB…
&gt; $ country                        &lt;chr&gt; "DEU", "PRT", "GBR", "ROU", "PRT", "GBR…
&gt; $ market_segment                 &lt;chr&gt; "Offline_TA/TO", "Direct", "Online_TA",…
&gt; $ distribution_channel           &lt;chr&gt; "TA/TO", "Direct", "TA/TO", "TA/TO", "D…
&gt; $ is_repeated_guest              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&gt; $ previous_cancellations         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&gt; $ previous_bookings_not_canceled &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&gt; $ reserved_room_type             &lt;chr&gt; "A", "D", "A", "A", "F", "A", "C", "B",…
&gt; $ assigned_room_type             &lt;chr&gt; "A", "K", "A", "A", "F", "A", "C", "A",…
&gt; $ booking_changes                &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&gt; $ deposit_type                   &lt;chr&gt; "No_Deposit", "No_Deposit", "No_Deposit…
&gt; $ days_in_waiting_list           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
&gt; $ customer_type                  &lt;chr&gt; "Transient-Party", "Transient", "Transi…
&gt; $ average_daily_rate             &lt;dbl&gt; 80.75, 170.00, 8.00, 81.00, 157.60, 49.…
&gt; $ required_car_parking_spaces    &lt;chr&gt; "none", "none", "none", "none", "none",…
&gt; $ total_of_special_requests      &lt;dbl&gt; 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, …
&gt; $ arrival_date                   &lt;date&gt; 2016-09-01, 2017-08-25, 2016-11-19, 20…
```

]

---

# Ejemplo tidymodels: algoritmo knn


### **Muestreo inicial**

Una de las primeras acciones que quizás queremos realizar es un **muestreo inicial** de los datos

* **Muestreo inicial**: `slice_sample` realiza un **muestreo aleatorio**, indicándole en `prop` el porcentaje de datos a extraer (**reminder**: los que no selecciones no continuan a lo largo del flujo).



```r
# Sample inicial: nos quedamos el 15% de los datos
hoteles &lt;- hoteles_raw %&gt;% slice_sample(prop = 0.15)
dim(hoteles_raw)
```

```
&gt; [1] 50000    23
```

```r
dim(hoteles)
```

```
&gt; [1] 7500   23
```

---

# Ejemplo tidymodels: algoritmo knn


### **Partición train/test**

Otra acción habitual será obtener varios **subconjuntos**:

* `train`: para entrenar el algoritmo.
* `validation`: para validar el modelo (quizás haya que volver a `train`)
* `test`: para puntuar nuestro algoritmo (con datos que no han sido usados ni para entrenar ni para validar)

Dado que **tenemos muy pocos 1's en nuestra variable objetivo**, debemos intentar preservar los máximos posibles (para que no nos quede un conjunto aún más desbalanceado).


```r
# Objetivo: predecir si la reserva tiene niños o no
hoteles_raw %&gt;%
  count(children) %&gt;%
  mutate(porc = 100 * n / sum(n))
```

```
&gt; # A tibble: 2 × 3
&gt;   children     n  porc
&gt;   &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;
&gt; 1 children  4038  8.08
&gt; 2 none     45962 91.9
```

---

# Ejemplo tidymodels: algoritmo knn


### **Partición train/test**

Realizaremos una **partición train-test** estratificado por la variable `children`: repartiremos en **80-20%** los datos **PERO asegurándonos de que la proporción de 0's-1's se preserva**.


```r
# Partición 80-20%: estratificada para que mantenga 1/0
hoteles_split &lt;- initial_split(hoteles, strata = children, prop = 0.8)
hoteles_split
```

```
&gt; &lt;Analysis/Assess/Total&gt;
&gt; &lt;6000/1500/7500&gt;
```

&amp;nbsp;

En `hoteles_split` tenemos **guardada las instrucciones** de lo que queremos hacer cuando lo apliquemos, pero hasta que no se aplique no realiza ninguna acción.


```r
hotel_train &lt;- training(hoteles_split)
hotel_test &lt;- testing(hoteles_split)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**

* Asignar roles a variables
* Recategorizar
* Procesar outliers
* Datos ausentes
* Fechas
* Estandarización por rango y/o normalización (tipificación)

La lista de arriba son **algunas de las opciones que quizás querramos hacer** con nuestras variables antes de poder aplicar un modelo.

&amp;nbsp;

Lo que haremos será indicarle una **lista de instrucciones**, algo así como la **receta escrita que tenemos guardada en un cajón** para preparar un plato. La receta por sí sola no se pone a cocinarte menús, simplemente es una lista de instrucciones, lista para cuando la necesites.

Las funciones que empiezan por `step_` tienen implementadas muchas de las funcionalidades que podemos realizar en depuración con `{tidyverse}`. La diferencia al incluirlo en la receta es que **se ejecutará cada vez que dicha receta se aplique (tanto a train como a test)**.

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor))
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), añadir rol amén del existente (`add_role()`), eliminar rol (`remove_role()`)


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel") 
```


---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), añadir rol amén del existente (`add_role()`), eliminar rol (`remove_role()`).
* **Recategorizar variables**: vamos a pasar a binaria `is_repeated_guest` y convertiremos a factor `previous_cancellations`, `reserved_room_type`, `assigned_room_type`, `booking_changes`, `deposit_type` y `days_in_waiting_list`. Con `step_other()` además le vamos a indicar que si en una **variable cualitativa hay algún nivel en proporciones ínfimas (por ejemplo, representa menos del 0.5% del total) las reagrupe** en una categoría llamada «others».


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel") %&gt;%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %&gt;%
  # Indicamos % mínimo en categoría (si &lt; 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), añadir rol amén del existente (`add_role()`), eliminar rol (`remove_role()`).
* **Recategorizar variables**.
* **Estandarizar por rango** con `step_range()` (siempre entre 0 y 1, aunque podríamos asignarle otro rango dando valores a `max` y `min`). Con `step_normalize()` normalizaríamos (media nula y varianza unitaria).



```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel")  %&gt;%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %&gt;%
  # Indicamos % mínimo en categoría (si &lt; 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %&gt;% 
  step_range(all_numeric_predictors(), min = 0, max = 1)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**.
* **Recategorizar variables**.
* **Estandarizar por rango**.
* **Rebalancear objetivo**: en nuestros datos tan solo tenemos cerca del 8% de registros con `children = children` (1's). Usaremos el **sobremuestreo**: duplicar algunos registros con 1 en la variable objetivo para que **equilibrar** la proporción de 0's vs 1's. Para ello basta con añadir en nuestra receta el paso `step_upsample` del paquete `{themis}`, indicándole en `over_ratio` la relación que queremos entre la clase minoritaria y la mayoritaria.



```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel")  %&gt;%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %&gt;%
  # Indicamos % mínimo en categoría (si &lt; 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %&gt;% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;%
  themis::step_upsample(children, over_ratio = 0.5)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo). **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**. **Recategorizar variables**.
* **Estandarizar** por rango. **Rebalancear** objetivo.
* **Tratar datos atípicos y missings**.


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel")  %&gt;%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %&gt;%
  # Indicamos % mínimo en categoría (si &lt; 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %&gt;% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;%
  themis::step_upsample(children, over_ratio = 0.5) %&gt;%
  step_mutate_at(all_numeric_predictors(),
                 fn = function(x) { ifelse(abs(x - mean(x, na.rm = TRUE)) &gt; 5 * sd(x, na.rm = TRUE), NA, x)}) %&gt;%
  # Imputamos por la media las numéricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %&gt;%
  step_impute_mode(all_nominal_predictors())
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` será nuestra outcome (variable objetivo). **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**. **Recategorizar variables**.
* **Estandarizar** por rango. **Rebalancear** objetivo.
* **Tratar datos atípicos y missings**. Convertimos **cualitativas a dummy**.


```r
receta &lt;-
  recipe(data = hotel_train, children ~ .) %&gt;% 
  step_mutate(across(where(is.character), as.factor)) %&gt;%
  add_role(hotel, new_role = "type_hotel")  %&gt;%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %&gt;%
  # Indicamos % mínimo en categoría (si &lt; 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %&gt;% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;%
  themis::step_upsample(children, over_ratio = 0.5) %&gt;%
  step_mutate_at(all_numeric_predictors(),
                 fn = function(x) { ifelse(abs(x - mean(x, na.rm = TRUE)) &gt; 5 * sd(x, na.rm = TRUE), NA, x)}) %&gt;%
  # Imputamos por la media las numéricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %&gt;%
  step_impute_mode(all_nominal_predictors()) %&gt;% 
  # nominales las pasemos a dummy 1/0 (country también)
  step_dummy(all_nominal(), -all_outcomes())
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


```r
receta
```

```
&gt; Recipe
&gt; 
&gt; Inputs:
&gt; 
&gt;        role #variables
&gt;     outcome          1
&gt;   predictor         22
&gt;  type_hotel          1
&gt; 
&gt; Operations:
&gt; 
&gt; Variable mutation for across(where(is.character), as.factor)
&gt; Variable mutation for (is_repeated_guest == 1), across(previous_...
&gt; Collapsing factor levels for all_nominal_predictors()
&gt; Range scaling to [0,1] for all_numeric_predictors()
&gt; $terms
&gt; &lt;list_of&lt;quosure&gt;&gt;
&gt; 
&gt; [[1]]
&gt; &lt;quosure&gt;
&gt; expr: ^children
&gt; env:  0x7ff1374153d8
&gt; 
&gt; 
&gt; $over_ratio
&gt; [1] 0.5
&gt; 
&gt; $ratio
&gt; [1] NA
&gt; 
&gt; $role
&gt; [1] NA
&gt; 
&gt; $trained
&gt; [1] FALSE
&gt; 
&gt; $column
&gt; NULL
&gt; 
&gt; $target
&gt; [1] NA
&gt; 
&gt; $skip
&gt; [1] TRUE
&gt; 
&gt; $id
&gt; [1] "upsample_2Zkur"
&gt; 
&gt; $seed
&gt; [1] 42898
&gt; 
&gt; attr(,"class")
&gt; [1] "step_upsample" "step"         
&gt; Variable mutation for all_numeric_predictors()
&gt; Mean imputation for all_numeric_predictors()
&gt; Mode imputation for all_nominal_predictors()
&gt; Dummy variables from all_nominal(), -all_outcomes()
```

---

# Ejemplo tidymodels: algoritmo knn

### **Horneado (bake)**

Tras escribir la receta vamos a **prepararla** y a **hornear los datos** con `bake()`: para hornearla en el conjunto de train, basta con poner `new_data = NULL`.


```r
# Aplicado a train
bake(receta %&gt;% prep(), new_data = NULL)
```

```
&gt; # A tibble: 8,295 × 72
&gt;    lead_time stays_in_weekend_nights stays_in_week_nigh… adults is_repeated_gue…
&gt;        &lt;dbl&gt;                   &lt;dbl&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;           
&gt;  1   0.0535                   0.143               0.2      0.25 FALSE           
&gt;  2   0                        0                   0.0286   0.5  FALSE           
&gt;  3   0.00185                  0.143               0.0286   0.5  FALSE           
&gt;  4   0.0554                   0.0714              0.0571   0.5  FALSE           
&gt;  5   0.0554                   0.0714              0.0857   0.5  FALSE           
&gt;  6   0.116                    0                   0.0286   0.5  FALSE           
&gt;  7   0                        0                   0.0286   0.25 FALSE           
&gt;  8   0.0867                   0                   0.0286   0.5  FALSE           
&gt;  9   0.0793                   0                   0.114    0.5  FALSE           
&gt; 10   0.0185                   0                   0.0286   0.5  FALSE           
&gt; # … with 8,285 more rows, and 67 more variables: average_daily_rate &lt;dbl&gt;,
&gt; #   total_of_special_requests &lt;dbl&gt;, arrival_date &lt;date&gt;, children &lt;fct&gt;,
&gt; #   hotel_Resort_Hotel &lt;dbl&gt;, meal_HB &lt;dbl&gt;, meal_SC &lt;dbl&gt;,
&gt; #   meal_Undefined &lt;dbl&gt;, meal_other &lt;dbl&gt;, country_BEL &lt;dbl&gt;,
&gt; #   country_BRA &lt;dbl&gt;, country_CHE &lt;dbl&gt;, country_CHN &lt;dbl&gt;, country_CN &lt;dbl&gt;,
&gt; #   country_DEU &lt;dbl&gt;, country_ESP &lt;dbl&gt;, country_FRA &lt;dbl&gt;, country_GBR &lt;dbl&gt;,
&gt; #   country_IRL &lt;dbl&gt;, country_ISR &lt;dbl&gt;, country_ITA &lt;dbl&gt;, …
```

---

# Ejemplo tidymodels: algoritmo knn

### **Horneado (bake)**

Para hornearla en el conjunto de test basta con poner `new_data = hotel_test`. Nuestra receta, aplicada a nuestros ingredientes, está lista. Este «horneado» con `bake()` solo lo necesitamos **si queremos ya aplicar la receta a nuestros datos**.


```r
bake(receta %&gt;% prep(), new_data = hotel_test)
```

```
&gt; # A tibble: 1,500 × 72
&gt;    lead_time stays_in_weekend_nights stays_in_week_nigh… adults is_repeated_gue…
&gt;        &lt;dbl&gt;                   &lt;dbl&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;           
&gt;  1   0.00185                  0.0714              0.0286   0.5  FALSE           
&gt;  2   0.0332                   0.0714              0.0286   0.5  FALSE           
&gt;  3   0.0387                   0.0714              0.0857   0.5  FALSE           
&gt;  4   0.00738                  0.143               0.0857   0.5  FALSE           
&gt;  5   0.00369                  0                   0.0286   0.25 FALSE           
&gt;  6   0.393                    0.0714              0.0286   0.5  FALSE           
&gt;  7   0.0627                   0.143               0.143    0.5  FALSE           
&gt;  8   0.0480                   0                   0.0286   0.25 FALSE           
&gt;  9   0.0185                   0.143               0.0857   0.5  FALSE           
&gt; 10   0.415                    0.0714              0.0857   0.25 FALSE           
&gt; # … with 1,490 more rows, and 67 more variables: average_daily_rate &lt;dbl&gt;,
&gt; #   total_of_special_requests &lt;dbl&gt;, arrival_date &lt;date&gt;, children &lt;fct&gt;,
&gt; #   hotel_Resort_Hotel &lt;dbl&gt;, meal_HB &lt;dbl&gt;, meal_SC &lt;dbl&gt;,
&gt; #   meal_Undefined &lt;dbl&gt;, meal_other &lt;dbl&gt;, country_BEL &lt;dbl&gt;,
&gt; #   country_BRA &lt;dbl&gt;, country_CHE &lt;dbl&gt;, country_CHN &lt;dbl&gt;, country_CN &lt;dbl&gt;,
&gt; #   country_DEU &lt;dbl&gt;, country_ESP &lt;dbl&gt;, country_FRA &lt;dbl&gt;, country_GBR &lt;dbl&gt;,
&gt; #   country_IRL &lt;dbl&gt;, country_ISR &lt;dbl&gt;, country_ITA &lt;dbl&gt;, …
```

---

# Ejemplo tidymodels: algoritmo knn

### **Utensilios (modelo)**

Una vez que tenemos nuestra lista de instrucciones, lo siguiente que haríamos al **cocinar un plato** es buscar los utensilios: cuchillos, cacerolas, batidora, etc. En nuestro caso los **utensilios serán nuestro modelo (en este caso de clasificación)**, con `nearest_neighbor()` (echa un vistazo a los modelos del paquete `{parsnip}`). 

- `mode`: admite dos opciones, `mode = "classification"` o `mode = "regression"`.
- `neighbors`: el número de vecinos `k` que consideramos como entorno de vecindad.
- `weight_func`: función (kernel) para promedir distancias (`weight_func = "inv"` nos promedia por el inverso de la distancia; ver opciones en &lt;https://epub.ub.uni-muenchen.de/1769/&gt;)
- `dist_power`: número `\(p\)` de la distancia de Minkowski


```r
knn_model &lt;-
  nearest_neighbor(mode = "classification", neighbors = 10,
                   weight_func = "inv", dist_power = 2) %&gt;%
  set_engine("kknn") # el «motor» que realiza el ajuste
knn_model
```

```
&gt; K-Nearest Neighbor Model Specification (classification)
&gt; 
&gt; Main Arguments:
&gt;   neighbors = 10
&gt;   weight_func = inv
&gt;   dist_power = 2
&gt; 
&gt; Computational engine: kknn
```

---

# Ejemplo tidymodels: algoritmo knn

### **Flujo de trabajo**


Con dichos ingredientes podemos crear ya un **flujo de trabajo** con `workflow()`


```r
# Flujo de trabajo
hoteles_wflow &lt;-
  workflow() %&gt;%
  add_recipe(receta) %&gt;%
  add_model(knn_model)
hoteles_wflow
```

```
&gt; ══ Workflow ════════════════════════════════════════════════════════════════════
&gt; Preprocessor: Recipe
&gt; Model: nearest_neighbor()
&gt; 
&gt; ── Preprocessor ────────────────────────────────────────────────────────────────
&gt; 9 Recipe Steps
&gt; 
&gt; • step_mutate()
&gt; • step_mutate()
&gt; • step_other()
&gt; • step_range()
&gt; • step_upsample()
&gt; • step_mutate_at()
&gt; • step_impute_mean()
&gt; • step_impute_mode()
&gt; • step_dummy()
&gt; 
&gt; ── Model ───────────────────────────────────────────────────────────────────────
&gt; K-Nearest Neighbor Model Specification (classification)
&gt; 
&gt; Main Arguments:
&gt;   neighbors = 10
&gt;   weight_func = inv
&gt;   dist_power = 2
&gt; 
&gt; Computational engine: kknn
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicción**

Dichos pasos vamos a **proporcionárselos a nuestro conjunto de entrenamiento** para que nos **aplique el flujo de trabajo** que hemos construido, usando `fit(data = hotel_train)`.


```r
# Aplicamos flujo
library("kknn")
hoteles_knn_fit &lt;-
  hoteles_wflow %&gt;% fit(data = hotel_train)
hoteles_knn_fit
```

```
&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════
&gt; Preprocessor: Recipe
&gt; Model: nearest_neighbor()
&gt; 
&gt; ── Preprocessor ────────────────────────────────────────────────────────────────
&gt; 9 Recipe Steps
&gt; 
&gt; • step_mutate()
&gt; • step_mutate()
&gt; • step_other()
&gt; • step_range()
&gt; • step_upsample()
&gt; • step_mutate_at()
&gt; • step_impute_mean()
&gt; • step_impute_mode()
&gt; • step_dummy()
&gt; 
&gt; ── Model ───────────────────────────────────────────────────────────────────────
&gt; 
&gt; Call:
&gt; kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(10,     data, 5), distance = ~2, kernel = ~"inv")
&gt; 
&gt; Type of response variable: nominal
&gt; Minimal misclassification: 0.02845087
&gt; Best kernel: inv
&gt; Best k: 10
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicción**

Dicho ajuste guardado en `hoteles_knn_fit` podemos usarlo para **predecir el conjunto test** de dos maneras:

* **Clase**
* **Probabilidades** (de pertenencia a dichas clases, necesario para el cálculo de la **curva ROC**)

Ambas se hacen con `predict()` pasándole un argumento de `type` diferente. **Reminder**: al tenerlo todo integrado en un flujo, **aplicará el procesamiento que necesite al conjunto de test**, tal cual lo hemos indicado en las instrucciones.


```r
# Predecir el conjunto test: devuelve la clase
predict(hoteles_knn_fit, hotel_test)
```

```
&gt; # A tibble: 1,500 × 1
&gt;    .pred_class
&gt;    &lt;fct&gt;      
&gt;  1 none       
&gt;  2 none       
&gt;  3 none       
&gt;  4 none       
&gt;  5 none       
&gt;  6 none       
&gt;  7 none       
&gt;  8 none       
&gt;  9 none       
&gt; 10 none       
&gt; # … with 1,490 more rows
```

```r
# Predecir las probabilidades (las necesitamos para la ROC)
predict(hoteles_knn_fit, hotel_test, type = "prob")
```

```
&gt; # A tibble: 1,500 × 2
&gt;    .pred_children .pred_none
&gt;             &lt;dbl&gt;      &lt;dbl&gt;
&gt;  1          0          1    
&gt;  2          0          1    
&gt;  3          0          1    
&gt;  4          0          1    
&gt;  5          0          1    
&gt;  6          0          1    
&gt;  7          0.167      0.833
&gt;  8          0          1    
&gt;  9          0.308      0.692
&gt; 10          0          1    
&gt; # … with 1,490 more rows
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicción**

Dentro del paquete `{parsnip}` que hemos cargado dentro de `{tidymodels}` tenemos a nuestra disposición una función llamada `augment()` que nos permite **incluir en una misma tabla las predicciones de la clase, de las probabilidades y los datos de test originales** (añadiendo columnas).


```r
# Para obtener las probabilidades en los datos (con variables)
prob_test &lt;- augment(hoteles_knn_fit, hotel_test)
print(prob_test, width = Inf)
```

```
&gt; # A tibble: 1,500 × 26
&gt;    hotel        lead_time stays_in_weekend_nights stays_in_week_nights adults
&gt;    &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;  &lt;dbl&gt;
&gt;  1 City_Hotel           1                       1                    1      2
&gt;  2 City_Hotel          18                       1                    1      2
&gt;  3 City_Hotel          21                       1                    3      2
&gt;  4 Resort_Hotel         4                       2                    3      2
&gt;  5 City_Hotel           2                       0                    1      1
&gt;  6 Resort_Hotel       213                       1                    1      2
&gt;  7 Resort_Hotel        34                       2                    5      2
&gt;  8 Resort_Hotel        26                       0                    1      1
&gt;  9 Resort_Hotel        10                       2                    3      2
&gt; 10 Resort_Hotel       225                       1                    3      1
&gt;    children meal  country market_segment distribution_channel is_repeated_guest
&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;                            &lt;dbl&gt;
&gt;  1 children BB    PRT     Direct         Direct                               0
&gt;  2 none     BB    PRT     Groups         TA/TO                                0
&gt;  3 none     SC    FRA     Online_TA      TA/TO                                0
&gt;  4 none     HB    FRA     Online_TA      TA/TO                                0
&gt;  5 none     BB    GBR     Online_TA      TA/TO                                0
&gt;  6 none     HB    DEU     Groups         TA/TO                                0
&gt;  7 none     BB    PRT     Direct         Direct                               0
&gt;  8 none     BB    NULL    Corporate      Corporate                            0
&gt;  9 none     BB    IRL     Online_TA      TA/TO                                0
&gt; 10 none     BB    GBR     Groups         Direct                               0
&gt;    previous_cancellations previous_bookings_not_canceled reserved_room_type
&gt;                     &lt;dbl&gt;                          &lt;dbl&gt; &lt;chr&gt;             
&gt;  1                      0                              0 A                 
&gt;  2                      0                              0 A                 
&gt;  3                      0                              0 A                 
&gt;  4                      0                              0 E                 
&gt;  5                      0                              0 A                 
&gt;  6                      0                              0 A                 
&gt;  7                      0                              0 F                 
&gt;  8                      0                              2 A                 
&gt;  9                      0                              0 A                 
&gt; 10                      0                              0 A                 
&gt;    assigned_room_type booking_changes deposit_type days_in_waiting_list
&gt;    &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;
&gt;  1 D                                0 No_Deposit                      0
&gt;  2 A                                0 No_Deposit                      0
&gt;  3 A                                0 No_Deposit                      0
&gt;  4 F                                1 No_Deposit                      0
&gt;  5 A                                1 No_Deposit                      0
&gt;  6 A                                0 No_Deposit                      0
&gt;  7 F                                0 No_Deposit                      0
&gt;  8 D                                0 No_Deposit                      0
&gt;  9 A                                0 No_Deposit                      0
&gt; 10 A                                2 No_Deposit                      0
&gt;    customer_type   average_daily_rate required_car_parking_spaces
&gt;    &lt;chr&gt;                        &lt;dbl&gt; &lt;chr&gt;                      
&gt;  1 Transient                     75   none                       
&gt;  2 Transient-Party               75   none                       
&gt;  3 Transient                     67.6 none                       
&gt;  4 Transient                    124   none                       
&gt;  5 Transient-Party              103.  none                       
&gt;  6 Transient-Party               85   none                       
&gt;  7 Transient                    256   none                       
&gt;  8 Transient                     48   none                       
&gt;  9 Transient                     76   parking                    
&gt; 10 Transient-Party               60   none                       
&gt;    total_of_special_requests arrival_date .pred_class .pred_children .pred_none
&gt;                        &lt;dbl&gt; &lt;date&gt;       &lt;fct&gt;                &lt;dbl&gt;      &lt;dbl&gt;
&gt;  1                         1 2015-08-08   none                 0          1    
&gt;  2                         0 2015-07-18   none                 0          1    
&gt;  3                         1 2017-01-12   none                 0          1    
&gt;  4                         0 2016-05-08   none                 0          1    
&gt;  5                         0 2016-08-02   none                 0          1    
&gt;  6                         0 2016-05-16   none                 0          1    
&gt;  7                         3 2017-08-24   none                 0.167      0.833
&gt;  8                         0 2015-10-27   none                 0          1    
&gt;  9                         3 2017-04-09   none                 0.308      0.692
&gt; 10                         0 2016-05-26   none                 0          1    
&gt; # … with 1,490 more rows
```

---

# Ejemplo tidymodels: algoritmo knn

### **Evaluación**

En realidad el conjunto de test solo deberíamos usarlo al final del proceso, no como evaluación intermedia de los hiperparámetros, ya que dicho rol le corresponde a un **tercer conjunto de validación**, pero de momento vamos a simplificarlo en train-test.

Una de las formas más sencillas de **evaluar un método de clasificación es con una matriz de confusión**: una matriz que nos cruce las frecuencias de las etiquetas reales frente a las predichas.


```r
# Matriz de confusión: etiqueta real vs etiqueta predicha
conf_mat_test &lt;-
  prob_test %&gt;% conf_mat(truth = children, estimate = .pred_class)

# La guardamos en una tabla
conf_mat_test &lt;- as_tibble(conf_mat_test$table)
conf_mat_test 
```

```
&gt; # A tibble: 4 × 3
&gt;   Prediction Truth        n
&gt;   &lt;chr&gt;      &lt;chr&gt;    &lt;int&gt;
&gt; 1 children   children    62
&gt; 2 none       children    67
&gt; 3 children   none       168
&gt; 4 none       none      1203
```

---


# Ejemplo tidymodels: algoritmo knn

### **Evaluación**

Muchas de las **métricas las podemos obtener automáticamente** de la matriz de confusión.


```r
# Matriz de confusión + resumen: etiqueta real vs etiqueta predicha
metricas &lt;-
  prob_test %&gt;% conf_mat(truth = children, estimate = .pred_class) %&gt;%
  summary()
metricas
```

```
&gt; # A tibble: 13 × 3
&gt;    .metric              .estimator .estimate
&gt;    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
&gt;  1 accuracy             binary         0.843
&gt;  2 kap                  binary         0.264
&gt;  3 sens                 binary         0.481
&gt;  4 spec                 binary         0.877
&gt;  5 ppv                  binary         0.270
&gt;  6 npv                  binary         0.947
&gt;  7 mcc                  binary         0.279
&gt;  8 j_index              binary         0.358
&gt;  9 bal_accuracy         binary         0.679
&gt; 10 detection_prevalence binary         0.153
&gt; 11 precision            binary         0.270
&gt; 12 recall               binary         0.481
&gt; 13 f_meas               binary         0.345
```

---

# PCA con tidymodels

Ahora que hemos visto por encima la idea de `{tidymodels}` vamos a calcular las componentes principales del conjunto `iris` con dicha idea.



```r
iris_ful7 &lt;- iris %&gt;% select(-Species)
library(corrplot)
iris_full %&gt;% cor() %&gt;% 
  corrplot(tl.col = "black", method = "ellipse")
```

&lt;img src="index_files/figure-html/iris-cor-1.png" width="3%" /&gt;

---

# PCA con tidymodels

La receta tendrá los siguientes pasos:

* Indicar la **variable objetivo** (`Species`)
* Imputamos **datos ausentes**
* **Estandarizar/normalizar** los datos
* Eliminamos variables de **cero varianza**


```r
receta &lt;- 
  recipe(Species ~ ., data = iris) %&gt;%
  # Imputamos por la media las numéricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %&gt;%
  step_impute_mode(all_nominal_predictors()) %&gt;%
  # Estandarizamos
  step_normalize(all_numeric_predictors()) %&gt;%
  # Filtro cero varianza
  step_zv(all_numeric_predictors())
receta
```

```
&gt; Recipe
&gt; 
&gt; Inputs:
&gt; 
&gt;       role #variables
&gt;    outcome          1
&gt;  predictor          4
&gt; 
&gt; Operations:
&gt; 
&gt; Mean imputation for all_numeric_predictors()
&gt; Mode imputation for all_nominal_predictors()
&gt; Centering and scaling for all_numeric_predictors()
&gt; Zero variance filter on all_numeric_predictors()
```

---

# PCA con tidymodels

Para añadir el **análisis de componentes principales** basta con añadir `step_pca()`


```r
receta &lt;-
  receta %&gt;%
  step_pca(all_numeric_predictors(), num_comp = 4,
           prefix = "PC") 
```
 
Con el argumento `num_comp = 4` indícamos el número de componentes y con `prefix` el prefijo con el que llamaremos a las nuevas variables

---

# PCA con tidymodels


```r
data_pc &lt;- bake(receta %&gt;% prep(), new_data = NULL)
data_pc
```

```
&gt; # A tibble: 150 × 5
&gt;    Species   PC1     PC2     PC3      PC4
&gt;    &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
&gt;  1 setosa  -2.26 -0.478   0.127   0.0241 
&gt;  2 setosa  -2.07  0.672   0.234   0.103  
&gt;  3 setosa  -2.36  0.341  -0.0441  0.0283 
&gt;  4 setosa  -2.29  0.595  -0.0910 -0.0657 
&gt;  5 setosa  -2.38 -0.645  -0.0157 -0.0358 
&gt;  6 setosa  -2.07 -1.48   -0.0269  0.00659
&gt;  7 setosa  -2.44 -0.0475 -0.334  -0.0367 
&gt;  8 setosa  -2.23 -0.222   0.0884 -0.0245 
&gt;  9 setosa  -2.33  1.11   -0.145  -0.0268 
&gt; 10 setosa  -2.18  0.467   0.253  -0.0398 
&gt; # … with 140 more rows
```

---

# PCA con tidymodels

.pull-left[


```r
ggplot(data_pc %&gt;% select(-Species),
       aes(x = .panel_x, y = .panel_y,
           color = Species, fill = Species)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-93-1.png" width="100%" /&gt;

]


---
 
# PCA con tidymodels

.pull-left[


```r
library(learntidymodels)
receta %&gt;% prep() %&gt;% 
  plot_top_loadings(component_number &lt;= 4, n = 4) + 
  scale_fill_brewer(palette = "Paired")
```

]

.pull-right[

&lt;img src="index_files/figure-html/unnamed-chunk-95-1.png" width="100%" /&gt;

]

---

class: inverse center middle

# BLOQUE II. Análisis clúster

&amp;nbsp;


### [¿Qué es el análisis clúster?](#intro-cluster)

### [Métricas](#metricas)

### [Algoritmos jerárquicos](#jerarquicos)

### [Algoritmos no jerárquicos](#no-jerarquicos)

### [Determinación del número de grupos](#n-grupos)


---

name: intro-cluster
class: center, middle

# ¿Qué es el análisis clúster?

---

# Introducción: ¿análisis clúster?

El **análisis clúster** forma parte de los **algoritmos de agrupación o clasificación** con **aprendizaje no supervisado**:

* **Clasificación supervisada**: los individuos se clasifican en un grupo a partir de la
información de un conjunto de variables observadas de unos inviduos **cuyo grupo conocemos**, los **datos están etiquetados** (sabemos que es acierto y error). Es el caso por ejemplo del **análisis discriminante**.


&lt;img src="./img/supervised_classification.png" width="80%" style="display: block; margin: auto 0 auto auto;" /&gt;


---

# Introducción: ¿análisis clúster?

El **análisis clúster** forma parte de los **algoritmos de agrupación o clasificación** con **aprendizaje no supervisado**:


* **Clasificación/agrupación no supervisada**: los individuos también se clasifican en un grupo a partir de la información de un conjunto de variables observadas PERO en esta ocasión **no sabemos a qué grupo pertenece cada individuo a priori**, no tenemos conocimiento de qué es acierto y qué es error. Es este el caso del **análisis clúster**.


&lt;img src="./img/unsupervised_classification.png" width="80%" style="display: block; margin: auto 0 auto auto;" /&gt;

---

# Introducción: ¿análisis clúster?

### **Objetivo**

El análisis clúster tiene como principal objetivo **encontrar grupos** dentro de los individuos, de forma que los individuos de cada grupo sean **lo más parecidos entre sí** (homogeneidad interna) y **lo más diferentes a los individuos de otros grupos** (heterogeneidad entre grupos)


&amp;nbsp;

### **Notación**

* `\(n\)` tamaño muestral (número de individuos --&gt; filas).

* `\(\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)\)` conjunto de `\(p\)` variables (--&gt; columnas) medidas para cada individuo `\(i=1,\ldots,n\)`.

* Nuestros datos estarán en forma de tabla o matriz `\(\boldsymbol{X}\)` de `\(n\)` filas y `\(p\)` columnas (con `\(p \ll n\)`)
 
---
 
# Introducción: ¿análisis clúster?
 
Los algoritmos los dividiremos en dos grandes grupos:


* **Algoritmos jerárquicos**: algoritmos que tienen como objetivo construir una jerarquía de grupos. Existen principalmente dos estrategias:
  - **Aglomerativa**: cada individuo empieza siendo su propio grupo, mientras se van uniendo de forma secuencial ascendiendo en la jerarquía, hasta acabar con un solo grupo que incluya todas las observaciones.
  - **Divisiva**: se construye una jerarquía descendiente, empezando con un único grupo hasta acabar con un grupo por individuo

Los métodos jerárquicos suelen ser muy **explicativos** y fáciles de visualizar (por ejemplo, con un **dendograma**) pero **muy costoso computacionalmente** (ya que siempre se construye la jerarquía entera, para decidir luego dónde cortar)

&amp;nbsp;

* **Algoritmos no jerárquicos**: dado un número `\(K\)` de grupos fijado a priori, se pretenden agrupar los datos de forma que obtengamos finalmente `\(K\)` agrupaciones de los mismos.

---

name: metricas
class: center, middle

# Métricas


---

# Métricas

En los algoritmos usados será clave el concepto de **métrica**. Los datos serán agrupados en base a los conceptos de «lejos» y «cerca», o mejor dicho, en base a «parecido» y «diferente»

**¿Qué es ser parecido? ¿Y diferente?**

&amp;nbsp;

### **Métrica entre observaciones**

Para medir distancias entre los individuos tenemos principalmente dos alternativas:

* **Distancias geométricas**: distancias que miden la distancia entre dos individuos como si fuesen dos puntos en un espacio geométrico. Son distancias determinísticas.

* **Distancias probabilísticas**: distancias que miden la distancia entre individuos teniendo en cuenta la distribución de las variables y su dependencia.

---

# Métricas determinísticas


* **Distancia euclídea bidimensional**: `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{(x_{i, 1} - x_{j,1})^2 + (x_{i, 2} - x_{j, 2})^2}\)`
   

* **Distancia euclídea multidimensional**: `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}\)`

* **Distancia Manhattan**:  `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \displaystyle \sum_{k=1}^{p} \left| x_{i,k} - x_{j,k} \right|\)`

* **Distancia de Minkowski**: `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left(\displaystyle \sum_{k=1}^{p} \left| x_{i,k} - x_{j,k} \right|^l\right)^{1/l}\)` (si `\(l=1\)` es Manhattan, si `\(l=2\)` es Euclídea)
   
Cuando usemos este tipo métricas es **muy importante** **reescalar por rango**: transformamos para crear nuevas observaciones `\(\widetilde{x}_{i, k} = \frac{x_{i, k} - min(\boldsymbol{x}_i)}{max(\boldsymbol{x}_i) - min(\boldsymbol{x}_i)}\)` de forma que todas las variables estén entre 0 y 1 (y así todas tengan el mismo peso dentro de las métricas).

---

# Métricas determinísticas

&lt;img src="./img/minkowski.png" width="65%" style="display: block; margin: auto;" /&gt;

&lt;img src="./img/minkowski_1.png" width="65%" style="display: block; margin: auto;" /&gt;


---

# Métricas probabilísticas

* **Distancia de Mahalanobis** multidimensional **(variables independientes)**:  `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\frac{x_{i, k} - x_{j,k}}{\sigma_i} \right)^2}\)` que se puede aproximar por `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\frac{x_{i, k} - x_{j,k}}{S_i} \right)^2}\)`

* **Distancia de Mahalanobis** multidimensional **(variables dependientes)**:  `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)^{T} \Sigma^{-1} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)}\)` que se puede aproximar por `\(d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)^{T} S^{-1} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)}\)`, donde `\(\Sigma\)` es la matriz de covarianzas, con `\(S\)` matriz de (cuasi)covarianzas.
  
Cuando usemos este tipo métricas es **muy importante** **estandarizar** para tener variables de media `\(0\)` y varianza `\(1\)`.
   
---

name: jerarquicos
class: center, middle

# Algoritmos jerárquicos

---

name: no-jerarquicos
class: center, middle

# Algoritmos no jerárquicos

---


name: n-grupos
class: center, middle

# Determinación del número de grupos

---


# Recursos y bibliografía

&amp;nbsp;

### **Leyenda de los recursos**

&amp;nbsp;

&amp;nbsp;


#### 📚 **Artículos o libros** científicos que han sido sometidos a revisión por pares.

&amp;nbsp;

#### 🔗 **Recursos online** recomendados

&amp;nbsp;

#### 💻 Recursos para la **programación en R**

---

# Bibliografía general

💻 **Tidy Data Tutor**: para visualizar la mecánica interna de `{tidyverse}`. &lt;https://tidydatatutor.com/&gt;

🔗 Web con recursos para la **introducción a la estadística y Machine Learning en R** &lt;https://artofstat.com/&gt;

📚 **«An Introduction to Multivariate Statistical Analysis»**. Anderson (1958) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/introduction_mva_anderson_2003.pdf&gt;

📚 **«A New Measure of Rank Correlation»**. Kendall (1938) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/correlation_kendall_1938.pdf&gt;

📚 **«The generalised product moment distribution in samples from a normal multivariate population»**. Wishart (1928) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/multivariate_normal_wishart_1928.pdf&gt;

📚 **«On lines and planes of closest fit to systems of points in space»**. Pearson (1901) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/fit_pearson_1901.pdf&gt;


---

# Recursos dataviz

### Dataviz

📚 **«Gramática de las gráficas: pistas para mejorar las representaciones de datos»**. Sevilla (2005) &lt;http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf&gt;

📚 **«Quantitative Graphics in Statistics: A Brief History»**. Beniger and Robyn &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/graphics_beniger_robin_1978.pdf&gt;
 
 
💻 **«Analizando datos, visualizando información, contando historias»** (curso de dataviz en R). Álvarez-Liébana y Valverde-Castilla (2022) &lt;https://dadosdelaplace.github.io/curso-dataviz-ECI-2022&gt;

---

# Bibliografía componentes principales

💻 **Componentes principales** en `{tidymodels}`. &lt;https://www.tmwr.org/dimensionality.html#beans&gt;


📚 **«Principal Component Analysis»**. Jolliffe (2002) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/pca_jolliffe_2002.pdf&gt;

📚 **«Principal Component Analysis»**. Hervé and Lynne (2010) &lt;http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf&gt;

📚 **«Principal Component Analysis: a review and recent developments»**. Jolliffe and Cadima (2016) &lt;https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202&gt;

🔗 **«The Mathematics Behind Principal Component Analysis»**. Dubey (2018).  &lt;https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643&gt;


🔗 **«A One-Stop Shop for Principal Component Analysis»**. Brems (2017). &lt;https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c&gt;

📚 **«On the number of principal components: a test of dimensionality based on measurements of similarity between matrices»**. Dray (2008) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/numer_pca_dray_2008.pdf&gt;


---

# Bibliografía análisis clúster

Multiclass classification of dry beans using computer vision and machine learning techniques
https://www.sciencedirect.com/science/article/abs/pii/S0168169919311573

https://rpubs.com/Joaquin_AR/310338

https://www.tidymodels.org/learn/statistics/k-means/

https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/unsupervised-learning.html#kmeans-clustering

Algorithm AS 136: A K-Means Clustering Algorithm Author(s): J. A. Hartigan and M. A. Wong Reviewed work(s): Source: Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 28, No. 1 (1979), pp. 100-108 Published by: Wiley-Blackwell for the Royal Statistical Society Stable URL: http://www.jstor.org/stable/2346830 .
kmeans_hartigan_wong_1979

https://cimentadaj.github.io/ml_socsci/unsupervised-methods.html

---

# Recursos y bibliografía

### Otras técnicas de reducción de la dimensión

🔗 Sobre **PCA y PLS**. Amat (2017). &lt;https://www.cienciadedatos.net/documentos/35_principal_component_analysis#Introducci%C3%B3n&gt;

📚 **«On the early history of the singular value decomposition»**. Stewart (1993) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/svd_stewart_1993.pdf&gt;

📚 **«UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction»**. McInnes, healy and Melville (2020) &lt;https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/umap_mcinnesetal_2020.pdf&gt;


---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
