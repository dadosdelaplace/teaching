---
title: "T√âCNICAS DE AN√ÅLISIS MULTIVARIANTE"
subtitle: "M√°ster propio (NTIC) en ¬´Big Data y Business Analytics¬ª"
author:
  - "Profesor: Javier √Ålvarez Li√©bana"
institute: "Facultad de Estudios Estad√≠sticos (UCM)"
date: "22/04/2022 - 23/04/2022 (actualizado: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    self_contained: false
    lib_dir: libs
    css: [default, style.css]
    nature:
      # beforeInit: "stylejs.js"
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---


```{r settings, include = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, out.width = "100%", cache = FALSE,
                      comment = ">", echo = TRUE, message = FALSE,
                      warning = FALSE, hiline = TRUE, dpi = 120)

# xaringan Extra
# devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
use_xaringan_extra(c("tile_view", "animate_css", "tachyons"))
use_tile_view() # panel
# xaringanExtra::use_scribble() # scribble
use_extra_styles(hover_code_line = TRUE,
                 mute_unhighlighted_code = FALSE) # Hover triangle code line
use_clipboard( # About clipboard
  button_text = "Click para copiar c√≥digo",
  success_text = "C√≥digo copiado",
  error_text = "Ctrl+C para copiar"
)
use_freezeframe() # restarting gifs
use_animate_all("fade") # animates
use_panelset() # panels 
```

class: inverse center middle

# ATAJOS DE LAS DIAPOSITIVAS



$$\\[2.7in]$$

.left[Pulsa <kbd-black>O</kbd-black> para ver el panel de diapositivas]
.left[Pulsa <kbd-black>H</kbd-black> para ver otros atajos]

---


# Material de las clases


.pull-left[

- **Diapositivas** del curso:
<https://dadosdelaplace.github.io/teaching/pca-clustering>

- **Scripts** del curso:
<https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/scripts>

- **Bibliograf√≠a**: <https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/biblio>

&nbsp;

- **Manual** de R: <https://dadosdelaplace.github.io/courses-intro-R/>

- **Materiales del curso de dataviz** en R: <https://dadosdelaplace.github.io/curso-dataviz-ECI-2022>


]

.pull-right[

```{r material, echo = FALSE,  out.width = "83%", fig.align = "right"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

---

# Me presento: la turra

.pull-left[

```{r echo = FALSE,  out.width = "80%", fig.align = "left"}
knitr::include_graphics("./img/me.jpeg")
``` 

]

.pull-right[

* **Javier √Ålvarez Li√©bana**, nacido en 1989 en Carabanchel Bajo (Madrid)


* No, lo siento, Manolito era de Caranbanchel Alto)


* Licenciado (UCM) en **Matem√°ticas** (Erasmus en Bologna mediante)

* **M√°ster (UCM) en Ingenier√≠a Matem√°tica** (2013-2014)


* **Doctorado en estad√≠stica** por la Universidad de Granada


* Encargado de la **visualizaci√≥n y an√°lisis de datos covid** de la Consejer√≠a de Salud del **Principado de Asturias**

]

&nbsp;

Intentando la **divulgaci√≥n cient√≠fica** por `Twitter (@dadosdelaplace)` e `Instagram (@javieralvarezliebana)`

---

# Objetivos


.pull-left[

El prop√≥sito de estas clases ser√° el tratamiento de **datos multidimensionales**, con tres objetivos principales:

- **Reducci√≥n de la dimensi√≥n**: ¬øtodas las variables (columnas) nos aportan informaci√≥n? ¬øTodas son necesarias? ¬øPodemos transformar las variables para mantener la informaci√≥n de los datos pero reducir la dimensionalidad de los mismos?

- **Visualizaci√≥n**: ¬øcu√°ntas dimensiones podemos incluir en un gr√°fico 2D? ¬øC√≥mo visualizar datos multidimensionales?

- **Encontrar patrones**: ¬øc√≥mo agrupar (clusterizar) los elementos en funci√≥n de sus diferencias y similitudes?

]

.pull-right[

```{r material2, echo = FALSE,  out.width = "120%", fig.align = "left"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

&nbsp;

üìö Estas **diapositivas** han sido elaboradas con el propio `R` haciendo uso del paquete `{xaringan}`
y `{xaringanExtra}`.


---


# Requisitos

Para el presente curso los √∫nicos **requisitos** ser√°n:

1. **Conexi√≥n a internet** (para la descarga de algunos datos y paquetes).

2. **Instalar R**: ser√° nuestro **lenguaje**, nuestro **castellano** para poder ¬´comunicarnos con el ordenador. La descarga la haremos (gratuitamente) desde <https://cran.r-project.org/>

3. **Instalar R Studio**. De la misma manera que podemos escribir castellano en un ordenador, en un Word, en un papel o en un tuit, podemos usar distintos IDE (entornos de desarrollo integrados, nuestro Office), para que el trabajo sea m√°s c√≥modo. Nuestro **Word** para nosotros ser√° **RStudio**.

.left[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/cran-R.jpg" alt = "cran-R" align = "left" width = "500" style = "margin-top: 5vh">
]

.right[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/R-studio.jpg" alt = "RStudio" align = "right" width = "500" style = "margin-top: 5vh;">
]


---

# R vs excel

![](./img/meme_barco.jpg)

---

# Incel vs excel

```{r echo = FALSE, out.width = '85%', fig.align = "center"}
knitr::include_graphics("./img/incel.jpg")
```

---

# Tips de RStudio: modo oscuro

<img src = "https://dadosdelaplace.github.io/courses-ECI-2022/img/menu_1.jpg" alt = "Rstudio" align = "left" width = "300" style = "margin-top: 3vh;margin-right: 2rem;">
<img src = "https://dadosdelaplace.github.io/courses-ECI-2022/img/menu_2.jpg" alt = "Rstudio" align = "left" width = "350" style = "margin-top: 3vh;margin-right: 2rem;">

**Consejo**: cambiar en tu `RStudio` la tonalidad del fondo de tu programa, en tonos oscuros y no blancos.

---

# Datos: de la celda a la tabla

<img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/celdas.jpg" alt = "celdas" align = "center" width = "850" style = "margin-top: 1vh;">


* **Celda**: un **dato individual** de un tipo concreto.
* **Variable**: **concatenaci√≥n de datos** del mismo tipo.
* **Matriz**: **concatenaci√≥n de variables** del **mismo tipo** y longitud.
* **Tabla**: **concatenaci√≥n de variables** de **distinto tipo** pero igual longitud.

---

class: inverse center middle

# BLOQUE I. Selecci√≥n de variables: PCA

&nbsp;


### [¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?](#intro-PCA)

### [Teor√≠a: an√°lisis de componentes principales](#teoria-PCA)

### [Pr√°ctica: PCA en R (visualizaci√≥n)](#practica-PCA)

---

name: intro-PCA
class: center, middle

# ¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?

### **¬øQu√© es el an√°lisis multivariante?**

---

# Breve historia de la estad√≠stica

* Del (neo)lat√≠n ¬´statisticum collegium¬ª: consejo de **Estado**.
* Del alem√°n ¬´statistik¬ª (ciencia del **Estado**, intoducido por G. Achenwall).

&nbsp;

**Origen**: una herramienta para la **administraci√≥n** eficiente del Estado, pero **sin intenci√≥n de comunicar ni de convertir el dato en informaci√≥n**.

---

# Breve historia de la estad√≠stica

.pull-left[

## Primeros usos: elaboraci√≥n de censos

Los **primeros usos** documentados de la estad√≠stica fueron la elaboraci√≥n de **censos** por parte de **mesopot√°micos, chinos y egipcios**, con tres fines:

* Cobrar **impuestos** (un saludo, Willyrex).
* Reparto de **tierras** y optimizaci√≥n de su uso.
* **Reclutamiento de soldados**.

]

.pull-right[

## Estad√≠stica en la guerra

Seg√∫n Tuc√≠dides, conceptos estad√≠sticos como la **moda** datan del **siglo V a.C.**: para asaltar la muralla de la ciudad de Platea, pon√≠an a contar a varios soldados el n√∫mero de ladrillos vistos en la muralla, qued√°ndose con el **conteo m√°s repetido (la moda, el m√°s frecuente)**, permitiendo el c√°lculo de la altura de la muralla.

```{r echo = FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("./img/peloponeso.jpg")
```

]

---

# ¬øQu√© han hecho los romanos por nosotros?

.pull-left[

Precisamente por el tama√±o de su Imperio, fueron los **romanos** quienes hicieron un uso m√°s intenso de la estad√≠stica:

* **Censos** (elaborados por la censura, que elaboraba no solo el censo sino la supervisi√≥n de la moralidad p√∫blica).
* Primeras **tablas de natalidad/mortalidad**
* Primeros **catastros** (registros oficiales de propiedades, primeros impuestos)

```{r echo = FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("./img/catastro.jpg")
```

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "left"}
knitr::include_graphics("https://www.publico.es/tremending/wp-content/uploads/2019/02/lifeofbrian3.jpg")
```

]

---

# Breve historia de la estad√≠stica

.pull-left[

## **√ÅRABES**

Autores de los **primeros tratados de estad√≠stica**, como el manuscrito de **Al-Kindi (801-873)**, que us√≥ la distribuci√≥n de **frecuencias de palabras** para el desarrollo de m√©todos de cifrado y descifrado de **mensajes encriptados**.

]

.pull-right[

## **M√âXICO**

Ya en el **a√±o 1116, el rey X√≥lotl** implement√≥ un **censo** que consist√≠a en la **estimaci√≥n de piedras**, tirando cada s√∫bdito una a un mont√≥n (Nepohualco).
]


&nbsp;

.pull-left[

## **INGLATERRA**

Desde el siglo XII se realiza la **Prueba del Pyx**, considerado uno de los **primeros controles de calidad**: se extre una de las monedas acu√±adas y se deposita en una caja, para un a√±o despu√©s comprobar su calidad y pureza.

]

.pull-right[

## **ITALIA**

En paralelo al **auge de los primeros ¬´sistemas financieros¬ª en Italia**, ¬´La Nuova Cr√≥nica¬ª de G. Villani fue considerado durante mucho tiempo el primer tratado de estad√≠stica (hasta el descubrimiento de los trabajos de Al-Kindi).

]

---

# Navegaci√≥n y astronom√≠a

Y es de aquella √©poca medieval, en la que la navegaci√≥n y la astronom√≠a empezaban a tomar relevancia cient√≠fica, cuando aparece la que se considera la primera gr√°fica (aunque no propiamente estad√≠stica) <sup>1</sup>, representando el **movimiento c√≠clico de los planetas** (entre los siglos X y XI)

```{r echo = FALSE,  out.width = "60%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Beniger y Robyn (1978)"}
knitr::include_graphics("./img/dataviz_historico_1.png")
``` 

[1] [üìö ¬´Quantitative Graphics in Statistics: A Brief History¬ª de James R. Beniger y Dorothy L. Robyn. The American Statistician (1978)](https://www.jstor.org/stable/2683467)

 
---

# Navegaci√≥n y astronom√≠a

Con una motivaci√≥n similar, en torno a 1360 el matem√°tico **Nicole Oresme** dise√±√≥ el **primer gr√°fico de barras**<sup>1</sup> (no estad√≠stico), con la idea de **visualizar a la vez dos magnitudes f√≠sicas te√≥ricas**.


```{r echo = FALSE,  out.width = "30%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Friendly y Valero-Mora (2010), de ¬´Tractatus De Latitudinibus Formarum¬ª"}
knitr::include_graphics("./img/dataviz_historico_2.jpeg")
``` 

[1] [üìö ¬´The First (Known) Statistical Graph: Michael Florent van Langren and the 'Secret' of Longitude¬ª de M. Friendly y P. M. Valero-Mora. The American Statistician (2010)](https://www.researchgate.net/publication/227369016_The_First_Known_Statistical_Graph_Michael_Florent_van_Langren_and_the_Secret_of_Longitude)

 
---

# Primer gr√°fico estad√≠stico

La mayor√≠a de expertos, como Tufte <sup>1,2</sup>, consideran **este gr√°fico** casi longitudinal como la **primera visualizaci√≥n de datos** de la historia, hecha por **van Langren** en 1644, representando la **distancia (en longitud) entre Toledo y Roma** (un poco mal medida ya que la distancia real es de 16.5¬∫).

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Gr√°fica original extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/longitud_dataviz.jpg")
``` 

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Adaptaci√≥n extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/dataviz_historico_3.jpeg")
``` 

[1] [üìö ¬´Visual explanations: images and quantities, evidence and narrative¬ª de E. Tufte](https://archive.org/details/visualexplanatio00tuft)

[2] [üìö ¬´PowerPoint is evil¬ª de E. Tufte](https://www.wired.com/2003/09/ppt2/)

---


# Historia de la estad√≠stica: navegaci√≥n y astronom√≠a

.pull-left[

### T. Brahe

Uno de los primeros usos ¬´modernos¬ª de la estad√≠stica fue en la **navegaci√≥n y la astronom√≠a**, siendo Tycho Brahe de los primeros en utilizar la estad√≠stica para **reducir los errores** observacionales.
]

.pull-right[

### E. Wright

Fue el primero en usar en 1599 lo que hoy llamamos **mediana** en su libro ¬´Certaine errors in navigation¬ª, aplicada a la navegaci√≥n.

]

.pull-left[

### G. Galileo

Aunque la fama se la llev√≥ **Gauss**, fue el primero en plantear una idea similar a la que hoy llamamos **m√©todo de m√≠nimos cuadrados**: los valores m√°s probables ser√≠an aquellos que minimizaran los errores.

]

.pull-right[
### C. F. Gauss y A. M. Legendre

El **m√©todo de los m√≠nimos cuadrados**, en el que basan modelos actuales como la regresi√≥n, fue desarrollado por **Legendre y Gauss** (el √∫ltimo lo aplic√≥ a la detecci√≥n m√°s probable del planeta enano Ceres).

]

---

# Historia de la estad√≠stica: demograf√≠a, epidemiolog√≠a y fisiolog√≠a

.pull-left[

#### J. Graunt

Autor de ¬´Natural and Political Observations Made upon the Bills of Mortality¬ª (1662), uno de los primeros trabajos en los que ya se hablaba de **exceso de mortalidad** a partir de las primeras tablas de natalidad y mortalidad, **estimando la poblaci√≥n de Londres**.
]

.pull-right[

#### G. Neumann

Las **fakes news** ya exist√≠an en el siglo XVII: Gaspar Neumann tambi√©n un precursor en el **an√°lisis estad√≠stico de tablas de mortalidad**, para desmentir bulos (ejemplo: desmont√≥ la creencia de que en los a√±os acabados en siete mor√≠an m√°s personas).
]


Son precisamente las tablas de Graunt las que us√≥ **Christiaan Huygens** (pionero en teor√≠a de probabilidad con su ¬´De ratiociniis in ludo aleae¬ª en 1656) para generar la **primera gr√°fica de densidad** de una distribuci√≥n continua, visualizando la **esperanza de vida** (en funci√≥n de la edad).


---

# Primer gr√°fico de densidad


```{r echo = FALSE,  out.width = "50%", fig.align = "center", fig.cap = "Primera funci√≥n de densidad, extra√≠da de https://omeka.lehigh.edu/exhibits/show/data_visualization/vital_statistics/huygen"}
knitr::include_graphics("https://omeka.lehigh.edu/files/fullsize/65fc32c11a768f1d3263a99caca28dff.jpg")
``` 

---

# El gran boom: los gr√°ficos de Playfair

La figura que cambi√≥ el dataviz fue, sin lugar a dudas, el economista y pol√≠tico **William Playfair (1759-1823)**. En 1786 public√≥ el **¬´Atlas pol√≠tico y comercial¬ª**<sup>1,2</sup> con 44 gr√°ficas (43 series temporales y el **diagrama de barras m√°s famoso**, aunque no el primero).

.pull-left[

```{r echo = FALSE, out.width = "85%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_1.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "35%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_2.jpg")
``` 

]

[1] [üìö ¬´Atlas pol√≠tico y comercial¬ª de William Playfair (1786)](https://www.amazon.es/Playfairs-Commercial-Political-Statistical-Breviary/dp/0521855543)

[2] [üìö ¬´Playfair and his charts¬ª de H. Gray Funkhouser and  Helen M. Walker (1935)](https://www.jstor.org/stable/45366440)

---

# Primer gr√°fico de barras

Playfair es adem√°s el **autor del gr√°fico de barras m√°s famoso** (aunque no fue el primero, pero s√≠ el que sent√≥ un precedente, quien lo hizo _mainstream_).

.pull-left[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair de importaciones (barras grises) y exportaciones (negras) de Escocia en 1781, extra√≠das de la wikipedia."}
knitr::include_graphics("./img/playfair_5.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Primer diagrama de barras (Philippe Buache y Guillaume de L‚ÄôIsle), visualizando los niveles del Sena desde 1732 hasta 1766, extra√≠da de https://friendly.github.io/HistDataVis"}
knitr::include_graphics("./img/playfair_6.jpg")
``` 

]


---


# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

#### F. Galton

Primo de Charles Darwin, inventor de los **silbatos para perretes**, de los mapas de predicci√≥n meteorol√≥gica y la persona que acu√±√≥ el concepto de **regresi√≥n** (y el de eugenesia :/).

```{r echo = FALSE, out.width = "93%", fig.align = "center"}
knitr::include_graphics("https://www.bogleheads.org/w/images/thumb/9/95/Screen_Shot_2012-01-03_at_7.36.29_AM.png/600px-Screen_Shot_2012-01-03_at_7.36.29_AM.png")
``` 


]

.pull-right[

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_1.jpg")
``` 

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_2.png")
``` 

]



---

# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

#### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street.

]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("https://media.revistagq.com/photos/5cc84a91c46d3a2b7435d7cf/2:3/w_1799,h_2699,c_limit/pelo%20jon%20snow.jpg")
``` 

]

---

# El boom de la estad√≠stica: epidemiolog√≠a y bioestad√≠stica


.pull-left[

#### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street<sup>1</sup>.


[1] [üìö ¬´El mapa fantasma¬ª, Steven Johnson, sobre la historia de John Snow](https://capitanswing.com/libros/el-mapa-fantasma/)


]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center", fig.cap = "John Snow, el epidemi√≥logo"}
knitr::include_graphics("https://s1.eestatic.com/2016/04/22/reportajes/reportajes_119248513_3987143_854x640.jpg")
``` 

]

---

# Nacimiento de la estad√≠stica espacial

```{r echo = FALSE, out.width = "77%", fig.align = "center", fig.cap = "Mapa de Londres, mostrando los casos de c√≥lera del 19 de agosto al 30 de septiembre de 1854, extra√≠do de https://friendly.github.io/HistDataVis."}
knitr::include_graphics("./img/snow_mapa.jpg")
``` 

---

# C√≥lera en Londres

Esa **epidemia de c√≥lera** en Londres fue una cat√°strofe en t√©rminos humanos pero supuso un verdadero auge de la **bioestad√≠stica y visualizaci√≥n de datos**. Unos a√±os antes que Snow, **William Farr** ya usaba el dataviz para **monitorizar las muertes diarias**<sup>1</sup> de c√≥lera (abajo, c√≥lera en azul, diarre en amarillo), en funci√≥n del tiempo meteorol√≥gico (arriba)


```{r echo = FALSE, out.width = "48%", fig.align = "center"}
knitr::include_graphics("./img/farr_colera.png")
``` 

[1] [¬´General Register Office, Report on the Mortality of Cholera in England, 1848‚Äì49¬ª publicado en 1852](https://wellcomecollection.org/works/pajtrpez/items?canvas=9)


---


# Florence Nigthingale: una revoluci√≥n

.pull-left[


* El 21 de octubre de 1854 **Florence Nigthingale** fue enviada para mejorar las **condiciones sanitarias** de los **soldados brit√°nicos en la guerra de Crimea**. 

* A su regreso se dedic√≥ a demostrar que los **soldados fallec√≠an por las condiciones sanitarias**: eran **muertes evitables**. Nigthingale es la creadora del famoso **diagrama de rosa**, permitiendo pintar tres variables a la vez y su estacionalidad.

* El 8 de febrero de 1955, The Times la describi√≥ como la **¬´√°ngel guardi√°n¬ª de los hospitales**, y al finalizar la contienda, fue recibida como una hero√≠na, conocida como **¬´The Lady with the Lamp¬ª** tras un poema de H. W. Longfellow publicado en 1857.

* A√±os despu√©s se convirti√≥ en la **primera mujer en la Royal Statistical Society** y renunci√≥ a su puesto para crear las primeras escuelas de enfermer√≠a

]

.pull-right[

```{r echo = FALSE, out.width = "90%", fig.align = "center"}
knitr::include_graphics("https://www.researchgate.net/profile/Miguel-Guevara-4/publication/325622727/figure/fig1/AS:635977265582080@1528640204506/FIGURA-2-La-dama-con-la-lampara-1891-en-ingles-The-lady-with-the-lamp.png")
``` 

]

---

# ¬øQu√© es el an√°lisis multidimensional?

Hasta la d√©cada de los 60, la mayor√≠a de la estad√≠stica que se realizaba era

* **estad√≠stica unidimensional**: extraer informaci√≥n de una sola variable (rentas, impuestos, exportaciones, etc).

* **estad√≠stica bidimensional**: desde que Galton acu√±√≥ la regresi√≥n, los grandes estad√≠sticos de principios de siglo se centraron en el **an√°lisis bidimensional**, analizando la dependencia entre una variable $X$ y otra variable $Y$, con herramientas como los **coeficientes de correlaci√≥n** de Pearson, Spearman o Kendall<sup>1</sup>


[1] [üìö Kendall, M. (1938). ¬´A New Measure of Rank Correlation¬ª. Biometrika 30 (1‚Äì2): 81-89. doi:10.1093/biomet/30.1-2.81](https://doi.org/10.1093/biomet/30.1-2.81)

--

&nbsp;

## Wishart, contigo empez√≥ todo

En 1928, Wishart public√≥ su famoso art√≠culo <sup>1</sup> en el que, se demostraba y desarrollaba expl√≠citamente la funci√≥n de distribuci√≥n de una 
**distribuci√≥n normal multivariante**<sup>2</sup>, trabajo m√°s tarde extendido por Fisher.

[2] [üìö Wishart, J. (1928). ¬´The generalised product moment distribution in samples from a normal multivariate population¬ª. Biometrika. 20A (1‚Äì2): 32‚Äì52. doi:10.1093/biomet/20A.1-2.32](https://doi.org/10.1093/biomet/20A.1-2.32)


---
 
# ¬øQu√© es el an√°lisis multidimensional?
 

El verdadero boom no lleg√≥ hasta la **d√©cada de los 60**,  con la publicaci√≥n del libro ¬´An Introduction to Multivariate Statistical Analysis¬ª<sup>1</sup> de Anderson (1958), proporcionando todo un marco te√≥rico

[1] [üìö Anderson, T.W. (1958). ¬´An Introduction to Multivariate Analysis¬ª. New York: Wiley ISBN 0471026409](http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/301_03_Anderson_An-Introduction-to-Multivariate-Statistical-Analysis-2003.pdf)

--

&nbsp;

## **Definici√≥n**

> El An√°lisis Multivariante es la rama de la estad√≠stica que estudia las relaciones (CONJUNTAMENTE) entre conjuntos de variables dependientes y los individuos para los cuales se han medido dichas variables (Kendall)

## **Notaci√≥n**

* $n$ tama√±o muestral (n√∫mero de individuos).

* $\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)$ conjunto de $p$ variables medidas para cada individuo $i=1,\ldots,n$.

* Nuestros datos ser√°n una matriz $\boldsymbol{X}$ de $n$ filas y $p$ columnas (con $p \ll n$)

---

# Ejemplo de distribuci√≥n bidimensional: normal bivariante

.pull-left[ 

Veamos un ejemplo sencillo con algo que seguramente nos sea familiar: la **distribuci√≥n Normal o campana de Gauss** $X \sim \mathcal{N}\left(\mu, \sigma \right)$, cuya funci√≥n de densidad es

$$f(x) = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}}, \quad \mu \in\mathbb{R},~\sigma >0$$

&nbsp;

La normal univariante depende de **dos par√°metros**:

* **esperanza o media** $\mu = {\rm E} [X]$ 
* **varianza** (unidimensional) $\sigma^2 := {\rm Var} [X] = {\rm E} [\left(X - \mu \right)^2] = {\rm E} [X^2] - \mu^2$


]


.pull-right[

```{r eval = FALSE}
# Generamos muestra normal
rnorm(n = 10000, mean = 0, sd = 1)
```

```{r echo = FALSE}
library(tidyverse)
# Generamos una muestra normal (n = 10 000)
data <- tibble("x" = rnorm(n = 10000, mean = 0, sd = 1))
```

```{r echo = FALSE, out.width = "80%"}
# Ploteamos
ggplot(data, aes(x = x)) +
  geom_density(fill = "#F29288", alpha = 0.5, size = 1.2) +
  labs(x = "x", y = "f(x) (funci√≥n densidad)")
```
  

]

---

# Normal bivariante

### **¬øY si medimos para cada individuo DOS variables?**

Si tenemos $\boldsymbol{X} = \left(X_1, X_2 \right)$, ¬øqu√© estad√≠sticos tenemos ahora a nuestra disposici√≥n?

* **Medidas marginales** (cada variable por separado):
  - medias $\mu_1:= {\rm E} [X_1]$ y $\mu_2:= {\rm E} [X_2]$
  - varianzas $\sigma_{1}^{2}:=\sigma_{1, 1}^{2} = \sigma_{X_1, X_1}^2$ y $\sigma_{2}^{2}:=\sigma_{2, 2}^{2} = \sigma_{X_2, X_2}^2$.

* **Varianza**: la varianza ${\rm Var} [X] := \sigma_{X}^2 = {\rm E} [ \left( X - \mu \right)^2 ]$ se puede entender c√≥mo una medida que nos **cuantifica** la relaci√≥n entre la variable consigo misma. ¬øY si en lugar de medir $X_1$ vs $X_1$ medimos $X_1$ vs $X_2$?

--

&nbsp;

### **Covarianza**

Definiremos la covarianza como una especie de varianza en la que cambiamos una de las $X$ por la otra variable


$${\rm Cov} [X_1, X_2] := \sigma_{1,2} =  {\rm E} [ \left( X_1 - \mu_1 \right) \left( X_2 - \mu_2 \right) ] = {\rm E}[X_1 * X_2] - \mu_1 * \mu_2 = \sigma_{2,1}$$

---

# Normal bivariante

### **Matriz de covarianzas**

Desde un punto de vista te√≥rico, dada una variable aleatoria bidimensional $\boldsymbol{X} = \left(X_1, X_2 \right)^{T}$, con vector de medias $\boldsymbol{\mu} = \left(\mu_1, \mu_2 \right)^{T}$ definiremos la **matriz de varianzas y covarianzas** $\Sigma$ de la siguiente manera:

$$\boldsymbol{\Sigma} := \begin{pmatrix} \sigma_{1,1}^2 & \sigma_{1,2} \\ \sigma_{2,1} & \sigma_{2,2}^2 \end{pmatrix} = \begin{pmatrix} \sigma_{1}^2 & \sigma_{1,2} \\ \sigma_{1,2} & \sigma_{2}^2 \end{pmatrix}, \quad \left| \boldsymbol{\Sigma} \right| = \sigma_{1}^{2}  \sigma_{2}^{2} - \sigma_{1,2}^{2} > 0$$

--

Se puede expresar **matricialmente** como

$\begin{eqnarray} \left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T}\left(\boldsymbol{X} - \boldsymbol{\mu} \right) &=& \left( X_1  - \mu_1, X_2 - \mu_2 \right)^{T} \begin{pmatrix} X_1  - \mu_1 \\ X_2 - \mu_2 \end{pmatrix} \\ &=& \begin{pmatrix} \left(X_1  - \mu_1 \right)^2 & \left(X_1  - \mu_1 \right)\left(X_2  - \mu_2 \right) \\ \left(X_2  - \mu_2 \right)\left(X_1  - \mu_1 \right) & \left(X_2  - \mu_2 \right)^2 \end{pmatrix} \end{eqnarray}$

**IMPORTANTE**: es una **matriz sim√©trica** (nos da igual medir $X$ vs $Y$, que $Y$ vs $X$).

---

# Normal multivariante

### **Normal univariante**

$$X \sim \mathcal{N} \left(\mu, \sigma^2 \right), \quad \boldsymbol{\Sigma} = \sigma^2, \quad f(x) =  \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}} = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-\frac{1}{2} (x-\mu ) \boldsymbol{\Sigma}^{-1} (x-\mu )}$$


### **Normal bivariante**

$$\boldsymbol{X} = \left(X_1, X_2 \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, x_2) = \frac{1}{2\pi \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

### **Normal multivariante (caso general)** 

Multivariante de $p \ll n$ variables

$$\boldsymbol{X} = \left(X_1, \ldots, X_p \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, \ldots, x_p) = \frac{1}{\left(2\pi \right)^{p/2} \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

$$\boldsymbol{\Sigma} = \left(\Sigma_{i,j} \right)_{i,j=1,\ldots,p}, \quad \Sigma_{i,j}:= {\rm Cov} [X_i, X_j ] = {\rm E}[(X_i-\mu_i) (X_j - \mu_j)]$$

---

# Versi√≥n muestral

Lo anterior nos permite conocer la **formulaci√≥n te√≥rica (poblacional)**: ¬øc√≥mo calculamos la varianza y covarianza cuando tenemos una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ observaciones medidas?


$$\boldsymbol{X} = \begin{pmatrix} x_{1, 1} & \ldots & x_{1, p} \\ \vdots & \ddots & \vdots \\ x_{n, 1} & \ldots & x_{n, p} \end{pmatrix} \quad \text{muestra}$$

#### **p = 2**

* **Varianzas muestrales**: $s_{x_1}^{2} := s_{1}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 1} - \overline{x}_1 \right)^2$ y $s_{x_2}^{2} := s_{2}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 2} - \overline{x}_2 \right)^2$, donde $\overline{x}_1$ y $\overline{x}_2$ son sus medias muestrales.

* **Covarianza muestral**: : $s_{x_1, x_2}^{2} := s_{1, 2} = s_{2, 1}^2 = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \left(x_{i, 1} - \overline{x}_1 \right)\left(x_{j, 2} - \overline{x}_2 \right)$

--

#### **Estimadores insesgados**

Seguramente dichos valores los hallas visto divididos por $n-1$ en lugar de $n$: los valores muestrales son estimadores de los valores poblacionales, y de aqu√≠ en adelante usaremos **estimadores insesgados**, estimadores $T$ del valor poblaci√≥n $U$ tal que ${\rm E}[T] = U$

* Estimador insesgado de $\mu_{x}$: $\overline{x}$ tal que ${\rm E}[\overline{x}] = \mu$

* Estimador insesgado de $\sigma_{x}^2$: la **cuasivarianza** $S_{x}^2 = \frac{n}{n-1} s_{x}^{2}$ tal que ${\rm E}[\sigma_{x}^2] = S_{x}^2$

* Estimador insesgado de $\sigma_{x, y}$: la **cuasicovarianza** $S_{x, y} = \frac{n}{n-1} s_{x, y}$ tal que ${\rm E}[\sigma_{x, y}] = S_{x, y}$

---

# Matriz de covarianzas (versi√≥n muestral)


En un **caso general**, dada una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ variables

$$S_{x_{k}}^2 := S_{k}^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_{i, k} - \overline{x}_k \right)^2 \quad \text{(cuasi) var. muestrales (marginales)}$$


$$S_{x_{k}, x_{l}} := S_{k, l} = \frac{1}{n-1} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_{i, k} - \overline{x}_k \right)\left(x_{j, l} - \overline{x}_l \right) \quad \text{(cuasi) covarianzas}$$

As√≠, la **matriz de (cuasi) covarianzas emp√≠ricas** quedar√° como

$$S := \frac{1}{n-1} \left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T} \left(\boldsymbol{X} - \boldsymbol{\mu} \right) =_{\boldsymbol{\mu} = 0} \frac{1}{n-1} \boldsymbol{X}^{T} \boldsymbol{X} = \begin{pmatrix} S_{1,1} & S_{1,2} & \ldots & S_{1, p} \\ S_{2,1} & S_{2,2} & \ldots & S_{2, p} \\ \vdots & \vdots & \ddots & \vdots \\ S_{p,1} & S_{p,2} & \ldots & S_{p, p} \end{pmatrix}$$

&nbsp;

--

Las **covarianzas (y varianzas)** tienen un **¬´problema¬ª**: **dependen de la magnitud** de los datos, proporcionando una medida que solo nos sirve para ser comparada con otra covariana, pero que **no nos proporciona una escala absoluta** para poder cuantificar.

---

# Matriz de correlaciones (versi√≥n muestral)


Para resolverlo, tenemos la **correlaci√≥n (de Pearson)** 

$$\rho_{k, l} := r_{k, l} = \frac{s_{k, l}}{\sqrt{s_{k}^2} \sqrt{s_{l}^2}} = \frac{S_{k, l}}{\sqrt{S_{k}^2} \sqrt{S_{l}^2}}$$

tal que siempre $-1 \leq r_{k, l} \leq 1$.

&nbsp;

--

De esta forma la **matriz de correlaciones** se puede expresar como

$$R := \left(r_{k, l} \right)_{k,l=1,\ldots,p} = D^{-1/2} S D^{-1/2}, \quad D = diag(S) = \begin{pmatrix} S_{1,1}^2 & \ldots & 0 \\ \vdots  & \ddots & \vdots \\  0 & \ldots & S_{p, p}^2 \end{pmatrix}$$


---


name: teoria-PCA
class: center, middle

# Teor√≠a: an√°lisis de componentes principales

---

# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

El **objetivo ¬´mainstream¬ª** del **an√°lisis de componentes principales** (PCA en ingl√©s) suele ser el de **reducir la dimensi√≥n** de nuestros datos: pasar de un conjunto de $n$ individuos y $p$ variables a otro de $k \leq p$ variables (para los mismos $n$ individuos).

&nbsp;

Esta reducci√≥n de la dimensi√≥n se suele hacer con **3 objetivos** principalmente:

* **Mejora computacional** de los algoritmos al tener un dataset m√°s reducido.

* **Permitir la visualizaci√≥n** en 2 o 3 dimensiones conjuntos $n$-dimensionales.

* **¬´Reflotar¬ª patrones** subyacentes en los datos.

]

.pull-right[

```{r echo = FALSE,  out.width = "100%", fig.align = "center", fig.cap = "Extra√≠da de https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa"}

knitr::include_graphics("https://miro.medium.com/max/959/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg")
``` 

]

---


# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

¬øEntonces? ¬øNo tiene sentido aplicar componentes principales o t√©cnicas de reducci√≥n de la dimensi√≥n en **datos bidimensionales**?

&nbsp;

Empecemos por un sencillo ejemplo, visualizando la **longitud y anchura de p√©talo** del famoso conjunto de datos `iris`

&nbsp;

**¬øCu√°les podr√≠an ser los objetivos?** ¬øTiene sentido en este ejemplo aplicar **t√©cnicas de reducci√≥n de la dimensi√≥n** como las componentes principales?

]

.pull-right[

```{r echo = FALSE, out.width = "100%"}
library(tidyverse)
ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point() +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).")
```

]

---

# Objetivo: maximizar la informaci√≥n


```{r echo = FALSE,  out.width = "80%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

Como veremos, el **objetivo real** ser√° **maximizar la informaci√≥n obtenido al menor coste posible**, y eso hace que siga siendo √∫til, aunque no reduzcamos dimensiones, hacerlo en el caso bidimensional: una **clave** de las componentes principales es que las **componentes resultantes** ser√°n **ortogonales** (perpendiculares), es decir, **linealmente independientes**.

&nbsp;

Las **componentes principales** pueden ser una herramienta muy √∫til para atajar problemas de **colinealidad** (variables altamente correladas entre s√≠, interfiriendo entre ellas)


---


# Idea principal

La **idea subyacente** tras el c√°lculo de las componentes principales se puede resumir de forma **geom√©trica**: para un conjunto de puntos $p$-dimensionales, encontrar un **nuevo sistema de coordenadas** de dimensi√≥n $k \leq p$ en el que expresar los datos, de forma que las **nuevas variables sean linealmente independientes**.

En el **caso bidimensional**, el resultado de aplicar componentes principales ser√° una especie de ¬´rotaci√≥n¬ª de los datos


```{r echo = FALSE,  out.width = "57%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>


---

# Idea principal: caso bidimensional


En el **caso bidimensional**, la idea ser√° buscar esa **elipse** en torno a la cual tenemos los datos, de forma que la direcci√≥n que marca el **eje mayor** ser√° la **primera componente** (la que tiene mayor rango --> mayor varianza) y la direcci√≥n que marca el **eje menor** ser√° la **segunda componente**.


.pull-left[

```{r echo = FALSE,  out.width = "77%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_1.jpg")
```

]


.pull-right[

```{r echo = FALSE,  out.width = "58%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_23.jpg")
```

]

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

---

# Caso inicial bidimensional


.pull-left[

Vamos a empezar por un **ejemplo sencillo (bidimensional)** tomando algunas observaciones de `{iris}` y solo las variables del p√©talo.

```{r echo = FALSE}
iris_bi <-
  tibble(iris) %>%
  select(contains("Petal")) %>%
  distinct(Petal.Width, .keep_all = TRUE) %>%
  slice(7:20)
```

```{r echo = FALSE}
iris_bi
```

]

.pull-right[

```{r echo = FALSE}
ggplot(iris_bi, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 3) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 23),
        axis.text.x = element_text(size = 15),
        axis.title.y = element_text(size = 23),
        axis.text.y = element_text(size = 15),
        plot.caption = element_text(size = 15))
```

]

---

# Caso bidimensional

.pull-left[

1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.

$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$


]

.pull-right[

```{r echo = FALSE,  out.width = "85%", fig.align = "center", fig.cap = "Direcciones de m√°xima varianza"}
knitr::include_graphics("./img/pca_iris_1.jpg")
```

]

---

# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---


# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

3. Las **nuevas direcciones** las seleccionaremos  **ortonormales** (m√≥dulo unitario):
$$x_{i, 1}'  =  \langle \boldsymbol{x}_i, \Phi_1 \rangle =  \left(x_{i, 1}, x_{i, 2} \right) \left(z_{1, 1}, z_{2, 1} \right)^{T} = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{1}^{T}, \quad x_{i, 2}' = \langle \boldsymbol{x}_i, \Phi_2 \rangle = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{2}^{T}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---

# Idea general


Nuestros datos originales $\boldsymbol{X}$  (dimensiones $n \times p$) ser√°n reconvertidos en un conjunto $\boldsymbol{X}'$ de dimensiones $n \times k$, con $k \leq p$, tal que 

$$\boldsymbol{X}' = \boldsymbol{X} \boldsymbol{\Phi}^{T}$$

--

tal que $\boldsymbol{\Phi}^{T}$ es una matriz $p \times k$ que contiene por columnas las $k$ **direcciones principales**

$$\boldsymbol{\Phi}^{T} = \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2} \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix}$$

--

bajo la condici√≥n de que sean **direcciones ortonormales**

$$\Phi \Phi^{T} = \begin{pmatrix} 1 & 0 & \ldots & 0 \\ 0 & 1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & 1 \end{pmatrix}$$

tal que dichas direcciones **maximicen la varianza**.

---

# Primera componente

Por ejemplo, para la **primera componente** el objetivo es encontrar, de entre todas las direcciones  $\boldsymbol{u}_1$ posibles, la direcci√≥n $\boldsymbol{\Phi}_1$ que **maximice la varianza de nuestros datos cuanos los proyectamos sobre dicha direcci√≥n**

$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ z_{1,2} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ x_{2,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$


--

&nbsp;

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

---

# Primera componente

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

--

Si **centramos los datos** (restamos su media para tener media nula)


$$\begin{eqnarray}\boldsymbol{\Phi}_1 &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} {\rm Var} \left( \boldsymbol{X}\boldsymbol{u}_{1}^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left({\rm E} \left[\left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right)^{T}\left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)\right] \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} \left({\rm E} \left[ \boldsymbol{u}_{1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{u}_1^{T} \right] \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 {\rm E} \left[\boldsymbol{X}^{T} \boldsymbol{X} \right] \boldsymbol{u}_{1}^{T}  \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_{1}^{T}  \right)\end{eqnarray}$$

--

Si **estandarizamos los datos** (restamos su media y dividimos entre su desviaci√≥n t√≠pica, teniendo **datos con media cero y varianza unitaria** para que todos los datos ponderen por igual)

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right)$$

---

# Primera componente

Para **encontrar esa direcci√≥n $\boldsymbol{u}_1$** que nos maximiza la varianza de los proyectados en ella, sujeto a la restrcci√≥n de que $\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1$, se puede usar la t√©cnica de los **multiplicadores de Lagrange** que nos dice que

$$\boldsymbol{\Phi}_1 =  \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right), \quad \lambda \in \mathbb{R}$$

--

Eso es equivalente a encontrar el valor que nos **iguale la derivada a cero**

$$\frac{\partial}{\partial \boldsymbol{u}_1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right) =   R \boldsymbol{u}_1  - \lambda \boldsymbol{u}_1^{T} = \left(R - \lambda \boldsymbol{Id}_{p} \right) \boldsymbol{u}_1^{T}  =  0$$
--

Esto es lo mismo que decir que $R \boldsymbol{u}_1^{T}  = \lambda \boldsymbol{u}_1^{T}$, es decir, la direcci√≥n que buscamos $\boldsymbol{u}_{1}^{T}$ es un **autovector de la matriz de covarianzas** (tras **estandarizar** los datos).

---

# Par√©ntesis: autovectores y autovalores

---

# Primera componente

Recapitulando, para obtener la **primera componente** $\boldsymbol{\Phi}_1$, debemos de 

* **Estandarizar** nuestros datos
* Calcular la **matriz de (cuasi)covarianzas** $\boldsymbol{S}$
* Calcula sus **autovectores** tal que $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$ (normalizados a m√≥dulo 1).

--

Adem√°s si es un autovector de la matriz de covarianzas tenemos entonces que la **varianza maximizada**, la **proporci√≥n de informaci√≥n** que **explica dicha componente**, ser√°

$$\boldsymbol{\Phi}_{1} \left(  S \boldsymbol{\Phi}_{1}^{T} \right) = \boldsymbol{\Phi}_{1} \left( R \boldsymbol{\Phi}_{1}^{T} \right) =\boldsymbol{\Phi}_{1}\left(  \lambda_1 \boldsymbol{\Phi}_{1}^{T} \right) =  \lambda_1 \boldsymbol{\Phi}_{1} \boldsymbol{\Phi}_{1}^{T} =_{\text{ortonormales}} \lambda_1$$ 

--

As√≠ que obtener la direcci√≥n (de todos los autovalores) que mayor informaci√≥n captura nos fijaremos en aquella que tenga **asociada el autovalor m√°s grande**.


$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & \ldots & x_{1, p}  \\ \vdots  & \ddots & \vdots
\\ x_{n,1}  & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$

donde $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$, siendo $\lambda_1$ el mayor de los autovalores de la matriz de (cuasi)covarianzas $S$, y $\boldsymbol{\Phi}_{1}^{T}$ su autovector asociado. El **resto de las componentes** se obtendr√°n de forma similar, siendo ortogonales a cada una de las direcciones obtenidas.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Dados unos datos $\boldsymbol{X}$ de $n$ individuos y $p$ variables, el objetivo es encontrar nuevas **direcciones ortonormales** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$, con $1 \leq k \leq p$, como combinaci√≥n lineal de las variables originales.

* Los **datos son estandarizados**  tal que

$$\begin{pmatrix} \frac{x_{1,1} - \overline{x}_1}{S_{1}} & \frac{x_{1,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{1,p} - \overline{x}_p}{S_{p}} \\  \frac{x_{2,1} - \overline{x}_1}{S_{1}} & \frac{x_{2,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{2,p} - \overline{x}_p}{S_{p}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{x_{n,1} - \overline{x}_1}{S_{1}} & \frac{x_{n,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{n,p} - \overline{x}_p}{S_{p}} \end{pmatrix}$$

* Calcular la **matriz $S$ de (cuasi)covarianzas** de dichos datos estandarizados.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Calculamos los $p$ **autovectores** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_p \right\rbrace$, y sus **autovalores asociados** $\left\lbrace \lambda_1, \ldots, \lambda_p \right\rbrace$, de la matriz $S$, tal que $S  \boldsymbol{\Phi}_k = \lambda_k  \boldsymbol{\Phi}_k$.

* Seleccionamos las primeras $k \leq p$ componentes $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$ asociadas a los primeros $\left\lbrace \lambda_1, \ldots, \lambda_k \right\rbrace$ autovalores.

* La **varianza (informaci√≥n) capturada** por la direcci√≥n $k$-√©sima ser√° igual a $\lambda_k$.

* Las nuevas coordenadas ser√°n

$$\boldsymbol{X}^{'} = \boldsymbol{X} \boldsymbol{\Phi}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2}  \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix} = \begin{pmatrix} \boldsymbol{x}_1 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_1 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_1 \boldsymbol{\Phi}_k^{T} \\ \boldsymbol{x}_2 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_2 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_2 \boldsymbol{\Phi}_k^{T} \\ \vdots & \vdots & \ddots & \vdots \\ \boldsymbol{x}_n \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_n \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_n \boldsymbol{\Phi}_k^{T}\end{pmatrix}$$

---

# Idea general paso a paso 

* Los **autovectores** nos indican la **direcci√≥n de la componente**, y sus **coeficientes** (conocidos como _loadings_, los que generan la combinaci√≥n lineal de las variables originales) nos indican el **peso que tiene cada variable original en dicha componente**. Si por ejemplo $\boldsymbol{\Phi}_1 = \left(0.95, 0.15, 0.273 \right)$, significa que la primera componente (la que m√°s varianza captura) ser√° $0.95* \boldsymbol{X}_1 + 0.15 * \boldsymbol{X}_2 + 0.273 * \boldsymbol{X}_3$, estando dominada por la variable original $\boldsymbol{X}_1$ (un peso de 0.95).


* El **signo** de dichos _loadings_ nos indican el sentido de la relaci√≥n entre la componente y la variable original (correlaci√≥n positiva/negativa). Si alguno de ellos fuese 0 significar√≠a que la nueva componente est√° incorrelada respecto a dicha variable original.

&nbsp;

--

* Para cada observaci√≥n $i$, las nuevas coordenadas $\left(x_{i, 1}^{'}, \ldots, x_{i, k}^{'} \right)$, calculadas tras **proyectar la observaci√≥n original en las direcciones principales** $\boldsymbol{x}_{i} \boldsymbol{\Phi}^{T}$, se conocen como _scores_

&nbsp;

--

* Para **seleccionar el n√∫mero $k$** de componentes a seleccionar el m√©todo m√°s sencillo es **fijar de antemano** un **umbral varianza explicada** que queremos conservar (por ejemplo, $95%$), de forma que nos quedemos con el primer n√∫mero $k$ tal que $\sum_{j=1}^{k} \lambda_k > 0.95$ (**varianza explicada acumulada**, teniendo los autovalores ordenados de mayor a menor).


---


name: practica-PCA
class: center, middle

# Pr√°ctica: PCA en R

---

class: inverse center middle

# Extra. Introducci√≥n al dataviz en R.

&nbsp;

### [¬øQu√© es una gr√°fica?](#intro-historica)

### [Introducci√≥n a ggplot2](#intro-ggplot2)

### [La importancia de visualizar datos](#importancia-dataviz)

### [Profundizando en ggplot2](#importancia-dataviz)


---

name: intro-historica
class: center, middle


# Primer gr√°fico estad√≠stico

La mayor√≠a de expertos, como Tufte <sup>1,2</sup>, consideran **este gr√°fico** casi longitudinal como la **primera visualizaci√≥n de datos** de la historia, hecha por **van Langren** en 1644, representando la **distancia (en longitud) entre Toledo y Roma** (un poco mal medida ya que la distancia real es de 16.5¬∫).

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Gr√°fica original extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/longitud_dataviz.jpg")
``` 

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Adaptaci√≥n extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/dataviz_historico_3.jpeg")
``` 

[1] [üìö ¬´Visual explanations: images and quantities, evidence and narrative¬ª de E. Tufte](https://archive.org/details/visualexplanatio00tuft)

[2] [üìö ¬´PowerPoint is evil¬ª de E. Tufte](https://www.wired.com/2003/09/ppt2/)

---

# ¬øQu√© es una gr√°fica estad√≠stica?

¬øPor qu√© ese gr√°fico se considera la primera visualizaci√≥n estad√≠stica de la historia? ¬øQu√© es lo que hace que una visualizaci√≥n sea una gr√°fica estad√≠stica? **¬øCu√°l es la frontera entre una ilustraci√≥n y una gr√°fica (de datos)?**

&nbsp;

Esas mismas preguntas se hizo **Joaqu√≠n Sevilla** en su manual <sup>1</sup>, argumentando que deben cumplir **3 requisitos**:

1. Que se base en el esquema de composici√≥n de **eje m√©trico** (proceso de medida): **no cualquier dibujo que incluya n√∫meros** lo podemos denominar ¬´gr√°fica estad√≠stica¬ª. 

.pull-left[

```{r echo = FALSE,  out.width = "55%", fig.align = "center", fig.cap = "INFOGRAF√çA extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/mapa_sevilla.jpg")
``` 

]

.pull-right[

```{r echo = FALSE,  out.width = "53%", fig.align = "center", fig.cap = "GR√ÅFICA extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/grafica_dinero_sevilla.jpg")
``` 

]

üìö [1] [¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª de Joaqu√≠n Sevilla](http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf)


---

# Vizfail

.pull-left[

```{r echo = FALSE,  out.width = "99%", fig.align = "center", fig.cap = "Ejemplo de met√°fora visual mal ejecutada"}
knitr::include_graphics("./img/persona_dataviz.jpg")
``` 

]

.pull-right[

* La figura elegida (una persona caminando) no guarda relaci√≥n alguna con lo que se pretende representar.

* Los sectores se√±alados no tienen relaci√≥n con el √≠tem a representar, lo que dificulta su interpretaci√≥n

* Los colores no dan informaci√≥n de ning√∫n tipo.

* La forma hace imposible la comparaci√≥n entre √°reas (salvo que leas el % adjunto).

* La suma total supera el 100% ¬ø?

* **Sin la fuente de los datos**.

]

---

# ¬øQu√© es una gr√°fica estad√≠stica?

Ese **proceso de medida** en el que una gr√°fica se debe apoyar lo ilustra muy bien Alberto Cairo <sup>1</sup> con esta **infograf√≠a**.


* ¬øSabr√≠as decir en 5 segundos en que regi√≥n el empleo ha crecido m√°s? ¬øY menos?
* ¬øSabr√≠as decir en 5 segundos si la variaci√≥n ha sido mayor en Madrid, La Rioja o Canarias?

```{r echo = FALSE,  out.width = "65%", fig.align = "center", fig.cap = "INFOGRAF√çA extra√≠da de Alberto Cairo"}
knitr::include_graphics("./img/mapa_cairo_esp.jpg")
``` 

üìö [1] ¬´The Functional Art: an introduction to information graphics and visualization¬ª de Alberto Cairo.

---

# ¬øQu√© es una gr√°fica estad√≠stica?


```{r echo = FALSE,  out.width = "90%", fig.align = "center", fig.cap = "GR√ÅFICA extra√≠da de Alberto Cairo"}
knitr::include_graphics("./img/mapa_cairo_2.jpg")
``` 

La principal diferencia entre otro tipo de comunicaci√≥n visual y una **gr√°fica estad√≠stica** radica en proporcionar **herramientas de medida**.

---

# ¬øQu√© es una gr√°fica estad√≠stica?

1. Que se base en el esquema de composici√≥n de **eje m√©trico**

2. Debe incluir **informaci√≥n cuantitativa** (debe visualizar datos)

3. La relaci√≥n de representatividad deber√≠a **ser reversible**: los **datos deber√≠an poder ¬´recuperarse¬ª** a partir de la gr√°fica (una gr√°fica estad√≠stica es un tipo particular de **aplicaci√≥n** matem√°tica).


```{r echo = FALSE,  out.width = "50%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/sevilla_densidad.jpg")
``` 

üìö [¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª de Joaqu√≠n Sevilla](http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf)


---

# ¬øQu√© es una gr√°fica estad√≠stica?

Hay muchas formas de hacer una gr√°fica estad√≠stica, y **no suele pasar** por hacer un gr√°fico de tartas. Los **diagramas de tartas o sectores** tiene un problema de reversibilidad:

* Si hay **muchas variables**: salvo que conozcas el montante total y tengas un transportador de √°ngulos a mano, es muy complicado que obtengas informaci√≥n.

* Si hay **pocas variables**: ¬øaporta algo distinto (y/o mejor) que una tabla?


```{r echo = FALSE,  out.width = "40%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/sectores_sevilla.jpg")
``` 

---

# ¬øQu√© es una gr√°fica estad√≠stica?

El principal problema de un **diagrama de sectores** es que la posible informaci√≥n est√° contenido en los **√°ngulos**, pero nuestra interpretaci√≥n la realizamos a trav√©s de la **comparaci√≥n de √°reas** (nuestros ojos no miden bien √°ngulos), las cuales dependen no solo del √°ngulo sino del radio.

Algo similar sucede con los mal llamados **gr√°ficos tridimensionales** (son bidimensionales con perspectiva en realidad): los valores m√°s cercanos aparecen sobredimensionados, siendo pr√°cticamente imposible la reversibilidad.


```{r echo = FALSE,  out.width = "70%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/sectores_3D_sevilla.jpg")
``` 

---

# Vizfail

Adem√°s, dado que un diagrama de sectores solo permite una **visualizaci√≥n relativa** de los datos, no son comparables con otros diagramas de tartas.

```{r echo = FALSE,  out.width = "78%", fig.align = "center"}
knitr::include_graphics("https://pbs.twimg.com/media/BgDR4urIMAAnHmG?format=jpg&name=medium")
``` 

Gr√°fica extra√≠da de <https://twitter.com/Dave_Andrade/status/432576336872624129?t=FGjLtxE8V1BJR_QdXdtZgQ&s=19>

---

# ¬øL√≠neas rojas?

* Entender el contexto y respetar el dato

```{r echo = FALSE, out.width = "60%", fig.align = "center", fig.cap = "Infograf√≠a extra√≠da de https://twitter.com/storywithdata"}
knitr::include_graphics("https://pbs.twimg.com/media/FIgZcF4X0AAzLnC?format=jpg&name=medium")
``` 

(en lo de los diagramas de tartas discrepamos :P)

---

# Contexto

Una **buena idea** puede estar mejor o peor ejecutada, y la forma de llevarla a cabo es importante

```{r echo = FALSE, out.width = "30%", fig.align = "center", fig.cap = "Un sem√°foro no es una mala idea, la forma de instalarlo s√≠ puede serlo. Imagen extra√≠da del manual de Joaqu√≠n Sevilla"}
knitr::include_graphics("./img/semaforo.jpg")
``` 

---

# Fundamentos de la visualizaci√≥n de datos estad√≠sticos

## **Recomendaciones**

[¬´The Functional Art: an introduction to information graphics and visualization¬ª de Alberto Cairo](https://www.amazon.es/Functional-Art-Voices-That-Matter/dp/0321834739)


[¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª de Joaqu√≠n Sevilla](https://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf)


[¬´A Brief History of Visualization¬ª de Friendly et al. (2008)](https://www.researchgate.net/publication/226400313_A_Brief_History_of_Data_Visualization)

[¬´Quantitative Graphics in Statistics: A Brief History¬ª de James R. Beniger y Dorothy L. Robyn. The American Statistician (1978)](https://www.jstor.org/stable/2683467)]

[¬´Presentation Graphics¬ª de Leland Wilkinson. International Encyclopedia of the Social & Behavioral Sciences](https://www.cs.uic.edu/~wilkinson/Publications/iesbs.pdf)

[¬´The Grammar of Graphics¬ª de Leland Wilkinson](https://www.amazon.es/Grammar-Graphics-Statistics-Computing/dp/0387245448)


[¬´The Minard System: The Graphical Works of Charles-Joseph Minard¬ª de Sandra Rendgen](https://www.amazon.es/gp/product/1616896337/ref=sw_img_1?smid=A1AT7YVPFBWXBL&psc=1)


[¬´The Visual Display of Quantitative Information¬ª de E. W. Tufte](https://www.amazon.es/Visual-Display-Quantitative-Information/dp/0961392142)

---

name: intro-ggplot2
class: center, middle

# Dataviz e introducci√≥n a ggplot2

## **La gram√°tica de los gr√°ficos**

&nbsp;

El paquete `{ggplot2}` se basa en la idea propuesta en ¬´Grammar of graphics¬ª del reci√©n fallecido Wilkinson.

---

## ggplot2: grammar of graphics (gg)

.pull-left[

```{r echo = FALSE,  out.width = "70%", fig.align = "center", fig.cap = "Imagen extra√≠da de Reddit"}
knitr::include_graphics("https://dadosdelaplace.github.io/courses-ECI-2022/img/telling-dataviz")
``` 

]

.pull-right[

Una de las **principales fortalezas** de `R` no solo es el manejo de datos con `{tidyverse}`, tambi√©n la visualizaci√≥n con uno de sus paquetes: el paquete `{ggplot2}`.

La **visualizaci√≥n de datos** deber√≠a ser una parte fundamental de todo an√°lisis de datos. No es solo una cuesti√≥n est√©tica. La visualizaci√≥n de datos es fundamental para convertir el dato en informaci√≥n.

]


---

## ggplot2: grammar of graphics (gg)

.pull-left[

La idea de la filosof√≠a detr√°s de `{ggplot2}` es entender los **gr√°ficos como parte integrada del flujo de datos**, dot√°ndoles de una **gram√°tica**, bas√°ndose en la idea de 
[¬´The Grammar of Graphics¬ª de Leland Wilkinson](https://www.amazon.es/Grammar-Graphics-Statistics-Computing/dp/0387245448).

El objetivo es empezar con un lienzo en blanco e ir **a√±adiendo capas a tu gr√°fico**, como har√≠as por ejemplo en Photoshop, con la diferencia de que nuestras capas podemos **ligarlas al conjunto de datos**.

La ventaja de `{ggplot2}` es poder **mapear atributos est√©ticos** (color, forma, tama√±o) de **objetos geom√©tricos** (puntos, barras, l√≠neas) en **funci√≥n de los datos**, a√±adiendo transformaciones de los datos, res√∫menes estad√≠sticos y transformaciones de las coordenadas.


La **documentaci√≥n** del paquete puedes consultarla en <https://ggplot2-book.org/introduction.html> 
]

.pull-right[

```{r echo = FALSE,  out.width = "100%", fig.align = "center", fig.cap = "Idea detr√°s de la ¬´Grammar of graphics¬ª de Wilkinson"}
knitr::include_graphics("https://dadosdelaplace.github.io/courses-ECI-2022/img/grammar.jpg")
``` 
]


---

## ggplot2: grammar of graphics (gg)

.pull-left[


```{r echo = FALSE,  out.width = "100%", fig.align = "center", fig.cap = "Idea detr√°s de la ¬´Grammar of graphics¬ª de Wilkinson"}
knitr::include_graphics("https://dadosdelaplace.github.io/courses-ECI-2022/img/grammar.jpg")
``` 

]

.pull-right[

Un gr√°fico se podr√° componer de las siguientes **capas**

* **Datos**
* **Mapeado de elementos est√©ticos (aesthetics)**: ejes, color, forma, tama√±o, etc (en funci√≥n de los datos)
* **Elementos geom√©tricos (geom)**: puntos, l√≠neas, barras, pol√≠gonos, etc.
* **Componer gr√°ficas (facet)**: visualizar varias gr√°ficas a la vez.
* **Transformaciones estad√≠sticas (stat)**: ordenar, resumir, agrupar, etc.
* **Sistema de coordenadas (coord)**: coordenadas, grids, etc.
* **Temas (theme)**: fuente, tama√±o de letra, subt√≠tulos, captions, leyenda, ejes, etc.

]

---



# Recursos y bibliograf√≠a

&nbsp;

### **Leyenda de los recursos**

&nbsp;

&nbsp;


#### üìö **Art√≠culos o libros** cient√≠ficos que han sido sometidos a revisi√≥n por pares.

&nbsp;

#### üîó **Recursos online** recomendados

&nbsp;

#### üíª Recursos para la **programaci√≥n en R**

---

# Bibliograf√≠a general

üíª **Tidy Data Tutor**: para visualizar la mec√°nica interna de `{tidyverse}`. <https://tidydatatutor.com/>

üîó Web con recursos para la **introducci√≥n a la estad√≠stica y Machine Learning en R** <https://artofstat.com/>

üìö **¬´An Introduction to Multivariate Statistical Analysis¬ª**. Anderson (1958) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/introduction_mva_anderson_2003.pdf>

üìö **¬´A New Measure of Rank Correlation¬ª**. Kendall (1938) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/correlation_kendall_1938.pdf>

üìö **¬´The generalised product moment distribution in samples from a normal multivariate population¬ª**. Wishart (1928) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/multivariate_normal_wishart_1928.pdf>

üìö **¬´On lines and planes of closest fit to systems of points in space¬ª**. Pearson (1901) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/fit_pearson_1901.pdf>


---

# Recursos dataviz

### Dataviz

üìö **¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª**. Sevilla (2005) <http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf>

üìö **¬´Quantitative Graphics in Statistics: A Brief History¬ª**. Beniger and Robyn <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/graphics_beniger_robin_1978.pdf>
 

---

# Bibliograf√≠a componentes principales

üíª **Componentes principales** en `{tidymodels}`. <https://www.tmwr.org/dimensionality.html#beans>


üìö **¬´Principal Component Analysis¬ª**. Jolliffe (2002) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/pca_jolliffe_2002.pdf>

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

üìö **¬´Principal Component Analysis: a review and recent developments¬ª**. Jolliffe and Cadima (2016) <https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202>

üîó **¬´The Mathematics Behind Principal Component Analysis¬ª**. Dubey (2018).  <https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643>


üîó **¬´A One-Stop Shop for Principal Component Analysis¬ª**. Brems (2017). <https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>

üìö **¬´On the number of principal components: a test of dimensionality based on measurements of similarity between matrices¬ª**. Dray (2008) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/numer_pca_dray_2008.pdf>


---

# Recursos y bibliograf√≠a

### Otras t√©cnicas de reducci√≥n de la dimensi√≥n

üîó Sobre **PCA y PLS**. Amat (2017). <https://www.cienciadedatos.net/documentos/35_principal_component_analysis#Introducci%C3%B3n>

üìö **¬´On the early history of the singular value decomposition¬ª**. Stewart (1993) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/svd_stewart_1993.pdf>

---

# Bibliograf√≠a: clustering



---

# Tareas pendientes

- cuadros en dataviz
- ¬øggplot2 aqu√≠?
- scripts covarianza/correlaci√≥n
- ¬øpls? ¬øcanonical analysis?
- clustering