---
title: "T√âCNICAS DE AN√ÅLISIS MULTIVARIANTE"
subtitle: "M√°ster propio (NTIC) en ¬´Big Data y Business Analytics¬ª"
author:
  - "Profesor: Javier √Ålvarez Li√©bana"
institute: "Facultad de Estudios Estad√≠sticos (UCM)"
date: "22/04/2022 - 23/04/2022 (actualizado: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    self_contained: false
    lib_dir: libs
    css: [default, style.css]
    nature:
      # beforeInit: "stylejs.js"
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---


```{r settings, include = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, out.width = "100%", cache = FALSE,
                      comment = ">", echo = TRUE, message = FALSE,
                      warning = FALSE, hiline = TRUE, dpi = 120)

# xaringan Extra
# devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
use_xaringan_extra(c("tile_view", "animate_css", "tachyons"))
use_tile_view() # panel
# xaringanExtra::use_scribble() # scribble
use_extra_styles(hover_code_line = TRUE,
                 mute_unhighlighted_code = FALSE) # Hover triangle code line
use_clipboard( # About clipboard
  button_text = "Click para copiar c√≥digo",
  success_text = "C√≥digo copiado",
  error_text = "Ctrl+C para copiar"
)
use_freezeframe() # restarting gifs
use_animate_all("fade") # animates
use_panelset() # panels 
```

class: inverse center middle

# ATAJOS DE LAS DIAPOSITIVAS



$$\\[2.7in]$$

.left[Pulsa <kbd-black>O</kbd-black> para ver el panel de diapositivas]
.left[Pulsa <kbd-black>H</kbd-black> para ver otros atajos]

---


# Material de las clases


.pull-left[

- **Diapositivas** del curso:
<https://dadosdelaplace.github.io/teaching/pca-clustering>

- **Scripts** del curso:
<https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/scripts>

- **Evaluaciones**:
<https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/eval>

- **Bibliograf√≠a**: <https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/biblio>

- **Manual** de R: <https://dadosdelaplace.github.io/courses-intro-R/>

- **Curso de dataviz** en R: <https://dadosdelaplace.github.io/curso-dataviz-ECI-2022>


]

.pull-right[

```{r material, echo = FALSE,  out.width = "83%", fig.align = "right"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

---

# Me presento: la turra

.pull-left[

```{r echo = FALSE,  out.width = "80%", fig.align = "left"}
knitr::include_graphics("./img/me.jpeg")
``` 

]

.pull-right[

* **Javier √Ålvarez Li√©bana**, nacido en 1989 en Carabanchel Bajo (Madrid)

* Licenciado (UCM) en **Matem√°ticas** (Erasmus en Bologna mediante). **M√°ster (UCM) en Ingenier√≠a Matem√°tica** (2013-2014)

&nbsp;

* **Doctorado en estad√≠stica** por la Universidad de Granada


* Encargado de la **visualizaci√≥n y an√°lisis de datos covid** de la Consejer√≠a de Salud del **Principado de Asturias**

]

&nbsp;

Intentando la **divulgaci√≥n** por `Twitter (@dadosdelaplace)` e `Instagram (@javieralvarezliebana)`. Tengo una newsletter: <https://cartasdelaplace.substack.com/>

---

# Objetivos


.pull-left[

El prop√≥sito de estas clases ser√° el tratamiento de **datos multidimensionales**, con tres objetivos principales:

- **Reducci√≥n de la dimensi√≥n**: ¬øtodas las variables aportan informaci√≥n? ¬øTodas son necesarias? ¬øPodemos transformar las variables para mantener la informaci√≥n de los datos pero reducir la dimensionalidad de los mismos?

- **Visualizaci√≥n**: ¬øcu√°ntas dimensiones podemos incluir en un gr√°fico 2D? ¬øC√≥mo visualizar datos multidimensionales?

- **Encontrar patrones**: ¬øc√≥mo agrupar (clusterizar) los elementos en funci√≥n de sus diferencias y similitudes?

]

.pull-right[

```{r material2, echo = FALSE,  out.width = "120%", fig.align = "left"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

&nbsp;

üìö Estas **diapositivas** han sido elaboradas con el propio `R` haciendo uso del paquete `{xaringan}`
y `{xaringanExtra}`.


---


# Requisitos

Para el presente curso los √∫nicos **requisitos** ser√°n:

1. **Conexi√≥n a internet** (para la descarga de algunos datos y paquetes).

2. **Instalar R**: ser√° nuestro **lenguaje**, nuestro **castellano** para poder ¬´comunicarnos con el ordenador. La descarga la haremos (gratuitamente) desde <https://cran.r-project.org/>

3. **Instalar R Studio**. De la misma manera que podemos escribir castellano en un ordenador, en un Word, en un papel o en un tuit, podemos usar distintos IDE (entornos de desarrollo integrados, nuestro Office), para que el trabajo sea m√°s c√≥modo. Nuestro **Word** para nosotros ser√° **RStudio**.

.left[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/cran-R.jpg" alt = "cran-R" align = "left" width = "500" style = "margin-top: 5vh">
]

.right[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/R-studio.jpg" alt = "RStudio" align = "right" width = "500" style = "margin-top: 5vh;">
]


---

class: inverse center middle

# POR SI ACASO...¬øPOR QU√â R Y NO EXCEL?


---

# R vs excel

![](./img/meme_barco.jpg)

---

# Incel vs excel

```{r echo = FALSE, out.width = '85%', fig.align = "center"}
knitr::include_graphics("./img/incel.jpg")
```

---

# Datos: de la celda a la tabla

<img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/celdas.jpg" alt = "celdas" align = "center" width = "850" style = "margin-top: 1vh;">


* **Celda**: un **dato individual** de un tipo concreto.
* **Variable**: **concatenaci√≥n de datos** del mismo tipo.
* **Matriz**: **concatenaci√≥n de variables** del **mismo tipo** y longitud.
* **Tabla**: **concatenaci√≥n de variables** de **distinto tipo** pero igual longitud.

---

class: inverse center middle

# BLOQUE I. Selecci√≥n de variables: PCA

&nbsp;


### [¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?](#intro-PCA)

### [Teor√≠a: an√°lisis de componentes principales](#teoria-PCA)

### [Pr√°ctica: PCA en R (visualizaci√≥n)](#practica-PCA)

---

name: intro-PCA
class: center, middle

# ¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?

### **¬øQu√© es el an√°lisis multivariante?**

---

# Breve historia de la estad√≠stica

.pull-left[

## Origen

* Del (neo)lat√≠n ¬´statisticum collegium¬ª: consejo de **Estado**.
* Del alem√°n ¬´statistik¬ª (ciencia del **Estado**, intoducido por G. Achenwall).


## Primeros usos: elaboraci√≥n de censos

Los **primeros usos** documentados de la estad√≠stica fueron la elaboraci√≥n de **censos** por parte de **mesopot√°micos, chinos y egipcios**, con tres fines:

* Cobrar **impuestos** (un saludo, Willyrex).
* Reparto de **tierras** y optimizaci√≥n de su uso.
* **Reclutamiento de soldados**.

]

.pull-right[

## Estad√≠stica en la guerra

Seg√∫n Tuc√≠dides, conceptos estad√≠sticos como la **moda** datan del **siglo V a.C.**: para asaltar la muralla de la ciudad de Platea, pon√≠an a contar a varios soldados el n√∫mero de ladrillos vistos en la muralla, qued√°ndose con el **conteo m√°s repetido (la moda, el m√°s frecuente)**, permitiendo el c√°lculo de la altura de la muralla.

```{r echo = FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("./img/peloponeso.jpg")
```

]

---

# ¬øQu√© han hecho los romanos por nosotros?

.pull-left[

Precisamente por el tama√±o de su Imperio, fueron los **romanos** quienes hicieron un uso m√°s intenso de la estad√≠stica:

* **Censos** (elaborados por la censura, que elaboraba no solo el censo sino la supervisi√≥n de la moralidad p√∫blica).
* Primeras **tablas de natalidad/mortalidad**
* Primeros **catastros** (registros oficiales de propiedades, primeros impuestos)

```{r echo = FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("./img/catastro.jpg")
```

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "left"}
knitr::include_graphics("https://www.publico.es/tremending/wp-content/uploads/2019/02/lifeofbrian3.jpg")
```

]

---

# Breve historia de la estad√≠stica

.pull-left[

## **√ÅRABES**

Autores de los **primeros tratados de estad√≠stica**, como el manuscrito de **Al-Kindi (801-873)**, que us√≥ la distribuci√≥n de **frecuencias de palabras** para el desarrollo de m√©todos de cifrado y descifrado de **mensajes encriptados**.

]

.pull-right[

## **M√âXICO**

Ya en el **a√±o 1116, el rey X√≥lotl** implement√≥ un **censo** que consist√≠a en la **estimaci√≥n de piedras**, tirando cada s√∫bdito una a un mont√≥n (Nepohualco).
]


&nbsp;

.pull-left[

## **INGLATERRA**

Desde el siglo XII se realiza la **Prueba del Pyx**, considerado uno de los **primeros controles de calidad**: se extre una de las monedas acu√±adas y se deposita en una caja, para un a√±o despu√©s comprobar su calidad y pureza.

]

.pull-right[

## **ITALIA**

En paralelo al **auge de los primeros ¬´sistemas financieros¬ª en Italia**, ¬´La Nuova Cr√≥nica¬ª de G. Villani fue considerado durante mucho tiempo el primer tratado de estad√≠stica (hasta el descubrimiento de los trabajos de Al-Kindi).

]

---

# Navegaci√≥n y astronom√≠a

Y es de aquella √©poca medieval, en la que la navegaci√≥n y la astronom√≠a empezaban a tomar relevancia cient√≠fica, cuando aparece la que se considera la primera gr√°fica (aunque no propiamente estad√≠stica) <sup>1</sup>, representando el **movimiento c√≠clico de los planetas** (entre los siglos X y XI)

```{r echo = FALSE,  out.width = "60%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Beniger y Robyn (1978)"}
knitr::include_graphics("./img/dataviz_historico_1.png")
``` 

[1] [üìö ¬´Quantitative Graphics in Statistics: A Brief History¬ª de James R. Beniger y Dorothy L. Robyn. The American Statistician (1978)](https://www.jstor.org/stable/2683467)

 
---

# Navegaci√≥n y astronom√≠a

Con una motivaci√≥n similar, en torno a 1360 el matem√°tico **Nicole Oresme** dise√±√≥ el **primer gr√°fico de barras**<sup>1</sup> (no estad√≠stico), con la idea de **visualizar a la vez dos magnitudes f√≠sicas te√≥ricas** (pero...a√∫n sin representar datos).


```{r echo = FALSE,  out.width = "30%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Friendly y Valero-Mora (2010), de ¬´Tractatus De Latitudinibus Formarum¬ª"}
knitr::include_graphics("./img/dataviz_historico_2.jpeg")
``` 

[1] [üìö ¬´The First (Known) Statistical Graph: Michael Florent van Langren and the 'Secret' of Longitude¬ª de M. Friendly y P. M. Valero-Mora. The American Statistician (2010)](https://www.researchgate.net/publication/227369016_The_First_Known_Statistical_Graph_Michael_Florent_van_Langren_and_the_Secret_of_Longitude)

 
---

# Primer gr√°fico estad√≠stico

La mayor√≠a de expertos, como Tufte <sup>1,2</sup>, consideran **este gr√°fico** casi longitudinal como la **primera visualizaci√≥n de datos** de la historia, hecha por **van Langren** en 1644, representando la **distancia (en longitud) entre Toledo y Roma** (un poco mal medida ya que la distancia real es de 16.5¬∫).

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Gr√°fica original extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/longitud_dataviz.jpg")
``` 

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Adaptaci√≥n extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/dataviz_historico_3.jpeg")
``` 

[1] [üìö ¬´Visual explanations: images and quantities, evidence and narrative¬ª de E. Tufte](https://archive.org/details/visualexplanatio00tuft)

[2] [üìö ¬´PowerPoint is evil¬ª de E. Tufte](https://www.wired.com/2003/09/ppt2/)

---


# Navegaci√≥n y astronom√≠a

.pull-left[

### T. Brahe

Uno de los primeros usos ¬´modernos¬ª de la estad√≠stica fue en la **navegaci√≥n y la astronom√≠a**, siendo Tycho Brahe de los primeros en utilizar la estad√≠stica para **reducir los errores** observacionales.
]

.pull-right[

### E. Wright

Fue el primero en usar en 1599 lo que hoy llamamos **mediana** en su libro ¬´Certaine errors in navigation¬ª, aplicada a la navegaci√≥n.

]

.pull-left[

### G. Galileo

Aunque la fama se la llev√≥ **Gauss**, fue el primero en plantear una idea similar a la que hoy llamamos **m√©todo de m√≠nimos cuadrados**: los valores m√°s probables ser√≠an aquellos que minimizaran los errores.

]

.pull-right[
### C. F. Gauss y A. M. Legendre

El **m√©todo de los m√≠nimos cuadrados**, en el que basan modelos actuales como la regresi√≥n, fue desarrollado por **Legendre y Gauss** (el √∫ltimo lo aplic√≥ a la detecci√≥n m√°s probable del planeta enano Ceres).

]

---

# Demograf√≠a, epidemiolog√≠a y fisiolog√≠a

.pull-left[

### J. Graunt

Autor de ¬´Natural and Political Observations Made upon the Bills of Mortality¬ª (1662), uno de los primeros trabajos en los que ya se hablaba de **exceso de mortalidad** a partir de las primeras tablas de natalidad y mortalidad, **estimando la poblaci√≥n de Londres**.
]

.pull-right[

### G. Neumann

Las **fakes news** ya exist√≠an en el siglo XVII: Gaspar Neumann tambi√©n un precursor en el **an√°lisis estad√≠stico de tablas de mortalidad**, para desmentir bulos (ejemplo: desmont√≥ la creencia de que en los a√±os acabados en siete mor√≠an m√°s personas).
]

&nbsp;

Son precisamente las tablas de Graunt las que us√≥ **Christiaan Huygens** (pionero en teor√≠a de probabilidad con su ¬´De ratiociniis in ludo aleae¬ª en 1656) para generar la **primera gr√°fica de densidad** de una distribuci√≥n continua, visualizando la **esperanza de vida** (en funci√≥n de la edad).


---

# Primer gr√°fico de densidad


```{r echo = FALSE,  out.width = "50%", fig.align = "center", fig.cap = "Primera funci√≥n de densidad, extra√≠da de https://omeka.lehigh.edu/exhibits/show/data_visualization/vital_statistics/huygen"}
knitr::include_graphics("https://omeka.lehigh.edu/files/fullsize/65fc32c11a768f1d3263a99caca28dff.jpg")
``` 

---

# El gran boom: los gr√°ficos de Playfair

La figura que cambi√≥ el dataviz fue, sin lugar a dudas, el economista y pol√≠tico **William Playfair (1759-1823)**. En 1786 public√≥ el **¬´Atlas pol√≠tico y comercial¬ª**<sup>1,2</sup> con 44 gr√°ficas (43 series temporales y el **diagrama de barras m√°s famoso**, aunque no el primero).

.pull-left[

```{r echo = FALSE, out.width = "85%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_1.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "35%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_2.jpg")
``` 

]

[1] [üìö ¬´Atlas pol√≠tico y comercial¬ª de William Playfair (1786)](https://www.amazon.es/Playfairs-Commercial-Political-Statistical-Breviary/dp/0521855543)

[2] [üìö ¬´Playfair and his charts¬ª de H. Gray Funkhouser and  Helen M. Walker (1935)](https://www.jstor.org/stable/45366440)

---

# Primer gr√°fico de barras

Playfair es adem√°s el **autor del gr√°fico de barras m√°s famoso** (aunque no fue el primero, pero s√≠ el que sent√≥ un precedente, quien lo hizo _mainstream_).

.pull-left[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair de importaciones (barras grises) y exportaciones (negras) de Escocia en 1781, extra√≠das de la wikipedia."}
knitr::include_graphics("./img/playfair_5.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Primer diagrama de barras (Philippe Buache y Guillaume de L‚ÄôIsle), visualizando los niveles del Sena desde 1732 hasta 1766, extra√≠da de https://friendly.github.io/HistDataVis"}
knitr::include_graphics("./img/playfair_6.jpg")
``` 

]


---


# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

### F. Galton

Primo de Charles Darwin, inventor de los **silbatos para perretes**, de los mapas de predicci√≥n meteorol√≥gica y la persona que acu√±√≥ el concepto de **regresi√≥n** (y el de eugenesia :/).

```{r echo = FALSE, out.width = "93%", fig.align = "center"}
knitr::include_graphics("https://www.bogleheads.org/w/images/thumb/9/95/Screen_Shot_2012-01-03_at_7.36.29_AM.png/600px-Screen_Shot_2012-01-03_at_7.36.29_AM.png")
``` 


]

.pull-right[

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_1.jpg")
``` 

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_2.png")
``` 

]



---

# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street.

]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("https://media.revistagq.com/photos/5cc84a91c46d3a2b7435d7cf/2:3/w_1799,h_2699,c_limit/pelo%20jon%20snow.jpg")
``` 

]

---

# El boom de la estad√≠stica: epidemiolog√≠a y bioestad√≠stica


.pull-left[

#### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street<sup>1</sup>.


[1] [üìö ¬´El mapa fantasma¬ª, Steven Johnson, sobre la historia de John Snow](https://capitanswing.com/libros/el-mapa-fantasma/)


]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center", fig.cap = "John Snow, el epidemi√≥logo"}
knitr::include_graphics("https://s1.eestatic.com/2016/04/22/reportajes/reportajes_119248513_3987143_854x640.jpg")
``` 

]

---

# Primeros usos de la estad√≠stica espacial

```{r echo = FALSE, out.width = "77%", fig.align = "center", fig.cap = "Mapa de Londres, mostrando los casos de c√≥lera del 19 de agosto al 30 de septiembre de 1854, extra√≠do de https://friendly.github.io/HistDataVis."}
knitr::include_graphics("./img/snow_mapa.jpg")
``` 

---

# ¬øQu√© es el an√°lisis multidimensional?

Hasta la d√©cada de los 60, la mayor√≠a de la estad√≠stica que se realizaba era

* **estad√≠stica unidimensional**: extraer informaci√≥n de una sola variable (rentas, impuestos, exportaciones, etc).

* **estad√≠stica bidimensional**: desde que Galton acu√±√≥ la regresi√≥n, los grandes estad√≠sticos de principios de siglo se centraron en el **an√°lisis bidimensional**, analizando la dependencia entre una variable $X$ y otra variable $Y$, con herramientas como los **coeficientes de correlaci√≥n** de Pearson, Spearman o Kendall<sup>1</sup>


[1] [üìö Kendall, M. (1938). ¬´A New Measure of Rank Correlation¬ª. Biometrika 30 (1‚Äì2): 81-89. doi:10.1093/biomet/30.1-2.81](https://doi.org/10.1093/biomet/30.1-2.81)

--

&nbsp;

## Wishart, contigo empez√≥ todo

En 1928, Wishart public√≥ su famoso art√≠culo <sup>2</sup> en el que, se demostraba y desarrollaba expl√≠citamente la funci√≥n de distribuci√≥n de una 
**distribuci√≥n normal multivariante**, trabajo m√°s tarde extendido y formalizado por Fisher.

[2] [üìö Wishart, J. (1928). ¬´The generalised product moment distribution in samples from a normal multivariate population¬ª. Biometrika. 20A (1‚Äì2): 32‚Äì52. doi:10.1093/biomet/20A.1-2.32](https://doi.org/10.1093/biomet/20A.1-2.32)


---
 
# ¬øQu√© es el an√°lisis multidimensional?
 
Aunque el verdadero boom no lleg√≥ hasta la **d√©cada de los 60**, con la publicaci√≥n del libro ¬´An Introduction to Multivariate Statistical Analysis¬ª<sup>1</sup> de Anderson (1958), proporcionando todo un marco te√≥rico con el que poder trabajar.

[1] [üìö Anderson, T.W. (1958). ¬´An Introduction to Multivariate Analysis¬ª. New York: Wiley ISBN 0471026409](http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/301_03_Anderson_An-Introduction-to-Multivariate-Statistical-Analysis-2003.pdf)

--

&nbsp;

## **Definici√≥n**

> El An√°lisis Multivariante es la rama de la estad√≠stica que estudia las relaciones (CONJUNTAMENTE) entre conjuntos de variables dependientes y los individuos para los cuales se han medido dichas variables (Kendall)


## **Notaci√≥n**

* $n$ tama√±o muestral (n√∫mero de individuos --> filas).

* $\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)$ conjunto de $p$ variables (--> columnas) medidas para cada individuo $i=1,\ldots,n$.

* Nuestros datos estar√°n en forma de tabla o matriz $\boldsymbol{X}$ de $n$ filas y $p$ columnas (con $p \ll n$)

---

# Ejemplo de distribuci√≥n bidimensional: normal bivariante

.pull-left[ 

Veamos un ejemplo sencillo con algo que seguramente nos sea familiar: la **distribuci√≥n Normal o campana de Gauss** $X \sim \mathcal{N}\left(\mu, \sigma \right)$, cuya funci√≥n de densidad es

$$f(x) = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}}, \quad \mu \in\mathbb{R},~\sigma >0$$

&nbsp;

La normal univariante depende de **dos par√°metros**:

* **esperanza o media** $\mu = {\rm E} [X]$ 
* **varianza** (unidimensional) $\sigma^2 := {\rm Var} [X] = {\rm E} [\left(X - \mu \right)^2] = {\rm E} [X^2] - \mu^2$


]


.pull-right[

```{r eval = FALSE}
# Generamos muestra normal
rnorm(n = 10000, mean = 0, sd = 1)
```

```{r echo = FALSE}
library(tidyverse)
# Generamos una muestra normal (n = 10 000)
data <- tibble("x" = rnorm(n = 10000, mean = 0, sd = 1))
```

```{r echo = FALSE, out.width = "80%"}
# Ploteamos
ggplot(data, aes(x = x)) +
  geom_density(fill = "#F29288", alpha = 0.5, size = 1.2) +
  labs(x = "x", y = "f(x) (funci√≥n densidad)")
```
  

]

---

# Normal bivariante

### **¬øY si medimos para cada individuo DOS variables?**

Si tenemos $\boldsymbol{X} = \left(X_1, X_2 \right)$, ¬øqu√© estad√≠sticos tenemos ahora a nuestra disposici√≥n?

* **Medidas marginales** (cada variable por separado):
  - medias $\mu_1:= {\rm E} [X_1]$ y $\mu_2:= {\rm E} [X_2]$
  - varianzas $\sigma_{1}^{2}:=\sigma_{1, 1}^{2} = \sigma_{X_1, X_1}^2$ y $\sigma_{2}^{2}:=\sigma_{2, 2}^{2} = \sigma_{X_2, X_2}^2$.

--

&nbsp;

### **Covarianza**

La varianza ${\rm Var} [X] := \sigma_{X}^2 = {\rm E} [ \left( X - \mu \right)^2 ]$ es una medida de dispersi√≥n que nos **cuantifica** la relaci√≥n de una variable consigo misma. ¬øY si en lugar de medir $X_1$ vs $X_1$ medimos $X_1$ vs $X_2$?

Definiremos la **covarianza** como una especie de varianza en la que cambiamos una de las $X$ por otra variable

$$Cov (X_1, X_2) := \sigma_{1,2} =  {\rm E} [ \left( X_1 - \mu_1 \right) \left( X_2 - \mu_2 \right) ] = {\rm E}[X_1 * X_2] - \mu_1 * \mu_2 = \sigma_{2,1}$$

---

# Normal bivariante

### **Matriz de covarianzas**

Desde un punto de vista te√≥rico, dada una variable aleatoria bidimensional $\boldsymbol{X} = \left(X_1, X_2 \right)^{T}$, con vector de medias $\boldsymbol{\mu} = \left(\mu_1, \mu_2 \right)^{T}$ definiremos la **matriz de varianzas y covarianzas** $\Sigma$ de la siguiente manera:

$$\boldsymbol{\Sigma} := \begin{pmatrix} \sigma_{1,1}^2 & \sigma_{1,2} \\ \sigma_{2,1} & \sigma_{2,2}^2 \end{pmatrix} = \begin{pmatrix} \sigma_{1}^2 & \sigma_{1,2} \\ \sigma_{1,2} & \sigma_{2}^2 \end{pmatrix}, \quad \left| \boldsymbol{\Sigma} \right| = \sigma_{1}^{2}  \sigma_{2}^{2} - \sigma_{1,2}^{2} > 0$$

--

Se puede expresar **matricialmente** como

$\begin{eqnarray}\boldsymbol{\Sigma} = {\rm E} \left[\left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T}\left(\boldsymbol{X} - \boldsymbol{\mu} \right) \right] &=& {\rm E} \left[\left( X_1  - \mu_1, X_2 - \mu_2 \right)^{T} \begin{pmatrix} X_1  - \mu_1 \\ X_2 - \mu_2 \end{pmatrix} \right] \\ &=& \begin{pmatrix} {\rm E} \left[ \left(X_1  - \mu_1 \right)^2 \right] & {\rm E} \left[\left(X_1  - \mu_1 \right)\left(X_2  - \mu_2 \right) \right] \\ {\rm E} \left[\left(X_2  - \mu_2 \right)\left(X_1  - \mu_1 \right) \right] & {\rm E} \left[\left(X_2  - \mu_2 \right)^2\right] \end{pmatrix} \end{eqnarray}$

**IMPORTANTE**: es una **matriz sim√©trica** (nos da igual medir $X$ vs $Y$, que $Y$ vs $X$).

---

# Normal multivariante

### **Normal univariante**

$$X \sim \mathcal{N} \left(\mu, \sigma^2 \right), \quad \boldsymbol{\Sigma} = \sigma^2, \quad f(x) =  \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}} = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-\frac{1}{2} (x-\mu ) \boldsymbol{\Sigma}^{-1} (x-\mu )}$$


### **Normal bivariante**

$$\boldsymbol{X} = \left(X_1, X_2 \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, x_2) = \frac{1}{2\pi \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

--

### **Normal multivariante (caso general)** 

Multivariante de $p \ll n$ variables

$$\boldsymbol{X} = \left(X_1, \ldots, X_p \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, \ldots, x_p) = \frac{1}{\left(2\pi \right)^{p/2} \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

$$\boldsymbol{\Sigma} = \left(\Sigma_{i,j} \right)_{i,j=1,\ldots,p}, \quad \Sigma_{i,j}:= Cov (X_i, X_j ) = {\rm E}[(X_i-\mu_i) (X_j - \mu_j)]$$

---

# Versi√≥n muestral

Lo anterior nos permite conocer la **formulaci√≥n te√≥rica (poblacional)**: ¬øc√≥mo calculamos la varianza y covarianza cuando tenemos una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ observaciones medidas?


$$\boldsymbol{X} = \begin{pmatrix} x_{1, 1} & \ldots & x_{1, p} \\ \vdots & \ddots & \vdots \\ x_{n, 1} & \ldots & x_{n, p} \end{pmatrix} \quad \text{muestra}$$

#### **p = 2**

* **Varianzas muestrales**: $s_{x_1}^{2} := s_{1}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 1} - \overline{x}_1 \right)^2$ y $s_{x_2}^{2} := s_{2}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 2} - \overline{x}_2 \right)^2$, donde $\overline{x}_1$ y $\overline{x}_2$ son sus medias muestrales.

* **Covarianza muestral**: $s_{x_1, x_2}^{2} := s_{1, 2} = s_{2, 1}^2 = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \left(x_{i, 1} - \overline{x}_1 \right)\left(x_{j, 2} - \overline{x}_2 \right)$

--

#### **Estimadores insesgados**

Seguramente dichos valores los hallas visto divididos por $n-1$ en lugar de $n$: los valores muestrales son estimadores de los valores poblacionales, y de aqu√≠ en adelante usaremos **estimadores insesgados**, estimadores $T$ del valor poblaci√≥n $U$ tal que ${\rm E}[T] = U$

* Estimador insesgado de $\mu_{x}$: $\overline{x}$ tal que ${\rm E}[\overline{x}] = \mu$

* Estimador insesgado de $\sigma_{x}^2$: la **cuasivarianza** $S_{x}^2 = \frac{n}{n-1} s_{x}^{2}$ tal que ${\rm E}[\sigma_{x}^2] = S_{x}^2$

* Estimador insesgado de $\sigma_{x, y}$: la **cuasicovarianza** $S_{x, y} = \frac{n}{n-1} s_{x, y}$ tal que ${\rm E}[\sigma_{x, y}] = S_{x, y}$

---

# Matriz de covarianzas (versi√≥n muestral)


En un **caso general**, dada una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ variables

$$S_{x_{k}}^2 := S_{k}^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_{i, k} - \overline{x}_k \right)^2 \quad \text{(cuasi) var. muestrales (marginales)}$$


$$S_{x_{k}, x_{l}} := S_{k, l} = \frac{1}{n-1} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_{i, k} - \overline{x}_k \right)\left(x_{j, l} - \overline{x}_l \right) \quad \text{(cuasi) covarianzas}$$

As√≠, la **matriz de (cuasi) covarianzas emp√≠ricas** quedar√° como

$$S := \frac{1}{n-1} \left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T} \left(\boldsymbol{X} - \boldsymbol{\mu} \right) =_{\boldsymbol{\mu} = 0} \frac{1}{n-1} \boldsymbol{X}^{T} \boldsymbol{X} = \begin{pmatrix} S_{1,1} &  \ldots & S_{1, p} \\ \vdots & \ddots & \vdots \\ S_{p,1} & \ldots & S_{p, p} \end{pmatrix}$$

&nbsp;

--

Las **covarianzas (y varianzas)** tienen un **¬´problema¬ª**: **dependen de la magnitud** de los datos, proporcionando una medida que solo nos sirve para ser comparada con otra covariana, pero que **no nos proporciona una escala absoluta** para poder cuantificar.

---

# Matriz de correlaciones (versi√≥n muestral)


Para resolverlo, tenemos la **correlaci√≥n (de Pearson)** 

$$\rho_{k, l} := r_{k, l} = \frac{s_{k, l}}{\sqrt{s_{k}^2} \sqrt{s_{l}^2}} = \frac{S_{k, l}}{\sqrt{S_{k}^2} \sqrt{S_{l}^2}}$$

tal que siempre $-1 \leq r_{k, l} \leq 1$.

&nbsp;

--

De esta forma la **matriz de correlaciones** se puede expresar como

$$R := \left(r_{k, l} \right)_{k,l=1,\ldots,p} = D^{-1/2} S D^{-1/2}, \quad D = diag(S) = \begin{pmatrix} S_{1,1}^2 & \ldots & 0 \\ \vdots  & \ddots & \vdots \\  0 & \ldots & S_{p, p}^2 \end{pmatrix}$$


---


name: teoria-PCA
class: center, middle

# Teor√≠a: an√°lisis de componentes principales

---

# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

El **objetivo ¬´mainstream¬ª** del **an√°lisis de componentes principales** (PCA en ingl√©s) suele ser el de **reducir la dimensi√≥n** de nuestros datos: pasar de un conjunto de $n$ individuos y $p$ variables a otro de $k < p$ variables (para los mismos $n$ individuos).

&nbsp;

Esta reducci√≥n de la dimensi√≥n se suele hacer con **3 objetivos** principalmente:

* **Mejora computacional** de los algoritmos al tener un dataset m√°s reducido.

* **Permitir la visualizaci√≥n** en 2 o 3 dimensiones de conjuntos $n$-dimensionales.

* **¬´Reflotar¬ª patrones** subyacentes en los datos.

]

.pull-right[

```{r echo = FALSE,  out.width = "100%", fig.align = "center", fig.cap = "Extra√≠da de https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa"}

knitr::include_graphics("https://miro.medium.com/max/959/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg")
``` 

]

---


# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

¬øEntonces? ¬øNo tiene sentido aplicar componentes principales o t√©cnicas de reducci√≥n de la dimensi√≥n en **datos bidimensionales**?

&nbsp;

Empecemos por un sencillo ejemplo, visualizando la **longitud y anchura de p√©talo** del famoso conjunto de datos `iris`

&nbsp;

**¬øCu√°les podr√≠an ser los objetivos?** ¬øTiene sentido en este ejemplo aplicar **t√©cnicas de reducci√≥n de la dimensi√≥n** como las componentes principales?

]

.pull-right[

```{r echo = FALSE, out.width = "100%"}
library(tidyverse)
ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).") +
  theme_minimal()
```

]

---

# Objetivo: maximizar la informaci√≥n


```{r echo = FALSE,  out.width = "80%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

Como veremos, el **objetivo real** ser√° **maximizar la informaci√≥n obtenido al menor coste posible**, y eso hace que siga siendo √∫til, aunque no reduzcamos dimensiones, hacerlo en el caso bidimensional: una **clave** de las componentes principales es que las **componentes resultantes** ser√°n **ortogonales** (perpendiculares), es decir, **linealmente independientes**.

&nbsp;

Las **componentes principales** pueden ser una herramienta muy √∫til para atajar problemas de **colinealidad** (variables altamente correladas entre s√≠, interfiriendo entre ellas)


---


# Idea principal

La **idea subyacente** tras el c√°lculo de las componentes principales se puede resumir de forma **geom√©trica**: para un conjunto de puntos $p$-dimensionales, encontrar un **nuevo sistema de coordenadas** de dimensi√≥n $k \leq p$ en el que expresar los datos, de forma que las **nuevas variables sean linealmente independientes**.

En el **caso bidimensional**, el resultado de aplicar componentes principales ser√° una especie de ¬´rotaci√≥n¬ª de los datos


```{r echo = FALSE, out.width = "75%", fig.align = "center"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>


---

# Idea principal: caso bidimensional


En el **caso bidimensional**, la idea ser√° buscar esa **elipse** en torno a la cual tenemos los datos, de forma que la direcci√≥n que marca el **eje mayor** ser√° la **primera componente** (la que tiene mayor rango --> mayor varianza) y la direcci√≥n que marca el **eje menor** ser√° la **segunda componente**.


.pull-left[

```{r echo = FALSE,  out.width = "77%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_1.jpg")
```

]


.pull-right[

```{r echo = FALSE,  out.width = "58%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_23.jpg")
```

]

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

---

# Caso inicial bidimensional


.pull-left[

Vamos a empezar por un **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del p√©talo.

```{r echo = FALSE}
iris_bi <-
  tibble(iris) %>%
  select(contains("Petal"))
```

```{r echo = FALSE}
iris_bi
```

]

.pull-right[

```{r echo = FALSE}
ggplot(iris_bi, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 3) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 23),
        axis.text.x = element_text(size = 15),
        axis.title.y = element_text(size = 23),
        axis.text.y = element_text(size = 15),
        plot.caption = element_text(size = 15))
```

]

---

# Caso bidimensional

.pull-left[

1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.

$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$


]

.pull-right[

```{r echo = FALSE,  out.width = "85%", fig.align = "center", fig.cap = "Direcciones de m√°xima varianza"}
knitr::include_graphics("./img/pca_iris_1.jpg")
```

]

---

# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---


# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

3. Las **nuevas direcciones** las seleccionaremos  **ortonormales** (m√≥dulo unitario):
$$x_{i, 1}'  =  \langle \boldsymbol{x}_i, \Phi_1 \rangle =  \left(x_{i, 1}, x_{i, 2} \right) \left(z_{1, 1}, z_{2, 1} \right)^{T} = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{1}^{T}, \quad x_{i, 2}' = \langle \boldsymbol{x}_i, \Phi_2 \rangle = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{2}^{T}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---

# Idea general


Nuestros datos originales $\boldsymbol{X}$  (dimensiones $n \times p$) ser√°n reconvertidos en un conjunto $\boldsymbol{X}'$ de dimensiones $n \times k$, con $k \leq p$, tal que 

$$\boldsymbol{X}' = \boldsymbol{X} \boldsymbol{\Phi}^{T}$$

--

tal que $\boldsymbol{\Phi}^{T}$ es una matriz $p \times k$ que contiene por columnas las $k$ **direcciones principales**

$$\boldsymbol{\Phi}^{T} = \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2} \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix}$$

--

bajo la condici√≥n de que sean **direcciones ortonormales**

$$\Phi \Phi^{T} = \begin{pmatrix} 1 & \ldots & 0 \\  \vdots &  \ddots & \vdots \\ 0  & \ldots & 1 \end{pmatrix}$$

tal que dichas direcciones **maximicen la varianza**.

---

# Primera componente

Por ejemplo, para la **primera componente** el objetivo es encontrar, de entre todas las direcciones  $\boldsymbol{u}_1$ posibles, la direcci√≥n $\boldsymbol{\Phi}_1$ que **maximice la varianza de nuestros datos cuando los proyectamos sobre dicha direcci√≥n**

$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ z_{1,2} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ x_{2,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$


--

&nbsp;

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

---

# Primera componente

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

--

Si **centramos los datos** (restamos su media para tener media nula)


$$\begin{eqnarray}\boldsymbol{\Phi}_1 &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} {\rm Var} \left( \boldsymbol{X}\boldsymbol{u}_{1}^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left({\rm E} \left[\left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right)^{T}\left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)\right] \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} \left({\rm E} \left[ \boldsymbol{u}_{1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{u}_1^{T} \right] \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 {\rm E} \left[\boldsymbol{X}^{T} \boldsymbol{X} \right] \boldsymbol{u}_{1}^{T}  \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_{1}^{T}  \right)\end{eqnarray}$$

--

Si **estandarizamos los datos** (restamos su media y dividimos entre su desviaci√≥n t√≠pica, teniendo **datos con media cero y varianza unitaria** para que todos los datos ponderen por igual)

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right)$$

---

# Primera componente

Para **encontrar esa direcci√≥n $\boldsymbol{u}_1$** que nos maximiza la varianza de los proyectados en ella, sujeto a la restrcci√≥n de que $\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1$, se puede usar la t√©cnica de los **multiplicadores de Lagrange** que nos dice que

$$\boldsymbol{\Phi}_1 =  \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right), \quad \lambda \in \mathbb{R}$$

--

Eso es equivalente a encontrar el valor que nos **iguale la derivada a cero**

$$\frac{\partial}{\partial \boldsymbol{u}_1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right) =   R \boldsymbol{u}_1  - \lambda \boldsymbol{u}_1^{T} = \left(R - \lambda \boldsymbol{Id}_{p} \right) \boldsymbol{u}_1^{T}  =  0$$
--

Esto es lo mismo que decir que $R \boldsymbol{u}_1^{T}  = \lambda \boldsymbol{u}_1^{T}$, es decir, la direcci√≥n que buscamos $\boldsymbol{u}_{1}^{T}$ es un **autovector de la matriz de covarianzas** (tras **estandarizar** los datos).

---

# Par√©ntesis: autovectores y autovalores

En √°lgebra matricial, dada una matriz $\boldsymbol{A}$ cuadrada de tama√±o $p \times p$, decimos que $v$ es su **autovector** y $\lambda$ su **autovalor asociado** si y solo s√≠

$$A v= \lambda v, \quad v = \left(v_1, \ldots, v_p \right) \neq 0$$

Esto es equivalente a decir que 

$$A v - \lambda v = 0 \rightarrow (A - \lambda I_{p}) v = 0$$

donde $I_p$ es la matriz identidad de tama√±o $p \times p$. Dicha ecuaci√≥n tiene soluci√≥n si y solo s√≠

$$\left| A - \lambda I_{p} \right| = 0$$

Adem√°s, por el **Teorema Fundamental del Algebra** sabemos que dicho determinante puede expresarse como un polinomio de grado $p$ (conocido como **polinomio caracter√≠stico**)

$$\left| A - \lambda I_{p} \right| = \left(\lambda_1 - \lambda \right)\left(\lambda_2 - \lambda \right) \ldots \left(\lambda_p - \lambda \right) = p (\lambda)$$

Adem√°s el determinante $\left| A \right|$ ser√° el producto de todos sus autovalores.

---

# Primera componente

Recapitulando, para obtener la **primera componente** $\boldsymbol{\Phi}_1$, debemos de 

* **Estandarizar** nuestros datos
* Calcular la **matriz de (cuasi)covarianzas** $\boldsymbol{S}$
* Calcula sus **autovectores** tal que $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$ (normalizados a m√≥dulo 1).

--

Adem√°s si es un autovector de la matriz de covarianzas tenemos entonces que la **varianza maximizada**, la **proporci√≥n de informaci√≥n** que **explica dicha componente**, ser√°

$$\boldsymbol{\Phi}_{1} \left(  S \boldsymbol{\Phi}_{1}^{T} \right) = \boldsymbol{\Phi}_{1} \left( R \boldsymbol{\Phi}_{1}^{T} \right) =\boldsymbol{\Phi}_{1}\left(  \lambda_1 \boldsymbol{\Phi}_{1}^{T} \right) =  \lambda_1 \boldsymbol{\Phi}_{1} \boldsymbol{\Phi}_{1}^{T} =_{\text{ortonormales}} \lambda_1$$ 

--

As√≠ que obtener la direcci√≥n (de todos los autovalores) que mayor informaci√≥n captura nos fijaremos en aquella que tenga **asociada el autovalor m√°s grande**.


$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & \ldots & x_{1, p}  \\ \vdots  & \ddots & \vdots
\\ x_{n,1}  & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$

donde $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$, siendo $\lambda_1$ el mayor de los autovalores de la matriz de (cuasi)covarianzas $S$, y $\boldsymbol{\Phi}_{1}^{T}$ su autovector asociado. El **resto de las componentes** se obtendr√°n de forma similar, siendo ortogonales a cada una de las direcciones obtenidas.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Dados unos datos $\boldsymbol{X}$ de $n$ individuos y $p$ variables, el objetivo es encontrar nuevas **direcciones ortonormales** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$, con $1 \leq k \leq p$, como combinaci√≥n lineal de las variables originales.

* Los **datos son estandarizados**  tal que

$$\begin{pmatrix} \frac{x_{1,1} - \overline{x}_1}{S_{1}} & \frac{x_{1,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{1,p} - \overline{x}_p}{S_{p}} \\  \frac{x_{2,1} - \overline{x}_1}{S_{1}} & \frac{x_{2,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{2,p} - \overline{x}_p}{S_{p}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{x_{n,1} - \overline{x}_1}{S_{1}} & \frac{x_{n,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{n,p} - \overline{x}_p}{S_{p}} \end{pmatrix}$$

* Calcular la **matriz $S$ de (cuasi)covarianzas** de dichos datos estandarizados.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Calculamos los $p$ **autovectores** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_p \right\rbrace$, y sus **autovalores asociados** $\left\lbrace \lambda_1, \ldots, \lambda_p \right\rbrace$, de la matriz $S$, tal que $S  \boldsymbol{\Phi}_k = \lambda_k  \boldsymbol{\Phi}_k$.

* Seleccionamos las primeras $k \leq p$ componentes $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$ asociadas a los primeros $\left\lbrace \lambda_1, \ldots, \lambda_k \right\rbrace$ autovalores.

* La **varianza (informaci√≥n) capturada** por la direcci√≥n $k$-√©sima ser√° igual a $\lambda_k$.

* Las nuevas coordenadas ser√°n

$$\boldsymbol{X}^{'} = \boldsymbol{X} \boldsymbol{\Phi}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2}  \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix} = \begin{pmatrix} \boldsymbol{x}_1 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_1 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_1 \boldsymbol{\Phi}_k^{T} \\ \boldsymbol{x}_2 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_2 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_2 \boldsymbol{\Phi}_k^{T} \\ \vdots & \vdots & \ddots & \vdots \\ \boldsymbol{x}_n \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_n \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_n \boldsymbol{\Phi}_k^{T}\end{pmatrix}$$

---

# Glosario 

* **Autovectores**: nos indican la **direcci√≥n de la componente**

* **Loadings**: ser√°n los **coeficientes** de dichos autovectores (los que generan la combinaci√≥n lineal de las variables originales) nos indican el **peso que tiene cada variable original en dicha componente**. Si por ejemplo $\boldsymbol{\Phi}_1 = \left(0.95, 0.15, 0.273 \right)$, significa que la primera componente (la que m√°s varianza captura) ser√° $0.95* \boldsymbol{X}_1 + 0.15 * \boldsymbol{X}_2 + 0.273 * \boldsymbol{X}_3$, estando dominada por la variable original $\boldsymbol{X}_1$ (un peso de 0.95).


* **Signo de los loadings**: nos indica el **sentido de la relaci√≥n entre la componente y la variable original** (correlaci√≥n positiva/negativa). Si alguno de ellos fuese 0 significar√≠a que la nueva componente est√° incorrelada respecto a dicha variable original.


* **Scores**: para cada observaci√≥n $i$, las nuevas coordenadas $\left(x_{i, 1}^{'}, \ldots, x_{i, k}^{'} \right)$, calculadas tras **proyectar la observaci√≥n original en las direcciones principales** $\boldsymbol{x}_{i} \boldsymbol{\Phi}^{T}$.


* **Truncamiento o n√∫mero de componentes**: para seleccionar el n√∫mero $k$ de componentes a seleccionar el m√©todo m√°s sencillo es **fijar de antemano** un **umbral varianza explicada** que queremos conservar (por ejemplo, $95%$), de forma que nos quedemos con el primer n√∫mero $k$ tal que $\sum_{j=1}^{k} \lambda_k > 0.95$ (**varianza explicada acumulada**, teniendo los autovalores ordenados de mayor a menor).


---


name: practica-PCA
class: center, middle

# Pr√°ctica: PCA en R

---

# PCA en R: caso ¬´manual¬ª


Volvemos a nuestro **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del p√©talo.

```{r}
iris_bi <- 
  tibble(iris) %>%
  select(contains("Petal"))
iris_bi
```

---

# PCA en R: caso ¬´manual¬ª


**Primer paso**: **estandarizar** los datos.

```{r}
iris_bi_std <-
  iris_bi %>%
  mutate(Petal.Length = (Petal.Length - mean(Petal.Length)) /  sd(Petal.Length),
         Petal.Width = (Petal.Width - mean(Petal.Width)) / sd(Petal.Width))

iris_bi_std
```

---

# PCA en R: caso ¬´manual¬ª

.pull-left[
```{r echo = FALSE}
ggplot(iris_bi, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "Datos originales") +
  theme_minimal()
```

]

.pull-right[
```{r echo = FALSE}
ggplot(iris_bi_std, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5, color = "darkolivegreen") +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "Datos estandarizados") +
  theme_minimal()
```

]

---


# PCA en R: caso ¬´manual¬ª

**Segundo paso**: calcular la **matriz de covarianzas**

```{r}
cov_mat <- cov(iris_bi_std)
cov_mat
```

Al estar estandarizados los datos, es equivalente a calcular la matriz de correlaciones

```{r}
library(corrr)
iris_bi_std %>% correlate(diagonal = 1)
```

---

# PCA en R: caso ¬´manual¬ª

**Tercer paso**: calcular los **autovalores y autovectores** de la matriz de covarianzas

```{r}
autoelementos <- eigen(cov_mat)
autoelementos
```

**IMPORTANTE** al tener las **variables estandarizadas**, la **suma de los autovalores** es $p$ (ya que ser√° la suma de las varianzas de las variables que tenemos).

---

# PCA en R: caso ¬´manual¬ª

**Cuarto paso**: **ordenar** autovectores segun autovalores (de mayor a menor)

```{r}
order_lambda <-
  order(autoelementos$values, decreasing = TRUE)
lambda <- autoelementos$values[order_lambda]
PC <- autoelementos$vectors[, order_lambda]
lambda # autovalores ordenadores
```

La **varianza capturada** por $\boldsymbol{\Phi}_1$ es $1.963$ y $0.037$ para la segunda componente $\boldsymbol{\Phi}_2$.

```{r}
PC # autovectores asociados --> direcciones principales
```

---

# PCA en R: caso ¬´manual¬ª


**Quinto paso**: calcular la **varianza explicada acumulada** por cada componente (una vez ordenadas)

```{r}
cumsum(lambda) / sum(lambda)
```

La **primera componente captura el 98.14% de la informaci√≥n (de la varianza)** y la segunda el 1.86% restante.

---

# PCA en R: caso ¬´manual¬ª

* **Sexto paso**: proyectar en las nuevas componentes para obtener las **nuevas coordenadas** (¬°respecto a la nueva base, a las nuevas componentes!)
 

```{r}
iris_pca <- iris_bi_std * t(PC)
names(iris_pca) <- c("PC_1", "PC_2")
iris_pca
```

---

# PCA en R: con prcomp

Dentro de los paquete b√°sicos cargados por `R` tenemos `prcomp` que nos permite realizar los c√°lculos anteriores de manera autom√°tica (`scale. = TRUE` debe ser indicado si los han datos no entran estandarizados previamente).

```{r}
pca <- prcomp(iris_bi, scale. = TRUE)
pca
```

* **Rotation**: la matriz cuyas columnas son las componentes principales $\boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2$ (recuerda que dijimos que est√°bamos ¬´rotando¬ª los datos).

* **Standard deviations**: dado que cada $\lambda_j = {\rm Var} \left(\boldsymbol{\Phi}_j \right)$ representa la varianza de las componentes principales, lo que nos proporciona la salida es $\sqrt{\lambda_j}$, para cada $j=1,\ldots,p$

```{r}
pca$sdev^2 # autovalores
```

---

# PCA en R: con prcomp

```{r}
pca <- prcomp(iris_bi, scale. = TRUE)
pca
```

La **primera componente** viene definida como

$$\boldsymbol{\Phi}_1 = 0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$ 

La **segunda componente** viene definida como 

$$\boldsymbol{\Phi}_2 = -0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$


---

# PCA en R: con prcomp

.pull-left[

En `pca$x` quedan guardados los **scores** o nuevas coordenadas de nuestros datos

```{r}
as_tibble(pca$x)
```

]

.pull-right[

Tambi√©n podemos calcularlas nosotros mismos **proyectando los datos en las nuevas componentes**

```{r}
as_tibble(as.matrix(iris_bi_std) %*%
            pca$rotation)
```

]

---

# Visualizando la transformaci√≥n

.pull-left[

```{r echo = FALSE}
ggplot(iris_bi_std, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5, color = "darkolivegreen") +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS ESTANDARIZADOS") +
  theme_minimal()
```

]

.pull-right[


```{r echo = FALSE}
ggplot(as_tibble(pca$x), aes(x = PC1, y = PC2)) +
  geom_point(size = 5, color = "pink") +
  labs(x = "PC 1", y = "PC 2",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS TRANSFORMADOS") +
  theme_minimal()
```

]

---

# Visualizando la transformaci√≥n


Si ahora pintamos los datos **codificando el color en funci√≥n de la especie** podemos darnos cuenta de por qu√© la primera componente es la que captura pr√°cticamente toda la informaci√≥n.

.pull-left[

```{r echo = FALSE}
ggplot(tibble(iris_bi_std, Species = iris$Species),
       aes(x = Petal.Width, y = Petal.Length,
           color = Species)) +
  geom_point(size = 5) +
  labs(color = "Especies",
       x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS ESTANDARIZADOS") +
  theme_minimal()
```

]

.pull-right[


```{r echo = FALSE}
ggplot(tibble(as_tibble(pca$x), Species = iris$Species),
       aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 5) +
  labs(color = "Especies", x = "PC 1", y = "PC 2",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS TRANSFORMADOS") +
  theme_minimal()
```

]


---

# PCA en R: con factominer y factoextra

Ahora que controlamos un poco c√≥mo se calculan y qu√© significan, vamos a ampliar al dataset entero de iris `{iris}` con sus **4 variables num√©ricas**

```{r}
iris_full <- iris %>% select(-Species)

# Covarianza y correlaci√≥n sin estandarizar antes
library(corrr)
cov(iris_full)
iris_full %>% correlate(diagonal = 1) %>% fashion()
```

---

# PCA en R: con factominer y factoextra

.pull-left[

Las correlaciones tambi√©n podemos **visualizarlas** con el paquete `{corrplot}`

Las variables con mayor correlaci√≥n (positiva adem√°s) es entre la longitud y la anchura del p√©talo.

]

.pull-right[

```{r}
library(corrplot)
corrplot(cor(iris_full), type = "upper", tl.col = "black")
```
]

---


# PCA en R: con factominer y factoextra

Con `{FactoMineR}` podemos calcular con `PCA()` de forma muy sencilla, indic√°ndole que de momento no queremos gr√°ficos, que queremos tantas componentes como variables (luego ya decidiremos con cual nos quedamos) y que estandarice los datos (`scale.unit = TRUE`).

```{r}
library(FactoMineR)
library(factoextra)
pca_fit <-
  PCA(iris_full, scale.unit = TRUE,
      ncp = ncol(iris_full), graph = FALSE)
```

--

Para mostrar los autovalores basta con `pca_fit$eig` (ya nos los da ordenados y con la varianza explicada, tanto componente a componente como acumulada). Tambi√©n se obtienen con `get_eig(pca_fit)`

```{r}
pca_fit$eig # Alternativa: get_eig(pca_fit)
```

---

# PCA en R: con factominer y factoextra

En `pca_fit$svd$V` se guardan los **autovectores o componentes principales (de nuevo por columnas)** asociados a los autovalores que ya tenemos ordenados

```{r}
pca_fit$svd$V
```

Adem√°s con `pca_fit$var$contrib` nos muestra en **porcentaje lo que aporta cada variable** a la varianza explicada por cada componente (la suma de cada columna es el 100%)

```{r}
pca_fit$var$contrib
```

---

# PCA en R: con factominer y factoextra


As√≠ que dar√≠an expresadas los **loadings de las nuevas componentes principales** en funci√≥n de las variables originales (estandarizadas $^*$)

```{r echo = FALSE}
knitr::kable(pca_fit$svd$V,
             col.names = c("Phi_1", "Phi_2",
                           "Phi_3", "Phi_4"),
             digits = 3)
```


$$\boldsymbol{\Phi}_1 = 0.521 * Sepal.Length^* - 0.269 * Sepal.Width^* + 0.580 * Petal.Length^*  + 0.565 * Petal.Width^*$$

$$\boldsymbol{\Phi}_2 = 0.377 * Sepal.Length^*  + 0.923 * Sepal.Width^*  + 0.024 * Petal.Length^*  + 0.067 * Petal.Width^*$$

$$\boldsymbol{\Phi}_3 = -0.719 * Sepal.Length^*  + 0.244 * Sepal.Width^*  + 0.142 * Petal.Length^*  + 0.634 * Petal.Width^*$$

$$\boldsymbol{\Phi}_4 = -0.261 * Sepal.Length^*  + 0.124 * Sepal.Width^*  + 0.801 * Petal.Length^*  - 0.524 * Petal.Width^*$$

---


# PCA en R: con factominer y factoextra

En `pca_fit$ind$coord` tenemos guardados los **scores**, las **nuevas coordenadas de los datos** (los **datos proyectados** en las nuevas direcciones).

```{r}
pca_scores <- as_tibble(pca_fit$ind$coord)
names(pca_scores) <- c("PC_1", "PC_2", "PC_3", "PC_4")
pca_scores # Nuevas coordenadas
```

---

# PCA en R: con factominer y factoextra


Podemos tambi√©n calcular las **covarianzas entre cada componente principal y las variables originales**, tal que $Cov(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \lambda_i z_{i,j}$, donde $z_{i,j}$ es el coeficiente $j$-√©simo de la componente $i$-√©sima (el peso de la variable $\boldsymbol{x}_j$ en la componente $\boldsymbol{\Phi}_i$)

La correlaci√≥n ser√° calculada como ${\rm Cor}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \frac{Cov (\boldsymbol{\Phi}_i, \boldsymbol{x}_j )}{s_{\boldsymbol{\Phi}_i} s_{\boldsymbol{x}_j}} = \frac{\lambda_i z_{i,j}}{\sqrt{\lambda_i} s_{\boldsymbol{x}_j}} = \frac{\sqrt{\lambda_i}}{s_{x_j}} z_{i,j}$

Dichas correlaciones las tenemos guardadas en `pca_fit$var$cor` y representan las **coordenadas de cada variable en cada componente** 

```{r}
pca_fit$var$cor
```

---


# PCA en R: con factominer y factoextra

.pull-left[

En `pca_fit$var$cos2` tenemos las **correlaciones al cuadrado**, que expresan la **proporci√≥n de varianza de cada variable explicada por cada componente**

```{r}
round(pca_fit$var$cos2, 3)
```

Con `fviz_pca_var` podemos **visualizar de forma bidimensional** como se relacionan las variables originales con las dos componentes que mayor cantidad de varianza capturan.

La **primera componente captura** sobre todo las **dos variables del p√©talo** (dichas variables pr√°cticamente est√°n sobre la horizontal de la primera componente). La **segunda componente** captura ligeramente el s√©palo, aunque longitud del s√©palo es la peor variable representada de todas.

]

.pull-right[

```{r out.width = "93%"}
col <- c("#00AFBB", "#E7B800", "#FC4E07")
fviz_pca_var(pca_fit, col.var = "cos2",
             gradient.cols = col,
             repel = TRUE) +
  theme_minimal() + 
  labs(title = "Coordenadas de las variables",
       color = "Prop. var. explicada")
```
]



---


# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_cos2()` podemos mostrar el **porcentaje de la varianza de las variables que es explicada** por las componentes que le indiquemos en `axes`

As√≠ podemos apreciar que todas las 
**dos primeras componentes ya son capaces de capturar** al menos el 75% de la varianza de todas y cada una de las variables, rozando el 100% en las variables `Sepal.Width` y `Petal.Length`

]

.pull-right[

```{r out.width = "93%"}
fviz_cos2(pca_fit, choice = "var",
          axes = 1:2)
```
]

---

# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_eig()` podemos visualizar la varianza explicada por cada componente

```{r eval = FALSE}
fviz_eig(pca_fit,
         barfill = "darkolivegreen",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

]

.pull-right[

```{r echo = FALSE}
fviz_eig(pca_fit,
         barfill = "darkolivegreen",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

]

---

# PCA en R: con factominer y factoextra



.pull-left[

Tambi√©n podemos visualizar la **varianza acumulada** de forma manual

```{r eval = FALSE}
cumvar <- as_tibble(pca_fit$eig)
names(cumvar) <- c("lambda", "var", "cumvar")

ggplot(cumvar, aes(x = 1:4, y = cumvar)) +
  geom_col(fill = "pink") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

]

.pull-right[

```{r echo = FALSE}
cumvar <- as_tibble(pca_fit$eig)
names(cumvar) <- c("autovalor", "var", "cumvar")

ggplot(cumvar, aes(x = 1:4, y = cumvar)) +
  geom_col(fill = "pink") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

]

---

# PCA en R: con factominer y factoextra

.pull-left[

Por √∫ltimo `fviz_pca_biplot()` nos permite visualizar en las dos dimensiones que m√°s varianza capturan, e incluso nos permite **visualizar cl√∫sters** de observaciones con las elipses definidas por las matrices de covarianza de cada uno de los grupos.

```{r eval = FALSE}
fviz_pca_biplot(pca_fit,
                col.ind = iris$Species,
                palette = "jco",
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE,
                legend.title = "Especies")
```

Observamos de nuevo como la componente determinante es la primera, que nos discrimina perfectamente la especie de Setosa.

]

.pull-right[

```{r echo = FALSE}
fviz_pca_biplot(pca_fit, col.ind = iris$Species, palette = "jco",
                addEllipses = TRUE, label = "var", col.var = "black", repel = TRUE,
legend.title = "Especie")
```

]

---

# PCA con tidymodels

Por √∫ltimo, ahora que entendemos las componentes principales vamos a calcularlas haciendo uso de `{tidymodels}`, un conjunto de paquetes y herramientas para obtener un **√∫nico flujo de trabajo en el preprocesamiento, modelizaci√≥n y evaluaci√≥n** de modelos.



```{r tidymodels, echo = FALSE, out.width = "75%", fig.align = "center", fig.cap = "Infograf√≠a extra√≠da de https://www.tmwr.org/dimensionality.html"}
knitr::include_graphics("https://www.tmwr.org/premade/data-science-model.svg")
``` 

```{r}
library(tidymodels)
```

---


# Filosof√≠a tidymodels

La idea detr√°s de `{tidymodels}` es **tratar por separado** la depuraci√≥n y preparaci√≥n de los datos, el modelo o paradigma de aprendizaje que se quiere aplicar, la optimizaci√≥n de los par√°metros de dicho modelo, el ajuste, la evaluaci√≥n y la predicci√≥n correspondiente, **creando un flujo de trabajo muy flexible**. La **filosof√≠a es la misma que hay detr√°s de cocinar un plato**:

1. Primero **escribimos la RECETA**, una lista de pasos e instrucciones.
2. Despu√©s **preparamos HERRAMIENTAS y utensilios para cocinar** (nuestro modelo).
3. Con la **RECETA + HERRAMIENTAS** podemos cocinar el plato muchas veces, con **distintos lotes de ingredientes (datos)**.

&nbsp;

```{r tidymodels2, echo = FALSE, out.width = "85%", fig.align = "center"}
knitr::include_graphics("https://www.tmwr.org/premade/recipes-process.svg")
``` 


---

# Ejemplo tidymodels: algoritmo knn

.pull-left[

Los datos utilizados forman parte de un cojunto de reservas de hotel elaborado por [Antonio et al., 2019](https://www.sciencedirect.com/science/article/pii/S2352340918315191?via%3Dihub). El **conjunto utilizado tiene 50 000 registros de reservas**, con algunas de las siguientes [variables](https://linkinghub.elsevier.com/retrieve/pii/S2352340918315191)

* `hotel`: nombre del hotel.
* `lead_time`: lapso de tiempo entre la reserva y la estancia.
* `children`: si la reserva tiene ni√±os o no.
* `meal`: r√©gimen de comidas.
* `country`: pa√≠s de origen.
* `average_daily_rate`: tarifa media diaria.
* `arrival_date`: fecha de llegada.

]

.pull-right[

El **objetivo es predecir si una serva incluye ni√±os/as o no**.

```{r carga-datos}
hoteles_raw <- read_csv("./DATOS/hotels.csv")
glimpse(hoteles_raw)
```

]

---

# Ejemplo tidymodels: algoritmo knn


### **Muestreo inicial**

Una de las primeras acciones que quiz√°s queremos realizar es un **muestreo inicial** de los datos

* **Muestreo inicial**: `slice_sample` realiza un **muestreo aleatorio**, indic√°ndole en `prop` el porcentaje de datos a extraer (**reminder**: los que no selecciones no continuan a lo largo del flujo).


```{r warning = FALSE}
# Sample inicial: nos quedamos el 15% de los datos
hoteles <- hoteles_raw %>% slice_sample(prop = 0.15)
dim(hoteles_raw)
dim(hoteles)
```

---

# Ejemplo tidymodels: algoritmo knn


### **Partici√≥n train/test**

Otra acci√≥n habitual ser√° obtener varios **subconjuntos**:

* `train`: para entrenar el algoritmo.
* `validation`: para validar el modelo (quiz√°s haya que volver a `train`)
* `test`: para puntuar nuestro algoritmo (con datos que no han sido usados ni para entrenar ni para validar)

Dado que **tenemos muy pocos 1's en nuestra variable objetivo**, debemos intentar preservar los m√°ximos posibles (para que no nos quede un conjunto a√∫n m√°s desbalanceado).

```{r}
# Objetivo: predecir si la reserva tiene ni√±os o no
hoteles_raw %>%
  count(children) %>%
  mutate(porc = 100 * n / sum(n))
```

---

# Ejemplo tidymodels: algoritmo knn


### **Partici√≥n train/test**

Realizaremos una **partici√≥n train-test** estratificado por la variable `children`: repartiremos en **80-20%** los datos **PERO asegur√°ndonos de que la proporci√≥n de 0's-1's se preserva**.

```{r}
# Partici√≥n 80-20%: estratificada para que mantenga 1/0
hoteles_split <- initial_split(hoteles, strata = children, prop = 0.8)
hoteles_split
```

&nbsp;

En `hoteles_split` tenemos **guardada las instrucciones** de lo que queremos hacer cuando lo apliquemos, pero hasta que no se aplique no realiza ninguna acci√≥n.

```{r split}
hotel_train <- training(hoteles_split)
hotel_test <- testing(hoteles_split)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**

* Asignar roles a variables
* Recategorizar
* Procesar outliers
* Datos ausentes
* Fechas
* Estandarizaci√≥n por rango y/o normalizaci√≥n (tipificaci√≥n)

La lista de arriba son **algunas de las opciones que quiz√°s querramos hacer** con nuestras variables antes de poder aplicar un modelo.

&nbsp;

Lo que haremos ser√° indicarle una **lista de instrucciones**, algo as√≠ como la **receta escrita que tenemos guardada en un caj√≥n** para preparar un plato. La receta por s√≠ sola no se pone a cocinarte men√∫s, simplemente es una lista de instrucciones, lista para cuando la necesites.

Las funciones que empiezan por `step_` tienen implementadas muchas de las funcionalidades que podemos realizar en depuraci√≥n con `{tidyverse}`. La diferencia al incluirlo en la receta es que **se ejecutar√° cada vez que dicha receta se aplique (tanto a train como a test)**.

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).

```{r}
receta <-
  recipe(data = hotel_train, children ~ .)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.

```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor))
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), a√±adir rol am√©n del existente (`add_role()`), eliminar rol (`remove_role()`)

```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel") 
```


---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), a√±adir rol am√©n del existente (`add_role()`), eliminar rol (`remove_role()`).
* **Recategorizar variables**: vamos a pasar a binaria `is_repeated_guest` y convertiremos a factor `previous_cancellations`, `reserved_room_type`, `assigned_room_type`, `booking_changes`, `deposit_type` y `days_in_waiting_list`. Con `step_other()` adem√°s le vamos a indicar que si en una **variable cualitativa hay alg√∫n nivel en proporciones √≠nfimas (por ejemplo, representa menos del 0.5% del total) las reagrupe** en una categor√≠a llamada ¬´others¬ª.

```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel") %>%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %>%
  # Indicamos % m√≠nimo en categor√≠a (si < 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**: actualizar rol de la variable (`update_role()`), a√±adir rol am√©n del existente (`add_role()`), eliminar rol (`remove_role()`).
* **Recategorizar variables**.
* **Estandarizar por rango** con `step_range()` (siempre entre 0 y 1, aunque podr√≠amos asignarle otro rango dando valores a `max` y `min`). Con `step_normalize()` normalizar√≠amos (media nula y varianza unitaria).


```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel")  %>%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %>%
  # Indicamos % m√≠nimo en categor√≠a (si < 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo).
* **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**.
* **Recategorizar variables**.
* **Estandarizar por rango**.
* **Rebalancear objetivo**: en nuestros datos tan solo tenemos cerca del 8% de registros con `children = children` (1's). Usaremos el **sobremuestreo**: duplicar algunos registros con 1 en la variable objetivo para que **equilibrar** la proporci√≥n de 0's vs 1's. Para ello basta con a√±adir en nuestra receta el paso `step_upsample` del paquete `{themis}`, indic√°ndole en `over_ratio` la relaci√≥n que queremos entre la clase minoritaria y la mayoritaria.


```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel")  %>%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %>%
  # Indicamos % m√≠nimo en categor√≠a (si < 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>%
  themis::step_upsample(children, over_ratio = 0.5)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo). **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**. **Recategorizar variables**.
* **Estandarizar** por rango. **Rebalancear** objetivo.
* **Tratar datos at√≠picos y missings**.

```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel")  %>%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %>%
  # Indicamos % m√≠nimo en categor√≠a (si < 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>%
  themis::step_upsample(children, over_ratio = 0.5) %>%
  step_mutate_at(all_numeric_predictors(),
                 fn = function(x) { ifelse(abs(x - mean(x, na.rm = TRUE)) > 5 * sd(x, na.rm = TRUE), NA, x)}) %>%
  # Imputamos por la media las num√©ricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors())
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**


* **Variable objetivo**: `children` ser√° nuestra outcome (variable objetivo). **Convertimos a cualitativa** las variables de texto.
* **Roles de variables**. **Recategorizar variables**.
* **Estandarizar** por rango. **Rebalancear** objetivo.
* **Tratar datos at√≠picos y missings**. Convertimos **cualitativas a dummy**.

```{r}
receta <-
  recipe(data = hotel_train, children ~ .) %>% 
  step_mutate(across(where(is.character), as.factor)) %>%
  add_role(hotel, new_role = "type_hotel")  %>%
  step_mutate(is_repeated_guest = (is_repeated_guest == 1),
              across(previous_cancellations:days_in_waiting_list,
                     as.factor)) %>%
  # Indicamos % m√≠nimo en categor√≠a (si < 0.5%, reagrupa a others)
  step_other(all_nominal_predictors(), threshold = 0.005) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>%
  themis::step_upsample(children, over_ratio = 0.5) %>%
  step_mutate_at(all_numeric_predictors(),
                 fn = function(x) { ifelse(abs(x - mean(x, na.rm = TRUE)) > 5 * sd(x, na.rm = TRUE), NA, x)}) %>%
  # Imputamos por la media las num√©ricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>% 
  # nominales las pasemos a dummy 1/0 (country tambi√©n)
  step_dummy(all_nominal(), -all_outcomes())
```

---

# Ejemplo tidymodels: algoritmo knn

### **Receta sencilla (recipe)**

```{r}
receta
```

---

# Ejemplo tidymodels: algoritmo knn

### **Horneado (bake)**

Tras escribir la receta vamos a **prepararla** y a **hornear los datos** con `bake()`: para hornearla en el conjunto de train, basta con poner `new_data = NULL`.

```{r bake}
# Aplicado a train
bake(receta %>% prep(), new_data = NULL)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Horneado (bake)**

Para hornearla en el conjunto de test basta con poner `new_data = hotel_test`. Nuestra receta, aplicada a nuestros ingredientes, est√° lista. Este ¬´horneado¬ª con `bake()` solo lo necesitamos **si queremos ya aplicar la receta a nuestros datos**.

```{r bake-est}
bake(receta %>% prep(), new_data = hotel_test)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Utensilios (modelo)**

Una vez que tenemos nuestra lista de instrucciones, lo siguiente que har√≠amos al **cocinar un plato** es buscar los utensilios: cuchillos, cacerolas, batidora, etc. En nuestro caso los **utensilios ser√°n nuestro modelo (en este caso de clasificaci√≥n)**, con `nearest_neighbor()` (echa un vistazo a los modelos del paquete `{parsnip}`). 

- `mode`: admite dos opciones, `mode = "classification"` o `mode = "regression"`.
- `neighbors`: el n√∫mero de vecinos `k` que consideramos como entorno de vecindad.
- `weight_func`: funci√≥n (kernel) para promedir distancias (`weight_func = "inv"` nos promedia por el inverso de la distancia; ver opciones en <https://epub.ub.uni-muenchen.de/1769/>)
- `dist_power`: n√∫mero $p$ de la distancia de Minkowski

```{r knn}
knn_model <-
  nearest_neighbor(mode = "classification", neighbors = 10,
                   weight_func = "inv", dist_power = 2) %>%
  set_engine("kknn") # el ¬´motor¬ª que realiza el ajuste
knn_model
```

---

# Ejemplo tidymodels: algoritmo knn

### **Flujo de trabajo**


Con dichos ingredientes podemos crear ya un **flujo de trabajo** con `workflow()`

```{r flujo}
# Flujo de trabajo
hoteles_wflow <-
  workflow() %>%
  add_recipe(receta) %>%
  add_model(knn_model)
hoteles_wflow
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicci√≥n**

Dichos pasos vamos a **proporcion√°rselos a nuestro conjunto de entrenamiento** para que nos **aplique el flujo de trabajo** que hemos construido, usando `fit(data = hotel_train)`.

```{r fit-knn-1}
# Aplicamos flujo
library("kknn")
hoteles_knn_fit <-
  hoteles_wflow %>% fit(data = hotel_train)
hoteles_knn_fit
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicci√≥n**

Dicho ajuste guardado en `hoteles_knn_fit` podemos usarlo para **predecir el conjunto test** de dos maneras:

* **Clase**
* **Probabilidades** (de pertenencia a dichas clases, necesario para el c√°lculo de la **curva ROC**)

Ambas se hacen con `predict()` pas√°ndole un argumento de `type` diferente. **Reminder**: al tenerlo todo integrado en un flujo, **aplicar√° el procesamiento que necesite al conjunto de test**, tal cual lo hemos indicado en las instrucciones.

```{r predict-knn-1}
# Predecir el conjunto test: devuelve la clase
predict(hoteles_knn_fit, hotel_test)

# Predecir las probabilidades (las necesitamos para la ROC)
predict(hoteles_knn_fit, hotel_test, type = "prob")
```

---

# Ejemplo tidymodels: algoritmo knn

### **Ajuste y predicci√≥n**

Dentro del paquete `{parsnip}` que hemos cargado dentro de `{tidymodels}` tenemos a nuestra disposici√≥n una funci√≥n llamada `augment()` que nos permite **incluir en una misma tabla las predicciones de la clase, de las probabilidades y los datos de test originales** (a√±adiendo columnas).

```{r augment}
# Para obtener las probabilidades en los datos (con variables)
prob_test <- augment(hoteles_knn_fit, hotel_test)
print(prob_test, width = Inf)
```

---

# Ejemplo tidymodels: algoritmo knn

### **Evaluaci√≥n**

En realidad el conjunto de test solo deber√≠amos usarlo al final del proceso, no como evaluaci√≥n intermedia de los hiperpar√°metros, ya que dicho rol le corresponde a un **tercer conjunto de validaci√≥n**, pero de momento vamos a simplificarlo en train-test.

Una de las formas m√°s sencillas de **evaluar un m√©todo de clasificaci√≥n es con una matriz de confusi√≥n**: una matriz que nos cruce las frecuencias de las etiquetas reales frente a las predichas.

```{r}
# Matriz de confusi√≥n: etiqueta real vs etiqueta predicha
conf_mat_test <-
  prob_test %>% conf_mat(truth = children, estimate = .pred_class)

# La guardamos en una tabla
conf_mat_test <- as_tibble(conf_mat_test$table)
conf_mat_test 
```

---


# Ejemplo tidymodels: algoritmo knn

### **Evaluaci√≥n**

Muchas de las **m√©tricas las podemos obtener autom√°ticamente** de la matriz de confusi√≥n.

```{r}
# Matriz de confusi√≥n + resumen: etiqueta real vs etiqueta predicha
metricas <-
  prob_test %>% conf_mat(truth = children, estimate = .pred_class) %>%
  summary()
metricas
```

---

# PCA con tidymodels

Ahora que hemos visto por encima la idea de `{tidymodels}` vamos a calcular las componentes principales del conjunto `iris` con dicha idea.


```{r iris-cor, out.width = "3%"}
iris_ful7 <- iris %>% select(-Species)
library(corrplot)
iris_full %>% cor() %>% 
  corrplot(tl.col = "black", method = "ellipse")
```

---

# PCA con tidymodels

La receta tendr√° los siguientes pasos:

* Indicar la **variable objetivo** (`Species`)
* Imputamos **datos ausentes**
* **Estandarizar/normalizar** los datos
* Eliminamos variables de **cero varianza**

```{r}
receta <- 
  recipe(Species ~ ., data = iris) %>%
  # Imputamos por la media las num√©ricas, por la moda las cuali
  step_impute_mean(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # Estandarizamos
  step_normalize(all_numeric_predictors()) %>%
  # Filtro cero varianza
  step_zv(all_numeric_predictors())
receta
```

---

# PCA con tidymodels

Para a√±adir el **an√°lisis de componentes principales** basta con a√±adir `step_pca()`

```{r}
receta <-
  receta %>%
  step_pca(all_numeric_predictors(), num_comp = 4,
           prefix = "PC") 
```
 
Con el argumento `num_comp = 4` ind√≠camos el n√∫mero de componentes y con `prefix` el prefijo con el que llamaremos a las nuevas variables

---

# PCA con tidymodels

```{r}
data_pc <- bake(receta %>% prep(), new_data = NULL)
data_pc
```

---

# PCA con tidymodels

.pull-left[

```{r eval = FALSE}
ggplot(data_pc %>% select(-Species),
       aes(x = .panel_x, y = .panel_y,
           color = Species, fill = Species)) +
    geom_point(alpha = 0.4, size = 0.7) +
    ggforce::geom_autodensity(alpha = 0.3) +
    ggforce::facet_matrix(layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
```

]

.pull-right[

```{r echo = FALSE}
ggplot(data_pc,
       aes(x = .panel_x, y = .panel_y,
           color = Species, fill = Species)) +
  geom_point(alpha = 0.4, size = 0.9) +
  ggforce::geom_autodensity(alpha = 0.3) +
  ggforce::facet_matrix(vars(-Species), layer.diag = 2) + 
  scale_color_brewer(palette = "Dark2") + 
  scale_fill_brewer(palette = "Dark2") +
  theme_minimal() +
  labs(title = "PCA con tidymodels")
```

]


---
 
# PCA con tidymodels

.pull-left[

```{r eval = FALSE}
library(learntidymodels)
receta %>% prep() %>% 
  plot_top_loadings(component_number <= 4, n = 4) + 
  scale_fill_brewer(palette = "Paired")
```

]

.pull-right[

```{r echo = FALSE}
library(learntidymodels)
receta %>% prep() %>% 
  plot_top_loadings(component_number <= 4, n = 4) + 
  scale_fill_brewer(palette = "Paired")
```

]

---

class: inverse center middle

# BLOQUE II. An√°lisis cl√∫ster

&nbsp;


### [¬øQu√© es el an√°lisis cl√∫ster?](#intro-cluster)

### [M√©tricas](#metricas)

### [Algoritmos jer√°rquicos](#jerarquicos)

### [Algoritmos no jer√°rquicos](#no-jerarquicos)

### [Determinaci√≥n del n√∫mero de grupos](#n-grupos)


---

name: intro-cluster
class: center, middle

# ¬øQu√© es el an√°lisis cl√∫ster?

---

# Introducci√≥n: ¬øan√°lisis cl√∫ster?

El **an√°lisis cl√∫ster** forma parte de los **algoritmos de agrupaci√≥n o clasificaci√≥n** con **aprendizaje no supervisado**:

* **Clasificaci√≥n supervisada**: los individuos se clasifican en un grupo a partir de la
informaci√≥n de un conjunto de variables observadas de unos inviduos **cuyo grupo conocemos**, los **datos est√°n etiquetados** (sabemos que es acierto y error). Es el caso por ejemplo del **an√°lisis discriminante**.


```{r supervised, echo = FALSE,  out.width = "80%", fig.align = "right"}
knitr::include_graphics("./img/supervised_classification.png")
``` 


---

# Introducci√≥n: ¬øan√°lisis cl√∫ster?

El **an√°lisis cl√∫ster** forma parte de los **algoritmos de agrupaci√≥n o clasificaci√≥n** con **aprendizaje no supervisado**:


* **Clasificaci√≥n/agrupaci√≥n no supervisada**: los individuos tambi√©n se clasifican en un grupo a partir de la informaci√≥n de un conjunto de variables observadas PERO en esta ocasi√≥n **no sabemos a qu√© grupo pertenece cada individuo a priori**, no tenemos conocimiento de qu√© es acierto y qu√© es error. Es este el caso del **an√°lisis cl√∫ster**.


```{r unsupervised, echo = FALSE,  out.width = "80%", fig.align = "right"}
knitr::include_graphics("./img/unsupervised_classification.png")
``` 

---

# Introducci√≥n: ¬øan√°lisis cl√∫ster?

### **Objetivo**

El an√°lisis cl√∫ster tiene como principal objetivo **encontrar grupos** dentro de los individuos, de forma que los individuos de cada grupo sean **lo m√°s parecidos entre s√≠** (homogeneidad interna) y **lo m√°s diferentes a los individuos de otros grupos** (heterogeneidad entre grupos)


&nbsp;

### **Notaci√≥n**

* $n$ tama√±o muestral (n√∫mero de individuos --> filas).

* $\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)$ conjunto de $p$ variables (--> columnas) medidas para cada individuo $i=1,\ldots,n$.

* Nuestros datos estar√°n en forma de tabla o matriz $\boldsymbol{X}$ de $n$ filas y $p$ columnas (con $p \ll n$)
 
---
 
# Introducci√≥n: ¬øan√°lisis cl√∫ster?
 
Los algoritmos los dividiremos en dos grandes grupos:


* **Algoritmos jer√°rquicos**: algoritmos que tienen como objetivo construir una jerarqu√≠a de grupos. Existen principalmente dos estrategias:
  - **Aglomerativa**: cada individuo empieza siendo su propio grupo, mientras se van uniendo de forma secuencial ascendiendo en la jerarqu√≠a, hasta acabar con un solo grupo que incluya todas las observaciones.
  - **Divisiva**: se construye una jerarqu√≠a descendiente, empezando con un √∫nico grupo hasta acabar con un grupo por individuo

Los m√©todos jer√°rquicos suelen ser muy **explicativos** y f√°ciles de visualizar (por ejemplo, con un **dendograma**) pero **muy costoso computacionalmente** (ya que siempre se construye la jerarqu√≠a entera, para decidir luego d√≥nde cortar)

&nbsp;

* **Algoritmos no jer√°rquicos**: dado un n√∫mero $K$ de grupos fijado a priori, se pretenden agrupar los datos de forma que obtengamos finalmente $K$ agrupaciones de los mismos.

---

name: metricas
class: center, middle

# M√©tricas


---

# M√©tricas

En los algoritmos usados ser√° clave el concepto de **m√©trica**. Los datos ser√°n agrupados en base a los conceptos de ¬´lejos¬ª y ¬´cerca¬ª, o mejor dicho, en base a ¬´parecido¬ª y ¬´diferente¬ª

**¬øQu√© es ser parecido? ¬øY diferente?**

&nbsp;

### **M√©trica entre observaciones**

Para medir distancias entre los individuos tenemos principalmente dos alternativas:

* **Distancias geom√©tricas**: distancias que miden la distancia entre dos individuos como si fuesen dos puntos en un espacio geom√©trico. Son distancias determin√≠sticas.

* **Distancias probabil√≠sticas**: distancias que miden la distancia entre individuos teniendo en cuenta la distribuci√≥n de las variables y su dependencia.

---

# M√©tricas determin√≠sticas


* **Distancia eucl√≠dea bidimensional**: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{(x_{i, 1} - x_{j,1})^2 + (x_{i, 2} - x_{j, 2})^2}$
   

* **Distancia eucl√≠dea multidimensional**: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} (x_{i,k} - x_{j,k})^2}$

* **Distancia Manhattan**:  $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \displaystyle \sum_{k=1}^{p} \left| x_{i,k} - x_{j,k} \right|$

* **Distancia de Minkowski**: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left(\displaystyle \sum_{k=1}^{p} \left| x_{i,k} - x_{j,k} \right|^l\right)^{1/l}$ (si $l=1$ es Manhattan, si $l=2$ es Eucl√≠dea)
   
Cuando usemos este tipo m√©tricas es **muy importante** **reescalar por rango**: transformamos para crear nuevas observaciones $\widetilde{x}_{i, k} = \frac{x_{i, k} - min(\boldsymbol{x}_i)}{max(\boldsymbol{x}_i) - min(\boldsymbol{x}_i)}$ de forma que todas las variables est√©n entre 0 y 1 (y as√≠ todas tengan el mismo peso dentro de las m√©tricas).

---

# M√©tricas determin√≠sticas

```{r mink, echo = FALSE,  out.width = "65%", fig.align = "center"}
knitr::include_graphics("./img/minkowski.png")
``` 

```{r mink2, echo = FALSE,  out.width = "65%", fig.align = "center"}
knitr::include_graphics("./img/minkowski_1.png")
``` 


---

# M√©tricas probabil√≠sticas

* **Distancia de Mahalanobis** multidimensional **(variables independientes)**:  $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\frac{x_{i, k} - x_{j,k}}{\sigma_i} \right)^2}$ que se puede aproximar por $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\frac{x_{i, k} - x_{j,k}}{S_i} \right)^2}$

* **Distancia de Mahalanobis** multidimensional **(variables dependientes)**:  $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)^{T} \Sigma^{-1} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)}$ que se puede aproximar por $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{\displaystyle \sum_{k=1}^{p} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)^{T} S^{-1} \left(\boldsymbol{x}_i - \boldsymbol{x}_j \right)}$, donde $\Sigma$ es la matriz de covarianzas, con $S$ matriz de (cuasi)covarianzas.
  
Cuando usemos este tipo m√©tricas es **muy importante** **estandarizar** para tener variables de media $0$ y varianza $1$.

---

# M√©tricas

### **M√©trica entre variables**

Adem√°s de medir la distancia entre individuos podemos plantearnos medir la **distancia entre las propias variables**, normalmente basadas en la correlaci√≥n de Pearson o sus derivados.

* **Correlaci√≥n de Pearson**: definida como $d(\boldsymbol{X}_1, \boldsymbol{X}_2) = 1 - \left| r_{1,2} \right|$ donde $r_{1,2}$ se define como la correlci√≥n entre las variables $\boldsymbol{X}_1$ y $\boldsymbol{X}_2$

* **Correlaci√≥n de Spearman**: definida como $d(\boldsymbol{X}_1, \boldsymbol{X}_2) = 1 - \left| \rho_{1,2} \right| = 1 - \left| \frac{6 \sum D^{2}}{n (n^{2}-1)} \right|$, donde $D$ es la diferencia entre los correspondientes estad√≠sticos de orden.


* **Correlaci√≥n de Kendall**: definida como  $d(\boldsymbol{X}_1, \boldsymbol{X}_2) = 1 - \left| \tau_{1,2} \right| =  1- \left| \frac{n_c - n_d}{\frac{1}{2} n (n-1)} \right|$ donde $n_c$ es el n√∫mero de pares concordantes (si el orden de clasificaci√≥n de $(x_{i},x_{j})$ y $(y_{i},y_{j})$ coinciden, tal que $x_{i}>x_{j}$ y $y_{i}>y_{j}$, donde $i<j$) y $n_d$ es el n√∫mero de pares discordantes

---

name: jerarquicos
class: center, middle

# Visualizaci√≥n previa

---

# Visualizaci√≥n previa

Vamos a empezar a trabajar con un conjunto familiar, el conjunto de `iris` que ya conocemos haciendo una selecci√≥n previa de 35 observaciones

```{r}
iris_sample <- iris %>% slice_sample(n = 35)
iris_sample 
```

---

# Visualizaci√≥n previa


Una primera visualizaci√≥n la podemos realizar con `heatmaply()` del paquete hom√≥nimo que nos permite realizar agrupaciones tanto de variables (dendograma superior) como de individuos (dendograma en el lateral).

```{r eval = FALSE}
library(heatmaply)
heatmaply(iris_sample,
          seriate = "mean",
          row_dend_left = TRUE,
          plot_method = "plotly")
```

Es importante recordar que el **clustering** es un m√©todo de **clasificaci√≥n NO SUPERVISADA**: no sabemos a priori el grupo correcto de cada individuo.

---

# Visualizaci√≥n previa

```{r echo = FALSE}
library(heatmaply)
heatmaply(iris_sample,
          seriate = "mean",
          row_dend_left = TRUE,
          plot_method = "plotly")
```


---


# Visualizaci√≥n previa


Podemos mejorar la representaci√≥n **estandarizando** las variables.

```{r eval = FALSE}
heatmaply(iris_sample %>%  mutate(across(where(is.numeric), ~scale(.))),
          seriate = "mean", row_dend_left = TRUE, plot_method = "plotly")
```

---


# Visualizaci√≥n previa

```{r echo = FALSE}
heatmaply(iris_sample %>%
            mutate(across(where(is.numeric), ~scale(.))),
          seriate = "mean",
          row_dend_left = TRUE,
          plot_method = "plotly")
```


---

# Visualizaci√≥n previa

Con `seriate = ...` podemos cambiar el m√©todo de ordenaci√≥n para una mejor visualizaci√≥n

```{r eval = FALSE}
heatmaply(iris_sample %>% 
            mutate(across(where(is.numeric), ~scale(.))),
          seriate = "OLO",
          row_dend_left = TRUE,
          plot_method = "plotly")
```

---

# Visualizaci√≥n previa

```{r echo = FALSE}
heatmaply(iris_sample %>%
            mutate(across(where(is.numeric), ~scale(.))),
          seriate = "OLO",
          row_dend_left = TRUE,
          plot_method = "plotly")
```

---

# Visualizaci√≥n previa

Otra forma de ver las relaciones entre individuos es calcular su **matriz de distancias** con `dist()`, indic√°ndole en `method = ...` el tipo de m√©trica.


```{r}
d <- dist(iris_sample %>%
            select(-Species) %>% 
            mutate(across(where(is.numeric), ~scale(.))),
          method = "euclidean")
d
```

---

# Visualizaci√≥n previa

.pull-left[

Con `fviz_dist()` podremos visualizar dicha matriz de distancias.

```{r eval = FALSE}
fviz_dist(d, show_labels = TRUE)
```

]

.pull-right[


```{r echo = FALSE}
fviz_dist(d, show_labels = TRUE) +
  labs(title = "Matriz de distancias (estandarizadas)")
```

]


---

name: jerarquicos
class: center, middle

# Algoritmos jer√°rquicos

---

# Algoritmos jer√°rquicos: simple

.pull-left[

En nuestro caso empezaremos usando solo **algoritmos aglomerativos**, de forma que empezaremos con un cl√∫ster por individuo, e iremos **juntando cl√∫ster** en base a distintos enlaces

* **Enlace simple (single)** o del vecino m√°s cercano: la distancia entre dos cl√∫steres ser√° definida como la **distancia m√≠nima** entre pares de observaciones (cada una perteneciente a uno de los dos cl√∫steres)

$$d(C_k, C_l) = \min_{x_i \in C_k, x_j \in C_l} d(x_i, x_j)$$

```{r eval = FALSE}
# Clustering (single)
single_clust <-
  hclust(d, method = "single")

# Dendograma
library(factoextra)
fviz_dend(single_clust, cex = 1) +
  labs(title = "Dendograma (single)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (single)
single_clust <- hclust(d, method = "single")

# Dendograma
library(factoextra)
fviz_dend(single_clust, cex = 1) +
  labs(title = "Dendograma para clustering single")
```

]

---

# Algoritmos jer√°rquicos: simple

.pull-left[

Con `cutree()` podemos indicarle el corte del n√∫mero de cl√∫sters que queremos

```{r}
single_clust <-
  hclust(d, method = "single")

# Seleccionamos 3 clusters
groups <- cutree(single_clust, k = 3)
```

Con `fviz_dend()` podemos visualizar el dendograma

```{r eval = FALSE}
# k = 3
fviz_dend(single_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (simple)")
```

]

.pull-right[

```{r echo = FALSE}
# k = 3
fviz_dend(single_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (clustering simple)")
```

]

---

# Algoritmos jer√°rquicos: simple

.pull-left[

Con `fviz_cluster()` visualizaremos los cl√∫sters en base a las dos componentes que m√°s varianza capturen

```{r eval = FALSE}
# Estandarizamos datos
iris_scale <-
  iris_sample %>%
  select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (single)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
iris_scale <-
  iris_sample %>%
  select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (single)") +
  theme_minimal()
```

]

---

# Algoritmos jer√°rquicos: simple

.pull-left[

As√≠ quedar√≠a con todas las observaciones

```{r eval = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (single)
single_clust <-
  hclust(d, method = "single")
groups <- cutree(single_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (single)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
iris_scale <-
  iris %>%
  select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <- dist(iris_scale, method = "euclidean")

# Clustering (single)
single_clust <-
  hclust(d, method = "single")
groups <- cutree(single_clust, k = 3)

fviz_cluster(list(data = iris_scale, cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (single)") +
  theme_minimal()
```

]

---

# Algoritmos jer√°rquicos: complete

.pull-left[

* **Enlace completo (complete)** o del vecino m√°s alejado (complete): la distancia entre dos cl√∫steres ser√° definida como la **distancia m√°xima** entre pares de observaciones (cada una perteneciente a uno de los dos cl√∫steres)

$$d(C_k, C_l) = \max_{x_i \in C_k, x_j \in C_l} d(x_i, x_j)$$

```{r eval = FALSE}
# Clustering (complete)
complete_clust <-
  hclust(d, method = "complete")

# k = 3
fviz_dend(complete_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (complete)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (complete
complete_clust <- hclust(d, method = "complete")

# k = 3
fviz_dend(complete_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (complete)")
```

]


---

# Algoritmos jer√°rquicos: complete

.pull-left[

```{r eval = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (single)
complete_clust <-
  hclust(d, method = "complete")
groups <- cutree(complete_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (complete)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (single)
complete_clust <-
  hclust(d, method = "complete")
groups <- cutree(complete_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (complete)") +
  theme_minimal()
```

]


---

# Algoritmos jer√°rquicos: average

.pull-left[

* **Enlace medio (average)**: la distancia entre dos cl√∫steres ser√° definida como la **distancia media** entre observaciones de distintos grupos

$$d(C_k, C_l) = \frac{\sum_{x_i \in C_k} \sum_{x_j \in C_l} d(x_i, x_j)}{n_k n_l}$$

siendo $n_k$ el n√∫mero de elementos del cl√∫ster $C_k$.

```{r eval = FALSE}
# Clustering (average)
average_clust <-
  hclust(d, method = "average")

# k = 3
fviz_dend(average_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (average)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (average)
average_clust <-
  hclust(d, method = "average")

# k = 3
fviz_dend(average_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (average)")
```

]

---

# Algoritmos jer√°rquicos: average

.pull-left[

```{r eval = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (single)
average_clust <-
  hclust(d, method = "average")
groups <- cutree(average_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (average)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (single)
average_clust <-
  hclust(d, method = "average")
groups <- cutree(average_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (average)") +
  theme_minimal()
```

]


---

# Algoritmos jer√°rquicos: centroid

.pull-left[

* **Distancia entre centroides** (centroid): la distancia entre dos cl√∫steres ser√° definida como la distancia entre los centroides de cada grupo.

$$d(C_k, C_l) = d(\overline{x}_k, \overline{x}_l)$$

donde $\overline{x}_k = \frac{\sum_{x_k \in C_k} x_{k}}{n_k}$


```{r eval = FALSE}
# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")

# k = 3
fviz_dend(centroid_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (centroid)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")

# k = 3
fviz_dend(centroid_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (centroid)")
```

]

---

# Algoritmos jer√°rquicos: centroid

.pull-left[

```{r eval = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")
groups <- cutree(centroid_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (centroid)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")
groups <- cutree(centroid_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (centroid)") +
  theme_minimal()
```

]


---

# Algoritmos jer√°rquicos: Ward

.pull-left[


* **M√©todo de Ward** (m√≠nima varianza): de todas las posibles aglomeraciones, selecciona aquella que **minimiza la variabilidad interna**


$$W = \sum_{k=1}^{G} W_k, \quad W_k = \sum_{\boldsymbol{X_i} \in C_k} \sum_{j=1}^{p} \left(x_{i,j} - \overline{x}_{j}^{k} \right)$$

donde $\overline{x}_{j}^{k}$ es la coordenada j-√©sima del centroide $k$.

```{r eval = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")

# k = 3
fviz_dend(ward_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (Ward)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")

# k = 3
fviz_dend(ward_clust, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (Ward)")
```

]

---

# Algoritmos jer√°rquicos: Ward

.pull-left[

```{r eval = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")
groups <- cutree(ward_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (Ward)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")

# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")
groups <- cutree(ward_clust, k = 3)

fviz_cluster(list(data = iris_scale,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (Ward)") +
  theme_minimal()
```

]

---

# Ejemplo de jer√°rquico divisivo

.pull-left[

El paquete `{cluster}` nos permite implementar el **algoritmo de clustering jer√°rquico divisivo** m√°s famoso, **DIANA (DIvisive ANAlysis Clustering)**. En cada iteraci√≥n se seleccionar√° el **cl√∫ster con mayor diametro** (la mayor de las diferencias entre dos de sus observaciones) y se identificar√° la observaci√≥n m√°s alejada (en promedio) del resto de observaciones, siendo esta observaci√≥n la que inicia el nuevo cl√∫ster. 


```{r eval = FALSE}
library(cluster)
hc_diana <- diana(x = d, diss = TRUE, stand = FALSE)

fviz_dend(hc_diana , k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (DIANA, divisivo)")
```

]

.pull-right[

```{r echo = FALSE}
# Estandarizamos
iris_scale <-
  iris %>% select(-Species) %>% 
  mutate(across(where(is.numeric),
                ~scale(.)))

# Matriz de distancias
d <-
  dist(iris_scale, method = "euclidean")


library(cluster)
hc_diana <- diana(x = d, diss = TRUE, stand = FALSE)

fviz_dend(hc_diana, k = 3,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB", "#E7B800"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (DIANA, divisivo)")
```


]

---


name: no-jerarquicos
class: center, middle

# Algoritmos no jer√°rquicos

---

# Algoritmo k-means

Dentro de los no jer√°rquicos nos centraremos en el **algoritmo k-means** (o algoritmo de Lloyd), basado en encontrar los $k$ mejores cl√∫sters, entiendo como mejor cl√∫ster aquel cuya **varianza interna sea lo m√°s peque√±a posible**

1. **Seleccionaremos $k$ puntos iniciales** (tantos c√≥mo n√∫mero de cl√∫sters) aleatorios de nuestro espacio (no tienen porque ser observaciones)  que definiran los **centroides** de nuestros cl√∫sters $\left\lbrace C_1, C_2, \ldots, C_k \right\rbrace$

2. Asignaremos **cada observaci√≥n a su cl√∫ster m√°s cercano**, entendiendo como cl√∫ster m√°s cercano aquel cuyo centroide tiene m√°s cerca (diagrama de Voronoi)

3. Una vez formado el cl√∫ster, **recalcularemos los $k$ centroides** (c√≥mo las coordenadas medias de las observaciones agrupadas en dichos cl√∫sters)

4. Se **repiten los pasos 2. y 3.** hasta que no haya ning√∫n cambio en los cl√∫ster, hasta que no hayan movido los centroides por encima de una tolerancia o hasta que se haya llegado a un n√∫mero m√°ximo de iteraciones

---

# Algoritmo k-means

En dicho algoritmo se deber√°n tomar dos decisiones:

* La m√©trica usada para medir la distancia entre los individuos (normalmente la Eucl√≠dea)

* **Selecci√≥n inicial de los $k$ centroides**:
  - Seleccionar las $k$ primeras observaciones
  - Seleccionar la primera observaci√≥n como primer centroide, y el centroide i-√©simo como aquella observaci√≥n cuya distancia a los $i-1$ centroides previos sea tan grande como una distancia predefinida.
  - Seleccionar $k$ observaciones aleatorias.
  - Seleccionar $k$ observaciones en base a conocimiento experto.

---

# Algoritmo k-means con R


Vamos a generar un **conjunto aleatorio bidimensional**.

```{r}
centers <- tibble(cluster = factor(1:3), num_points =  c(100, 150, 50),
                  x = c(5, 0, -3), y = c(-1, 1, -2))
centers

set.seed(1234567)
labelled_points <- centers %>% mutate(x = map2(num_points, x, rnorm), y = map2(num_points, y, rnorm)) %>% 
  select(-num_points) %>%  unnest(cols = c(x, y))
labelled_points
```

---

# Algoritmo k-means con R

.pull-left[

```{r eval = FALSE}
ggplot(labelled_points,
       aes(x = x, y = y, color = cluster)) +
  geom_point(alpha = 0.5, size = 5) +
  labs(x = "Coordenada X",
       y = "Coordenada Y",
       title = "Datos simulados")
```
  
]

.pull-right[

```{r echo = FALSE}
ggplot(labelled_points,
       aes(x = x, y = y, color = cluster)) +
  geom_point(alpha = 0.5, size = 5) +
  labs(x = "Coordenada X", y = "Coordenada Y",
       title = "Conjunto de datos a clusterizar") + 
  theme_minimal()
```
  
]

---

# Algoritmo k-means con R

Para realizar el clustering basta usar la funci√≥n `kmeans()`, indic√°ndole el **n√∫mero de cl√∫sters** y el **n√∫mero m√°ximo de iteraciones**

```{r}
kclust <- kmeans(labelled_points %>% select(-cluster),
                 centers = 3, iter.max = 50)
kclust
```

---

# Algoritmo k-means con R

.pull-left[

Los grupos est√°n guardados en `kclust$cluster` y los centroides en `kclust$centers`

```{r}
kclust$centers
```

```{r eval = FALSE}
fviz_cluster(list(data =
                    labelled_points %>%
                    select(-cluster),
                  cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
fviz_cluster(list(data = labelled_points %>% select(-cluster), cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]

---

# Algoritmo k-means con R

.pull-left[

```{r echo = FALSE}
ggplot(labelled_points,
       aes(x = x, y = y, color = cluster)) +
  geom_point(alpha = 0.5, size = 5) +
  labs(x = "Coordenada X", y = "Coordenada Y",
       title = "Cl√∫sters originales") + 
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
ggplot(labelled_points %>%
         mutate(cluster_predict = as_factor(kclust$cluster)),
       aes(x = x, y = y, color = cluster_predict)) +
  geom_point(alpha = 0.5, size = 5) +
  labs(x = "Coordenada X", y = "Coordenada Y",
       title = "Cl√∫sters tras k-means") + 
  theme_minimal()
```

]

---

# Algoritmo k-means con R

Podemos tambi√©n implementar el k-means para nuestro conjunto conocido `iris`


```{r}
kclust <- kmeans(iris %>% select(-Species),
                 centers = 3, iter.max = 50)
kclust$centers
kclust
```

---

# Algoritmo k-means con R

.pull-left[

```{r eval = FALSE}
fviz_cluster(list(data =
                    iris %>%
                    select(-Species),
                  cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

Recordatorio: de la 1 a la 50 son setosa, de la 51 a la 100 son versicolor, y de la 101 a la 151 virginica. 

]

.pull-right[

```{r echo = FALSE}
fviz_cluster(list(data = iris %>% select(-Species), cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB", "#E7B800"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]


---

name: n-grupos
class: center, middle

# Determinaci√≥n del n√∫mero de grupos

---

# Determinaci√≥n del n√∫mero de grupos

Hasta ahora **hemos hecho ¬´trampas¬ª** ya que el **n√∫mero de cl√∫sters se lo hemos indicado a priori**, en un caso porque hemos simulado nosotros los datos, y en otro caso porque es un conjunto del que conocemos su etiqueta (aprendizaje supervisado).

Para decidir el **n√∫mero de cl√∫ster √≥ptimo** (recuerda que estamos en aprendizaje NO supervisado, no sabemos a priori la etiqueta de los datos), tenemos **varias m√©tricas** a nuestra disposici√≥n

* **Variabilidad total**

$$T = \sum_{j=1}^{p} \sum_{i=1}^{n} \left(x_{ij} - \overline{x}_j \right)^2$$

donde $\overline{x}_j  = \frac{\sum_{i=1}^{n} x_{ij}}{n}$

```{r}
kclust$totss
```

---

# Determinaci√≥n del n√∫mero de grupos


* **Variabilidad del cl√∫ster $C_k$**

$$W_k = \sum_{j=1}^{p} \sum_{\boldsymbol{X}_i \in C_k} \left(x_{ij} - \overline{x}_{j}^{k} \right)^2$$

donde $\overline{x}_{j}^{k}$ es la coordenada j-√©sima del centroide $k$.

```{r}
kclust$withinss
```

* **Variabilidad total intra-cl√∫steres**

$$W = \sum_{k=1}^{K} W_k$$

```{r}
kclust$tot.withinss
```

---

# Determinaci√≥n del n√∫mero de grupos


Para decidir el **n√∫mero de cl√∫ster √≥ptimo** (recuerda que estamos en aprendizaje NO supervisado, no sabemos a priori la etiqueta de los datos), tenemos **varias m√©tricas** a nuestra disposici√≥n


* **Variabilidad total entre-cl√∫steres**

$$E = \sum_{k=1}^{K} \sum_{j=1}^{p} \left( \overline{x}_{j}^{k} - \overline{x}_j  \right)^2$$

tal que $T = W + E$

```{r}
kclust$betweenss
```

```{r}
kclust$betweenss + kclust$tot.withinss
```

```{r}
kclust$totss
```

---

# Determinaci√≥n del n√∫mero de grupos

.pull-left[

Para decidir el **n√∫mero √≥ptimo** se usan principalmente dos m√©todos:

* Basado en la **variabilidad total intra-cl√∫steres (W)**:


```{r eval = FALSE}
fviz_nbclust(iris_scale, kmeans,
             method = "wss") +
  geom_vline(xintercept = 3,
             linetype = 2) +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad total intra-cl√∫steres")
```

Con el **¬´m√©todo del codo¬ª (Elbow method)** cuando $W$ ya no se reduce de forma significativa al aumentar el n√∫mero de cl√∫sters.

]

.pull-right[

```{r echo = FALSE}
fviz_nbclust(iris_scale, kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad total intra-cl√∫steres")
```


]

---

# Determinaci√≥n del n√∫mero de grupos

.pull-left[

As√≠ ser√≠a para el conjunto de datos simulados

```{r echo = FALSE}
fviz_nbclust(labelled_points %>%
               select(-cluster),
             kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad total intra-cl√∫steres")
```

]

.pull-right[

```{r eval = FALSE}
fviz_nbclust(labelled_points %>% select(-cluster),
             kmeans, method = "wss") +
  geom_vline(xintercept = 3, linetype = 2) +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad total intra-cl√∫steres")
```

Con el **¬´m√©todo del codo¬ª (Elbow method)** cuando $W$ ya no se reduce de forma significativa al aumentar el n√∫mero de cl√∫sters.

]

---

# Determinaci√≥n del n√∫mero de grupos

.pull-left[

Para decidir el **n√∫mero √≥ptimo** se usan principalmente dos m√©todos:

* Basado en la **compacidad** (√≠ndice silhouette)

$$S= \frac{\sum_{i=1}^{n} S(i)}{n}, \quad S(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$

donde $a(i)$ representa las **distancia media de la observaci√≥n i-√©sima** a las observaciones de su cl√∫ster, y y $b(i)$ respecto a la de otros cl√∫sters.

```{r eval = FALSE}
fviz_nbclust(iris_scale, kmeans,
             method = "silhouette") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Silhouette media",
       title = "N√∫mero √≥ptimo basado en silhouette")
```

]

.pull-right[

```{r echo = FALSE}
fviz_nbclust(iris_scale, kmeans,
             method = "silhouette") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en silhouette")
```

]

---

# Determinaci√≥n del n√∫mero de grupos

La funci√≥n `silhouette()` del paquete `{cluster}` nos permite calcular dicho √≠ndice para la agrupaci√≥n realizada (m√°s adelante veremos como visualizarlo)

```{r}
kclust <- kmeans(iris_scale,
                 centers = 3, iter.max = 50)
d <- dist(iris_scale, method = "euclidean")
sil <- silhouette(kclust$cluster, d)
sil
```

---

# Poniendo en pr√°ctica: clustering de pa√≠ses

Para poner en pr√°ctica lo aprendido vamos a analizar los **datos de esperanza de vida de distintos pa√≠ses**, en hombres y mujeres, a distintas edades, que tenemos en el archivo `EsperanzaVida.xlsx`

```{r}
library(readxl)
esperanza <-
  read_xlsx(path = "./DATOS/EsperanzaVida.xlsx")
glimpse(esperanza)
```

---

# Clustering de pa√≠ses

### Visualizaci√≥n previa


.pull-left[

La funci√≥n `heatmaply()` del paquete hom√≥nimo qos permite realizar agrupaciones tanto de variables (dendograma superior) como de individuos (dendograma en el lateral).

```{r eval = FALSE}
# Convertimos a data.frame para
# tener row.names y eliminamos PAIS
esperanza_df <-
  as.data.frame(esperanza) %>%
  select(-PAIS)
row.names(esperanza_df) <- esperanza %>% pull(PAIS)

library(heatmaply)
heatmaply(esperanza_df,
          seriate = "mean",
          row_dend_left = TRUE,
          plot_method = "plotly")
```

]

.pull-right[

```{r echo = FALSE}
esperanza_df <-
  as.data.frame(esperanza) %>%
  select(-PAIS)
row.names(esperanza_df) <- esperanza %>% pull(PAIS)
```

```{r echo = FALSE, out.width = "95%", fig.align = "right"}
knitr::include_graphics("./img/heatmap_paises_1.jpg")
``` 

]


---

# Clustering de pa√≠ses

### Visualizaci√≥n previa

.pull-left[

Dicha representaci√≥n puede ser mejorada estandarizando los datos

```{r eval = FALSE}
# Estandarizamos
esperanza_scale_df <-
  esperanza_df %>%
  mutate(across(where(is.numeric),
                ~scale(.)))

heatmaply(esperanza_scale_df,
          seriate = "mean",
          row_dend_left = TRUE,
          plot_method = "plotly")
```

]

.pull-right[

```{r echo = FALSE}
esperanza_scale_df <-
  esperanza_df %>%
  mutate(across(where(is.numeric),
                ~scale(.)))
```

```{r echo = FALSE, out.width = "95%", fig.align = "right"}
knitr::include_graphics("./img/heatmap_paises_2.jpg")
``` 

]

---

# Clustering de pa√≠ses

### Matriz de distancias

.pull-left[

Con los datos estandarizados podemos visualizar su matriz de distancias

```{r eval = FALSE}
# matriz de distancias
d <- dist(esperanza_scale_df,
          method = "euclidean")

# Visualizamos
fviz_dist(d, show_labels = TRUE)
```

Podemos ya atisbar cuatro grupos de pa√≠ses: Madasgar-Camer√∫n, Nicaragua-Algeria-Tunez, Groenlandia-...-Guatemala, y los dem√°s.

]
.pull-right[

```{r echo = FALSE, out.width = "95%"}
# matriz de distancias
d <- dist(esperanza_scale_df, method = "euclidean")

# Visualizamos
fviz_dist(d, show_labels = TRUE)
```

]

---

# Clustering de pa√≠ses

### Clustering (single)

.pull-left[

```{r eval = FALSE}
# Clustering (single)
single_clust <-
  hclust(d, method = "single")

# Dendograma
library(factoextra)
fviz_dend(single_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          # a√±ade rect√°ngulo
          rect = TRUE) +
  labs(title = "Dendograma (single)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (single)
single_clust <-
  hclust(d, method = "single")

# Dendograma
library(factoextra)
fviz_dend(single_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a clusters
          color_labels_by_k = TRUE, 
          # A√±ade rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (single)")
```

]

---

# Clustering de pa√≠ses

### Clustering (complete)

.pull-left[

```{r eval = FALSE}
# Clustering (complete)
complete_clust <-
  hclust(d, method = "complete")

# Dendograma
fviz_dend(complete_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          # A√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (complete)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (complete)
complete_clust <-
  hclust(d, method = "complete")

# Dendograma
fviz_dend(complete_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          # A√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (complete)")
```

]

---

# Clustering de pa√≠ses

### Clustering (average)

.pull-left[

```{r eval = FALSE}
# Clustering (average)
average_clust <-
  hclust(d, method = "average")

# Dendograma
fviz_dend(average_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (average)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (average)
average_clust <-
  hclust(d, method = "average")

# Dendograma
fviz_dend(average_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (average)")
```

]

---


# Clustering de pa√≠ses

### Clustering (centroid)

.pull-left[

```{r eval = FALSE}
# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")

# Dendograma
fviz_dend(centroid_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (centroid)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (centroid)
centroid_clust <-
  hclust(d, method = "centroid")

# Dendograma
fviz_dend(centroid_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (centroid)")
```

]

---

# Clustering de pa√≠ses

### Clustering (Ward)

.pull-left[

```{r eval = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")

# Dendograma
fviz_dend(ward_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (Ward)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")

# Dendograma
fviz_dend(ward_clust, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (Ward)")
```

]

---


# Clustering de pa√≠ses

### Clustering divisivo

.pull-left[

```{r eval = FALSE}
# Clustering divisivo
hc_diana <-
  diana(x = d, diss = TRUE,
        stand = FALSE)

# Dendograma
fviz_dend(hc_diana, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (divisivo)")
```

]

.pull-right[

```{r echo = FALSE}
# Clustering divisivo
hc_diana <-
  diana(x = d, diss = TRUE,
        stand = FALSE)

# Dendograma
fviz_dend(hc_diana, k = 4,
          cex = 0.5, 
          k_colors =
            c("#2E9FDF", "#00AFBB",
              "#E7B800", "#FC4E07"),
          # Diferentes colores a los clusters
          color_labels_by_k = TRUE, 
          #a√±ade un rect√°ngulo alrededor
          rect = TRUE) +
  labs(title = "Dendograma (divisivo)")
```

]

---

# Clustering de pa√≠ses

### Clustering no jer√°rquico

.pull-left[

```{r eval = FALSE}
# Clustering k-means
kclust <- kmeans(esperanza_scale_df,
                 centers = 4,
                 iter.max = 50)
kclust$totss

# Clustering
fviz_cluster(list(data =
                    esperanza_scale_df,
                  cluster =
                    kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
# Clustering k-means
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)

# Clustering
fviz_cluster(list(data = esperanza_scale_df,
                  cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]

---


# Clustering de pa√≠ses

### Comparativa de clusterings


.pull-left[

```{r echo = FALSE}
groups <- cutree(single_clust, k = 4)

fviz_cluster(list(data = esperanza_scale_df,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (single)") +
  theme_minimal()
```

]

.pull-right[
```{r echo = FALSE}
groups <- cutree(complete_clust, k = 4)

fviz_cluster(list(data = esperanza_scale_df,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (complete)") +
  theme_minimal()
```

]

---

# Clustering de pa√≠ses

### Comparativa de clusterings

.pull-left[

```{r echo = FALSE}
groups <- cutree(average_clust, k = 4)

fviz_cluster(list(data = esperanza_scale_df,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (average)") +
  theme_minimal()
```

]

.pull-right[

```{r echo = FALSE}
groups <- cutree(centroid_clust, k = 4)

fviz_cluster(list(data = esperanza_scale_df,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (centroid)") +
  theme_minimal()
```

]

---

# Clustering de pa√≠ses

### Comparativa de clusterings

.pull-left[

```{r echo = FALSE}
groups <- cutree(ward_clust, k = 4)

fviz_cluster(list(data = esperanza_scale_df,
                  cluster = groups),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (Ward)") +
  theme_minimal()
```

]

.pull-right[


```{r echo = FALSE}
# Clustering k-means
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)

# Clustering
fviz_cluster(list(data = esperanza_scale_df,
                  cluster = kclust$cluster),
             palette =
               c("#2E9FDF", "#00AFBB",
                 "#E7B800", "#FC4E07"),
             ellipse.type = "convex", 
             repel = TRUE,
             show.clust.cent = FALSE) +
  labs(title = "Cluster (k-means)") +
  theme_minimal()
```

]

---

# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

.pull-left[

```{r eval = FALSE}
fviz_nbclust(esperanza_scale_df, kmeans,
             method = "wss") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad")
```

]

.pull-right[

```{r echo = FALSE}
fviz_nbclust(esperanza_scale_df, kmeans,
             method = "wss") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en variabilidad")
```

]

---

# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

.pull-left[

```{r eval = FALSE}
fviz_nbclust(esperanza_scale_df, kmeans,
             method = "silhouette") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en compacidad")
```

]

.pull-right[

```{r echo = FALSE}
fviz_nbclust(esperanza_scale_df, kmeans,
             method = "silhouette") +
  theme_minimal() +
  labs(x = "n¬∫ cl√∫steres (k)",
       y = "Variabilidad total intra-cl√∫steres (W)",
       title = "N√∫mero √≥ptimo basado en compacidad")
```

]

---

# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

.pull-left[


```{r eval = FALSE}
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)
sil <- silhouette(kclust$cluster, d)
sil
```

]

.pull-right[

```{r echo = FALSE}
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)
sil <- silhouette(kclust$cluster, d)
sil
```

]

---


# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

.pull-left[


```{r eval = FALSE}
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)
sil <- silhouette(kclust$cluster, d)
row.names(sil) <- row.names(esperanza_scale_df)

# Visualizaci√≥n
fviz_silhouette(sil, label = TRUE) +
  scale_fill_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  scale_color_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  theme_minimal() +
  labs(title =
         "√çndice silhouette para k-means con k = 4") +
  # Giramos etiquetas eje
  theme(axis.text.x =
          element_text(angle = 90,
                       vjust = 0.5,
                       hjust=1))
```

]

.pull-right[


```{r echo = FALSE}
kclust <- kmeans(esperanza_scale_df,
                 centers = 4, iter.max = 50)
sil <- silhouette(kclust$cluster, d)
row.names(sil) <- row.names(esperanza_scale_df)

# Visualizaci√≥n
fviz_silhouette(sil, label = TRUE,
                print.summary = FALSE) +
  scale_fill_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  scale_color_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  theme_minimal() +
  labs(title =
         "√çndice silhouette para k-means con k = 4") +
  # Giramos etiquetas eje
  theme(axis.text.x =
          element_text(angle = 90,
                       vjust = 0.5,
                       hjust=1))
```

]

---

# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

.pull-left[


```{r eval = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")
groups <- cutree(ward_clust, k = 4)
sil <- silhouette(groups, d)
row.names(sil) <- row.names(esperanza_scale_df)

# Visualizaci√≥n
fviz_silhouette(sil, label = TRUE,
                print.summary = FALSE) +
  scale_fill_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  scale_color_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  theme_minimal() +
  labs(title =
         "√çndice silhouette para jer√°rquico Ward con k = 4") +
  # Giramos etiquetas eje
  theme(axis.text.x =
          element_text(angle = 90,
                       vjust = 0.5,
                       hjust = 1))
```

]

.pull-right[


```{r echo = FALSE}
# Clustering (ward)
ward_clust <-
  hclust(d, method = "ward.D2")
groups <- cutree(ward_clust, k = 4)
sil <- silhouette(groups, d)
row.names(sil) <- row.names(esperanza_scale_df)

# Visualizaci√≥n
fviz_silhouette(sil, label = TRUE,
                print.summary = FALSE) +
  scale_fill_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  scale_color_manual(values =
                      c("#2E9FDF", "#00AFBB",
                        "#E7B800", "#FC4E07")) +
  theme_minimal() +
  labs(title =
         "√çndice silhouette para jer√°rquico Ward con k = 4") +
  # Giramos etiquetas eje
  theme(axis.text.x =
          element_text(angle = 90,
                       vjust = 0.5,
                       hjust = 1))
```

]

---

# Clustering de pa√≠ses

### Elecci√≥n del n√∫mero de clusters

Podemos ejecutar varios clustering a la vez mapeando sobre el n√∫mero de clusters (por ejemplo, usando el k-medias)

```{r}
library(tidymodels)
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(kclust =
           map(k, ~kmeans(esperanza_scale_df, .x)),
         tidied = map(kclust, tidy),
         glanced = map(kclust, glance),
         augmented = map(kclust, augment,
                         esperanza_scale_df)
         )
clusters <- 
  kclusts %>% unnest(cols = c(tidied))

assignments <- 
  kclusts %>% unnest(cols = c(augmented))

clusterings <- 
  kclusts %>% unnest(cols = c(glanced))
```

---

# Recursos y bibliograf√≠a

&nbsp;

### **Leyenda de los recursos**

&nbsp;

&nbsp;


#### üìö **Art√≠culos o libros** cient√≠ficos que han sido sometidos a revisi√≥n por pares.

&nbsp;

#### üîó **Recursos online** recomendados

&nbsp;

#### üíª Recursos para la **programaci√≥n en R**

---

# Bibliograf√≠a general

üíª **Tidy Data Tutor**: para visualizar la mec√°nica interna de `{tidyverse}`. <https://tidydatatutor.com/>

üîó Web con recursos para la **introducci√≥n a la estad√≠stica y Machine Learning en R** <https://artofstat.com/>

üìö **¬´An Introduction to Multivariate Statistical Analysis¬ª**. Anderson (1958) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/introduction_mva_anderson_2003.pdf>

üìö **¬´A New Measure of Rank Correlation¬ª**. Kendall (1938) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/correlation_kendall_1938.pdf>

üìö **¬´The generalised product moment distribution in samples from a normal multivariate population¬ª**. Wishart (1928) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/multivariate_normal_wishart_1928.pdf>

üìö **¬´On lines and planes of closest fit to systems of points in space¬ª**. Pearson (1901) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/fit_pearson_1901.pdf>


---

# Recursos dataviz

### Dataviz

üìö **¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª**. Sevilla (2005) <http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf>

üìö **¬´Quantitative Graphics in Statistics: A Brief History¬ª**. Beniger and Robyn <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/graphics_beniger_robin_1978.pdf>
 
 
üíª **¬´Analizando datos, visualizando informaci√≥n, contando historias¬ª** (curso de dataviz en R). √Ålvarez-Li√©bana y Valverde-Castilla (2022) <https://dadosdelaplace.github.io/curso-dataviz-ECI-2022>

---

# Bibliograf√≠a componentes principales

üíª **Componentes principales** en `{tidymodels}`. <https://www.tmwr.org/dimensionality.html#beans>


üìö **¬´Principal Component Analysis¬ª**. Jolliffe (2002) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/pca_jolliffe_2002.pdf>

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

üìö **¬´Principal Component Analysis: a review and recent developments¬ª**. Jolliffe and Cadima (2016) <https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202>

üîó **¬´The Mathematics Behind Principal Component Analysis¬ª**. Dubey (2018).  <https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643>


üîó **¬´A One-Stop Shop for Principal Component Analysis¬ª**. Brems (2017). <https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>

üìö **¬´On the number of principal components: a test of dimensionality based on measurements of similarity between matrices¬ª**. Dray (2008) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/numer_pca_dray_2008.pdf>


---

# Bibliograf√≠a an√°lisis cl√∫ster

üìö **¬´Multiclass classification of dry beans using computer vision and machine learning techniques¬ª**. Koklu y Ozkan (2020) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/multiclass_classif_koklu_ozlan_2020.pdf>

üíª **¬´Clustering y heatmaps: aprendizaje no supervisado¬ª**. Amat (2017). <https://rpubs.com/Joaquin_AR/310338>

üíª **¬´K-means clustering with tidy data principles¬ª** <https://www.tidymodels.org/learn/statistics/k-means/>

üîó **¬´ISLR tidymodels Labs¬ª** <https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/unsupervised-learning.html#kmeans-clustering>


üìö **¬´Algorithm AS 136: a K-Means Clustering Algorithm¬ª**. Hartigan y Wong (1979) <http://www.jstor.org/stable/2346830>

üîó **¬´Machine Learning for Social Scientists¬ª**. Cimentada (2020) <https://cimentadaj.github.io/ml_socsci/unsupervised-methods.html>

---

# Recursos y bibliograf√≠a

### Otras t√©cnicas de reducci√≥n de la dimensi√≥n

üîó Sobre **PCA y PLS**. Amat (2017). <https://www.cienciadedatos.net/documentos/35_principal_component_analysis#Introducci%C3%B3n>

üìö **¬´On the early history of the singular value decomposition¬ª**. Stewart (1993) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/svd_stewart_1993.pdf>

üìö **¬´UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction¬ª**. McInnes, healy and Melville (2020) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/umap_mcinnesetal_2020.pdf>

