---
title: "T√âCNICAS DE AN√ÅLISIS MULTIVARIANTE"
subtitle: "M√°ster propio (NTIC) en ¬´Big Data y Business Analytics¬ª"
author:
  - "Profesor: Javier √Ålvarez Li√©bana"
institute: "Facultad de Estudios Estad√≠sticos (UCM)"
date: "22/04/2022 - 23/04/2022 (actualizado: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    self_contained: false
    lib_dir: libs
    css: [default, style.css]
    nature:
      # beforeInit: "stylejs.js"
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---


```{r settings, include = FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, out.width = "100%", cache = FALSE,
                      comment = ">", echo = TRUE, message = FALSE,
                      warning = FALSE, hiline = TRUE, dpi = 120)

# xaringan Extra
# devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
use_xaringan_extra(c("tile_view", "animate_css", "tachyons"))
use_tile_view() # panel
# xaringanExtra::use_scribble() # scribble
use_extra_styles(hover_code_line = TRUE,
                 mute_unhighlighted_code = FALSE) # Hover triangle code line
use_clipboard( # About clipboard
  button_text = "Click para copiar c√≥digo",
  success_text = "C√≥digo copiado",
  error_text = "Ctrl+C para copiar"
)
use_freezeframe() # restarting gifs
use_animate_all("fade") # animates
use_panelset() # panels 
```

class: inverse center middle

# ATAJOS DE LAS DIAPOSITIVAS



$$\\[2.7in]$$

.left[Pulsa <kbd-black>O</kbd-black> para ver el panel de diapositivas]
.left[Pulsa <kbd-black>H</kbd-black> para ver otros atajos]

---


# Material de las clases


.pull-left[

- **Diapositivas** del curso:
<https://dadosdelaplace.github.io/teaching/pca-clustering>

- **Scripts** del curso:
<https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/scripts>

- **Evaluaciones**:
<https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/eval>

- **Bibliograf√≠a**: <https://github.com/dadosdelaplace/teaching/tree/main/bdba-pca-clustering-2022/biblio>

- **Manual** de R: <https://dadosdelaplace.github.io/courses-intro-R/>

- **Curso de dataviz** en R: <https://dadosdelaplace.github.io/curso-dataviz-ECI-2022>


]

.pull-right[

```{r material, echo = FALSE,  out.width = "83%", fig.align = "right"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

---

# Me presento: la turra

.pull-left[

```{r echo = FALSE,  out.width = "80%", fig.align = "left"}
knitr::include_graphics("./img/me.jpeg")
``` 

]

.pull-right[

* **Javier √Ålvarez Li√©bana**, nacido en 1989 en Carabanchel Bajo (Madrid)

* Licenciado (UCM) en **Matem√°ticas** (Erasmus en Bologna mediante). **M√°ster (UCM) en Ingenier√≠a Matem√°tica** (2013-2014)

&nbsp;

* **Doctorado en estad√≠stica** por la Universidad de Granada


* Encargado de la **visualizaci√≥n y an√°lisis de datos covid** de la Consejer√≠a de Salud del **Principado de Asturias**

]

&nbsp;

Intentando la **divulgaci√≥n** por `Twitter (@dadosdelaplace)` e `Instagram (@javieralvarezliebana)`. Tengo una newsletter: <https://cartasdelaplace.substack.com/>

---

# Objetivos


.pull-left[

El prop√≥sito de estas clases ser√° el tratamiento de **datos multidimensionales**, con tres objetivos principales:

- **Reducci√≥n de la dimensi√≥n**: ¬øtodas las variables aportan informaci√≥n? ¬øTodas son necesarias? ¬øPodemos transformar las variables para mantener la informaci√≥n de los datos pero reducir la dimensionalidad de los mismos?

- **Visualizaci√≥n**: ¬øcu√°ntas dimensiones podemos incluir en un gr√°fico 2D? ¬øC√≥mo visualizar datos multidimensionales?

- **Encontrar patrones**: ¬øc√≥mo agrupar (clusterizar) los elementos en funci√≥n de sus diferencias y similitudes?

]

.pull-right[

```{r material2, echo = FALSE,  out.width = "120%", fig.align = "left"}
knitr::include_graphics("./img/portada_master.jpg")
``` 

]

&nbsp;

üìö Estas **diapositivas** han sido elaboradas con el propio `R` haciendo uso del paquete `{xaringan}`
y `{xaringanExtra}`.


---


# Requisitos

Para el presente curso los √∫nicos **requisitos** ser√°n:

1. **Conexi√≥n a internet** (para la descarga de algunos datos y paquetes).

2. **Instalar R**: ser√° nuestro **lenguaje**, nuestro **castellano** para poder ¬´comunicarnos con el ordenador. La descarga la haremos (gratuitamente) desde <https://cran.r-project.org/>

3. **Instalar R Studio**. De la misma manera que podemos escribir castellano en un ordenador, en un Word, en un papel o en un tuit, podemos usar distintos IDE (entornos de desarrollo integrados, nuestro Office), para que el trabajo sea m√°s c√≥modo. Nuestro **Word** para nosotros ser√° **RStudio**.

.left[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/cran-R.jpg" alt = "cran-R" align = "left" width = "500" style = "margin-top: 5vh">
]

.right[
  <img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/R-studio.jpg" alt = "RStudio" align = "right" width = "500" style = "margin-top: 5vh;">
]


---

class: inverse center middle

# POR SI ACASO...¬øPOR QU√â R Y NO EXCEL?


---

# R vs excel

![](./img/meme_barco.jpg)

---

# Incel vs excel

```{r echo = FALSE, out.width = '85%', fig.align = "center"}
knitr::include_graphics("./img/incel.jpg")
```

---

# Datos: de la celda a la tabla

<img src = "https://raw.githubusercontent.com/dadosdelaplace/slides-ECI-2022/main/img/celdas.jpg" alt = "celdas" align = "center" width = "850" style = "margin-top: 1vh;">


* **Celda**: un **dato individual** de un tipo concreto.
* **Variable**: **concatenaci√≥n de datos** del mismo tipo.
* **Matriz**: **concatenaci√≥n de variables** del **mismo tipo** y longitud.
* **Tabla**: **concatenaci√≥n de variables** de **distinto tipo** pero igual longitud.

---

class: inverse center middle

# BLOQUE I. Selecci√≥n de variables: PCA

&nbsp;


### [¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?](#intro-PCA)

### [Teor√≠a: an√°lisis de componentes principales](#teoria-PCA)

### [Pr√°ctica: PCA en R (visualizaci√≥n)](#practica-PCA)

---

name: intro-PCA
class: center, middle

# ¬øPor qu√© es un paso importante en el an√°lisis de datos multidimensional?

### **¬øQu√© es el an√°lisis multivariante?**

---

# Breve historia de la estad√≠stica

.pull-left[

## Origen

* Del (neo)lat√≠n ¬´statisticum collegium¬ª: consejo de **Estado**.
* Del alem√°n ¬´statistik¬ª (ciencia del **Estado**, intoducido por G. Achenwall).


## Primeros usos: elaboraci√≥n de censos

Los **primeros usos** documentados de la estad√≠stica fueron la elaboraci√≥n de **censos** por parte de **mesopot√°micos, chinos y egipcios**, con tres fines:

* Cobrar **impuestos** (un saludo, Willyrex).
* Reparto de **tierras** y optimizaci√≥n de su uso.
* **Reclutamiento de soldados**.

]

.pull-right[

## Estad√≠stica en la guerra

Seg√∫n Tuc√≠dides, conceptos estad√≠sticos como la **moda** datan del **siglo V a.C.**: para asaltar la muralla de la ciudad de Platea, pon√≠an a contar a varios soldados el n√∫mero de ladrillos vistos en la muralla, qued√°ndose con el **conteo m√°s repetido (la moda, el m√°s frecuente)**, permitiendo el c√°lculo de la altura de la muralla.

```{r echo = FALSE, out.width = "70%", fig.align = "center"}
knitr::include_graphics("./img/peloponeso.jpg")
```

]

---

# ¬øQu√© han hecho los romanos por nosotros?

.pull-left[

Precisamente por el tama√±o de su Imperio, fueron los **romanos** quienes hicieron un uso m√°s intenso de la estad√≠stica:

* **Censos** (elaborados por la censura, que elaboraba no solo el censo sino la supervisi√≥n de la moralidad p√∫blica).
* Primeras **tablas de natalidad/mortalidad**
* Primeros **catastros** (registros oficiales de propiedades, primeros impuestos)

```{r echo = FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("./img/catastro.jpg")
```

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "left"}
knitr::include_graphics("https://www.publico.es/tremending/wp-content/uploads/2019/02/lifeofbrian3.jpg")
```

]

---

# Breve historia de la estad√≠stica

.pull-left[

## **√ÅRABES**

Autores de los **primeros tratados de estad√≠stica**, como el manuscrito de **Al-Kindi (801-873)**, que us√≥ la distribuci√≥n de **frecuencias de palabras** para el desarrollo de m√©todos de cifrado y descifrado de **mensajes encriptados**.

]

.pull-right[

## **M√âXICO**

Ya en el **a√±o 1116, el rey X√≥lotl** implement√≥ un **censo** que consist√≠a en la **estimaci√≥n de piedras**, tirando cada s√∫bdito una a un mont√≥n (Nepohualco).
]


&nbsp;

.pull-left[

## **INGLATERRA**

Desde el siglo XII se realiza la **Prueba del Pyx**, considerado uno de los **primeros controles de calidad**: se extre una de las monedas acu√±adas y se deposita en una caja, para un a√±o despu√©s comprobar su calidad y pureza.

]

.pull-right[

## **ITALIA**

En paralelo al **auge de los primeros ¬´sistemas financieros¬ª en Italia**, ¬´La Nuova Cr√≥nica¬ª de G. Villani fue considerado durante mucho tiempo el primer tratado de estad√≠stica (hasta el descubrimiento de los trabajos de Al-Kindi).

]

---

# Navegaci√≥n y astronom√≠a

Y es de aquella √©poca medieval, en la que la navegaci√≥n y la astronom√≠a empezaban a tomar relevancia cient√≠fica, cuando aparece la que se considera la primera gr√°fica (aunque no propiamente estad√≠stica) <sup>1</sup>, representando el **movimiento c√≠clico de los planetas** (entre los siglos X y XI)

```{r echo = FALSE,  out.width = "60%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Beniger y Robyn (1978)"}
knitr::include_graphics("./img/dataviz_historico_1.png")
``` 

[1] [üìö ¬´Quantitative Graphics in Statistics: A Brief History¬ª de James R. Beniger y Dorothy L. Robyn. The American Statistician (1978)](https://www.jstor.org/stable/2683467)

 
---

# Navegaci√≥n y astronom√≠a

Con una motivaci√≥n similar, en torno a 1360 el matem√°tico **Nicole Oresme** dise√±√≥ el **primer gr√°fico de barras**<sup>1</sup> (no estad√≠stico), con la idea de **visualizar a la vez dos magnitudes f√≠sicas te√≥ricas** (pero...a√∫n sin representar datos).


```{r echo = FALSE,  out.width = "30%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Friendly y Valero-Mora (2010), de ¬´Tractatus De Latitudinibus Formarum¬ª"}
knitr::include_graphics("./img/dataviz_historico_2.jpeg")
``` 

[1] [üìö ¬´The First (Known) Statistical Graph: Michael Florent van Langren and the 'Secret' of Longitude¬ª de M. Friendly y P. M. Valero-Mora. The American Statistician (2010)](https://www.researchgate.net/publication/227369016_The_First_Known_Statistical_Graph_Michael_Florent_van_Langren_and_the_Secret_of_Longitude)

 
---

# Primer gr√°fico estad√≠stico

La mayor√≠a de expertos, como Tufte <sup>1,2</sup>, consideran **este gr√°fico** casi longitudinal como la **primera visualizaci√≥n de datos** de la historia, hecha por **van Langren** en 1644, representando la **distancia (en longitud) entre Toledo y Roma** (un poco mal medida ya que la distancia real es de 16.5¬∫).

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Gr√°fica original extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/longitud_dataviz.jpg")
``` 

```{r echo = FALSE,  out.width = "45%", fig.align = "center", fig.cap = "Adaptaci√≥n extra√≠da de Friendly y Valero-Mora (2010)"}
knitr::include_graphics("./img/dataviz_historico_3.jpeg")
``` 

[1] [üìö ¬´Visual explanations: images and quantities, evidence and narrative¬ª de E. Tufte](https://archive.org/details/visualexplanatio00tuft)

[2] [üìö ¬´PowerPoint is evil¬ª de E. Tufte](https://www.wired.com/2003/09/ppt2/)

---


# Navegaci√≥n y astronom√≠a

.pull-left[

### T. Brahe

Uno de los primeros usos ¬´modernos¬ª de la estad√≠stica fue en la **navegaci√≥n y la astronom√≠a**, siendo Tycho Brahe de los primeros en utilizar la estad√≠stica para **reducir los errores** observacionales.
]

.pull-right[

### E. Wright

Fue el primero en usar en 1599 lo que hoy llamamos **mediana** en su libro ¬´Certaine errors in navigation¬ª, aplicada a la navegaci√≥n.

]

.pull-left[

### G. Galileo

Aunque la fama se la llev√≥ **Gauss**, fue el primero en plantear una idea similar a la que hoy llamamos **m√©todo de m√≠nimos cuadrados**: los valores m√°s probables ser√≠an aquellos que minimizaran los errores.

]

.pull-right[
### C. F. Gauss y A. M. Legendre

El **m√©todo de los m√≠nimos cuadrados**, en el que basan modelos actuales como la regresi√≥n, fue desarrollado por **Legendre y Gauss** (el √∫ltimo lo aplic√≥ a la detecci√≥n m√°s probable del planeta enano Ceres).

]

---

# Demograf√≠a, epidemiolog√≠a y fisiolog√≠a

.pull-left[

### J. Graunt

Autor de ¬´Natural and Political Observations Made upon the Bills of Mortality¬ª (1662), uno de los primeros trabajos en los que ya se hablaba de **exceso de mortalidad** a partir de las primeras tablas de natalidad y mortalidad, **estimando la poblaci√≥n de Londres**.
]

.pull-right[

### G. Neumann

Las **fakes news** ya exist√≠an en el siglo XVII: Gaspar Neumann tambi√©n un precursor en el **an√°lisis estad√≠stico de tablas de mortalidad**, para desmentir bulos (ejemplo: desmont√≥ la creencia de que en los a√±os acabados en siete mor√≠an m√°s personas).
]

&nbsp;

Son precisamente las tablas de Graunt las que us√≥ **Christiaan Huygens** (pionero en teor√≠a de probabilidad con su ¬´De ratiociniis in ludo aleae¬ª en 1656) para generar la **primera gr√°fica de densidad** de una distribuci√≥n continua, visualizando la **esperanza de vida** (en funci√≥n de la edad).


---

# Primer gr√°fico de densidad


```{r echo = FALSE,  out.width = "50%", fig.align = "center", fig.cap = "Primera funci√≥n de densidad, extra√≠da de https://omeka.lehigh.edu/exhibits/show/data_visualization/vital_statistics/huygen"}
knitr::include_graphics("https://omeka.lehigh.edu/files/fullsize/65fc32c11a768f1d3263a99caca28dff.jpg")
``` 

---

# El gran boom: los gr√°ficos de Playfair

La figura que cambi√≥ el dataviz fue, sin lugar a dudas, el economista y pol√≠tico **William Playfair (1759-1823)**. En 1786 public√≥ el **¬´Atlas pol√≠tico y comercial¬ª**<sup>1,2</sup> con 44 gr√°ficas (43 series temporales y el **diagrama de barras m√°s famoso**, aunque no el primero).

.pull-left[

```{r echo = FALSE, out.width = "85%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_1.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "35%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair, extra√≠das de Funkhouser y Walker (1935)"}
knitr::include_graphics("./img/playfair_2.jpg")
``` 

]

[1] [üìö ¬´Atlas pol√≠tico y comercial¬ª de William Playfair (1786)](https://www.amazon.es/Playfairs-Commercial-Political-Statistical-Breviary/dp/0521855543)

[2] [üìö ¬´Playfair and his charts¬ª de H. Gray Funkhouser and  Helen M. Walker (1935)](https://www.jstor.org/stable/45366440)

---

# Primer gr√°fico de barras

Playfair es adem√°s el **autor del gr√°fico de barras m√°s famoso** (aunque no fue el primero, pero s√≠ el que sent√≥ un precedente, quien lo hizo _mainstream_).

.pull-left[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Gr√°ficas de Playfair de importaciones (barras grises) y exportaciones (negras) de Escocia en 1781, extra√≠das de la wikipedia."}
knitr::include_graphics("./img/playfair_5.jpg")
``` 

]

.pull-right[

```{r echo = FALSE, out.width = "95%", fig.align = "center", fig.cap = "Primer diagrama de barras (Philippe Buache y Guillaume de L‚ÄôIsle), visualizando los niveles del Sena desde 1732 hasta 1766, extra√≠da de https://friendly.github.io/HistDataVis"}
knitr::include_graphics("./img/playfair_6.jpg")
``` 

]


---


# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

### F. Galton

Primo de Charles Darwin, inventor de los **silbatos para perretes**, de los mapas de predicci√≥n meteorol√≥gica y la persona que acu√±√≥ el concepto de **regresi√≥n** (y el de eugenesia :/).

```{r echo = FALSE, out.width = "93%", fig.align = "center"}
knitr::include_graphics("https://www.bogleheads.org/w/images/thumb/9/95/Screen_Shot_2012-01-03_at_7.36.29_AM.png/600px-Screen_Shot_2012-01-03_at_7.36.29_AM.png")
``` 


]

.pull-right[

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_1.jpg")
``` 

```{r echo = FALSE, out.width = "58%", fig.align = "center"}
knitr::include_graphics("./img/galton_2.png")
``` 

]



---

# Epidemiolog√≠a y bioestad√≠stica

.pull-left[

### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street.

]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("https://media.revistagq.com/photos/5cc84a91c46d3a2b7435d7cf/2:3/w_1799,h_2699,c_limit/pelo%20jon%20snow.jpg")
``` 

]

---

# El boom de la estad√≠stica: epidemiolog√≠a y bioestad√≠stica


.pull-left[

#### John Snow

Se le considera uno de los pioneros de la **epidemiolog√≠a moderna** y la **estad√≠stica espacial**: aunque los **diagramas de Voronoi** tardar√≠an a√±os en ser formalizados, John Snow aplic√≥ el mismo concepto para mitigar la **epidemia de c√≥lera en Londres**, con su **mapa con diagrama de barras**, localizando el foco en la conocida fuente de Broad Street<sup>1</sup>.


[1] [üìö ¬´El mapa fantasma¬ª, Steven Johnson, sobre la historia de John Snow](https://capitanswing.com/libros/el-mapa-fantasma/)


]

.pull-right[

```{r echo = FALSE, out.width = "100%", fig.align = "center", fig.cap = "John Snow, el epidemi√≥logo"}
knitr::include_graphics("https://s1.eestatic.com/2016/04/22/reportajes/reportajes_119248513_3987143_854x640.jpg")
``` 

]

---

# Primeros usos de la estad√≠stica espacial

```{r echo = FALSE, out.width = "77%", fig.align = "center", fig.cap = "Mapa de Londres, mostrando los casos de c√≥lera del 19 de agosto al 30 de septiembre de 1854, extra√≠do de https://friendly.github.io/HistDataVis."}
knitr::include_graphics("./img/snow_mapa.jpg")
``` 

---

# ¬øQu√© es el an√°lisis multidimensional?

Hasta la d√©cada de los 60, la mayor√≠a de la estad√≠stica que se realizaba era

* **estad√≠stica unidimensional**: extraer informaci√≥n de una sola variable (rentas, impuestos, exportaciones, etc).

* **estad√≠stica bidimensional**: desde que Galton acu√±√≥ la regresi√≥n, los grandes estad√≠sticos de principios de siglo se centraron en el **an√°lisis bidimensional**, analizando la dependencia entre una variable $X$ y otra variable $Y$, con herramientas como los **coeficientes de correlaci√≥n** de Pearson, Spearman o Kendall<sup>1</sup>


[1] [üìö Kendall, M. (1938). ¬´A New Measure of Rank Correlation¬ª. Biometrika 30 (1‚Äì2): 81-89. doi:10.1093/biomet/30.1-2.81](https://doi.org/10.1093/biomet/30.1-2.81)

--

&nbsp;

## Wishart, contigo empez√≥ todo

En 1928, Wishart public√≥ su famoso art√≠culo <sup>2</sup> en el que, se demostraba y desarrollaba expl√≠citamente la funci√≥n de distribuci√≥n de una 
**distribuci√≥n normal multivariante**, trabajo m√°s tarde extendido y formalizado por Fisher.

[2] [üìö Wishart, J. (1928). ¬´The generalised product moment distribution in samples from a normal multivariate population¬ª. Biometrika. 20A (1‚Äì2): 32‚Äì52. doi:10.1093/biomet/20A.1-2.32](https://doi.org/10.1093/biomet/20A.1-2.32)


---
 
# ¬øQu√© es el an√°lisis multidimensional?
 
Aunque el verdadero boom no lleg√≥ hasta la **d√©cada de los 60**, con la publicaci√≥n del libro ¬´An Introduction to Multivariate Statistical Analysis¬ª<sup>1</sup> de Anderson (1958), proporcionando todo un marco te√≥rico con el que poder trabajar.

[1] [üìö Anderson, T.W. (1958). ¬´An Introduction to Multivariate Analysis¬ª. New York: Wiley ISBN 0471026409](http://www.ru.ac.bd/stat/wp-content/uploads/sites/25/2019/03/301_03_Anderson_An-Introduction-to-Multivariate-Statistical-Analysis-2003.pdf)

--

&nbsp;

## **Definici√≥n**

> El An√°lisis Multivariante es la rama de la estad√≠stica que estudia las relaciones (CONJUNTAMENTE) entre conjuntos de variables dependientes y los individuos para los cuales se han medido dichas variables (Kendall)


## **Notaci√≥n**

* $n$ tama√±o muestral (n√∫mero de individuos --> filas).

* $\boldsymbol{X}_i = \left(\boldsymbol{X}_{1, i}, \ldots, \boldsymbol{X}_{i, p} \right)$ conjunto de $p$ variables (--> columnas) medidas para cada individuo $i=1,\ldots,n$.

* Nuestros datos estar√°n en forma de tabla o matriz $\boldsymbol{X}$ de $n$ filas y $p$ columnas (con $p \ll n$)

---

# Ejemplo de distribuci√≥n bidimensional: normal bivariante

.pull-left[ 

Veamos un ejemplo sencillo con algo que seguramente nos sea familiar: la **distribuci√≥n Normal o campana de Gauss** $X \sim \mathcal{N}\left(\mu, \sigma \right)$, cuya funci√≥n de densidad es

$$f(x) = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}}, \quad \mu \in\mathbb{R},~\sigma >0$$

&nbsp;

La normal univariante depende de **dos par√°metros**:

* **esperanza o media** $\mu = {\rm E} [X]$ 
* **varianza** (unidimensional) $\sigma^2 := {\rm Var} [X] = {\rm E} [\left(X - \mu \right)^2] = {\rm E} [X^2] - \mu^2$


]


.pull-right[

```{r eval = FALSE}
# Generamos muestra normal
rnorm(n = 10000, mean = 0, sd = 1)
```

```{r echo = FALSE}
library(tidyverse)
# Generamos una muestra normal (n = 10 000)
data <- tibble("x" = rnorm(n = 10000, mean = 0, sd = 1))
```

```{r echo = FALSE, out.width = "80%"}
# Ploteamos
ggplot(data, aes(x = x)) +
  geom_density(fill = "#F29288", alpha = 0.5, size = 1.2) +
  labs(x = "x", y = "f(x) (funci√≥n densidad)")
```
  

]

---

# Normal bivariante

### **¬øY si medimos para cada individuo DOS variables?**

Si tenemos $\boldsymbol{X} = \left(X_1, X_2 \right)$, ¬øqu√© estad√≠sticos tenemos ahora a nuestra disposici√≥n?

* **Medidas marginales** (cada variable por separado):
  - medias $\mu_1:= {\rm E} [X_1]$ y $\mu_2:= {\rm E} [X_2]$
  - varianzas $\sigma_{1}^{2}:=\sigma_{1, 1}^{2} = \sigma_{X_1, X_1}^2$ y $\sigma_{2}^{2}:=\sigma_{2, 2}^{2} = \sigma_{X_2, X_2}^2$.

--

&nbsp;

### **Covarianza**

La varianza ${\rm Var} [X] := \sigma_{X}^2 = {\rm E} [ \left( X - \mu \right)^2 ]$ es una medida de dispersi√≥n que nos **cuantifica** la relaci√≥n de una variable consigo misma. ¬øY si en lugar de medir $X_1$ vs $X_1$ medimos $X_1$ vs $X_2$?

Definiremos la **covarianza** como una especie de varianza en la que cambiamos una de las $X$ por otra variable

$${\rm Cov} [X_1, X_2] := \sigma_{1,2} =  {\rm E} [ \left( X_1 - \mu_1 \right) \left( X_2 - \mu_2 \right) ] = {\rm E}[X_1 * X_2] - \mu_1 * \mu_2 = \sigma_{2,1}$$

---

# Normal bivariante

### **Matriz de covarianzas**

Desde un punto de vista te√≥rico, dada una variable aleatoria bidimensional $\boldsymbol{X} = \left(X_1, X_2 \right)^{T}$, con vector de medias $\boldsymbol{\mu} = \left(\mu_1, \mu_2 \right)^{T}$ definiremos la **matriz de varianzas y covarianzas** $\Sigma$ de la siguiente manera:

$$\boldsymbol{\Sigma} := \begin{pmatrix} \sigma_{1,1}^2 & \sigma_{1,2} \\ \sigma_{2,1} & \sigma_{2,2}^2 \end{pmatrix} = \begin{pmatrix} \sigma_{1}^2 & \sigma_{1,2} \\ \sigma_{1,2} & \sigma_{2}^2 \end{pmatrix}, \quad \left| \boldsymbol{\Sigma} \right| = \sigma_{1}^{2}  \sigma_{2}^{2} - \sigma_{1,2}^{2} > 0$$

--

Se puede expresar **matricialmente** como

$\begin{eqnarray}\boldsymbol{\Sigma} = {\rm E} \left[\left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T}\left(\boldsymbol{X} - \boldsymbol{\mu} \right) \right] &=& {\rm E} \left[\left( X_1  - \mu_1, X_2 - \mu_2 \right)^{T} \begin{pmatrix} X_1  - \mu_1 \\ X_2 - \mu_2 \end{pmatrix} \right] \\ &=& \begin{pmatrix} {\rm E} \left[ \left(X_1  - \mu_1 \right)^2 \right] & {\rm E} \left[\left(X_1  - \mu_1 \right)\left(X_2  - \mu_2 \right) \right] \\ {\rm E} \left[\left(X_2  - \mu_2 \right)\left(X_1  - \mu_1 \right) \right] & {\rm E} \left[\left(X_2  - \mu_2 \right)^2\right] \end{pmatrix} \end{eqnarray}$

**IMPORTANTE**: es una **matriz sim√©trica** (nos da igual medir $X$ vs $Y$, que $Y$ vs $X$).

---

# Normal multivariante

### **Normal univariante**

$$X \sim \mathcal{N} \left(\mu, \sigma^2 \right), \quad \boldsymbol{\Sigma} = \sigma^2, \quad f(x) =  \frac{1}{\sigma {\sqrt{2\pi}}} e^{-{\frac{(x-\mu )^{2}}{2\sigma^{2}}}} = \frac{1}{\sigma {\sqrt{2\pi}}} e^{-\frac{1}{2} (x-\mu ) \boldsymbol{\Sigma}^{-1} (x-\mu )}$$


### **Normal bivariante**

$$\boldsymbol{X} = \left(X_1, X_2 \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, x_2) = \frac{1}{2\pi \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

--

### **Normal multivariante (caso general)** 

Multivariante de $p \ll n$ variables

$$\boldsymbol{X} = \left(X_1, \ldots, X_p \right)^{T}  \sim \mathcal{N} \left( \boldsymbol{\mu}, \boldsymbol{\Sigma} \right), \quad f(x_1, \ldots, x_p) = \frac{1}{\left(2\pi \right)^{p/2} \left| \Sigma \right|^{1/2}} e^{-\frac{1}{2}{(\boldsymbol{x} - \mu )^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x} - \mu )}}$$

$$\boldsymbol{\Sigma} = \left(\Sigma_{i,j} \right)_{i,j=1,\ldots,p}, \quad \Sigma_{i,j}:= {\rm Cov} [X_i, X_j ] = {\rm E}[(X_i-\mu_i) (X_j - \mu_j)]$$

---

# Versi√≥n muestral

Lo anterior nos permite conocer la **formulaci√≥n te√≥rica (poblacional)**: ¬øc√≥mo calculamos la varianza y covarianza cuando tenemos una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ observaciones medidas?


$$\boldsymbol{X} = \begin{pmatrix} x_{1, 1} & \ldots & x_{1, p} \\ \vdots & \ddots & \vdots \\ x_{n, 1} & \ldots & x_{n, p} \end{pmatrix} \quad \text{muestra}$$

#### **p = 2**

* **Varianzas muestrales**: $s_{x_1}^{2} := s_{1}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 1} - \overline{x}_1 \right)^2$ y $s_{x_2}^{2} := s_{2}^2 = \frac{1}{n} \sum_{i=1}^n \left(x_{i, 2} - \overline{x}_2 \right)^2$, donde $\overline{x}_1$ y $\overline{x}_2$ son sus medias muestrales.

* **Covarianza muestral**: $s_{x_1, x_2}^{2} := s_{1, 2} = s_{2, 1}^2 = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \left(x_{i, 1} - \overline{x}_1 \right)\left(x_{j, 2} - \overline{x}_2 \right)$

--

#### **Estimadores insesgados**

Seguramente dichos valores los hallas visto divididos por $n-1$ en lugar de $n$: los valores muestrales son estimadores de los valores poblacionales, y de aqu√≠ en adelante usaremos **estimadores insesgados**, estimadores $T$ del valor poblaci√≥n $U$ tal que ${\rm E}[T] = U$

* Estimador insesgado de $\mu_{x}$: $\overline{x}$ tal que ${\rm E}[\overline{x}] = \mu$

* Estimador insesgado de $\sigma_{x}^2$: la **cuasivarianza** $S_{x}^2 = \frac{n}{n-1} s_{x}^{2}$ tal que ${\rm E}[\sigma_{x}^2] = S_{x}^2$

* Estimador insesgado de $\sigma_{x, y}$: la **cuasicovarianza** $S_{x, y} = \frac{n}{n-1} s_{x, y}$ tal que ${\rm E}[\sigma_{x, y}] = S_{x, y}$

---

# Matriz de covarianzas (versi√≥n muestral)


En un **caso general**, dada una muestra $\boldsymbol{X}$ de $n$ individuos y $p$ variables

$$S_{x_{k}}^2 := S_{k}^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left(x_{i, k} - \overline{x}_k \right)^2 \quad \text{(cuasi) var. muestrales (marginales)}$$


$$S_{x_{k}, x_{l}} := S_{k, l} = \frac{1}{n-1} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_{i, k} - \overline{x}_k \right)\left(x_{j, l} - \overline{x}_l \right) \quad \text{(cuasi) covarianzas}$$

As√≠, la **matriz de (cuasi) covarianzas emp√≠ricas** quedar√° como

$$S := \frac{1}{n-1} \left(\boldsymbol{X} - \boldsymbol{\mu} \right)^{T} \left(\boldsymbol{X} - \boldsymbol{\mu} \right) =_{\boldsymbol{\mu} = 0} \frac{1}{n-1} \boldsymbol{X}^{T} \boldsymbol{X} = \begin{pmatrix} S_{1,1} &  \ldots & S_{1, p} \\ \vdots & \ddots & \vdots \\ S_{p,1} & \ldots & S_{p, p} \end{pmatrix}$$

&nbsp;

--

Las **covarianzas (y varianzas)** tienen un **¬´problema¬ª**: **dependen de la magnitud** de los datos, proporcionando una medida que solo nos sirve para ser comparada con otra covariana, pero que **no nos proporciona una escala absoluta** para poder cuantificar.

---

# Matriz de correlaciones (versi√≥n muestral)


Para resolverlo, tenemos la **correlaci√≥n (de Pearson)** 

$$\rho_{k, l} := r_{k, l} = \frac{s_{k, l}}{\sqrt{s_{k}^2} \sqrt{s_{l}^2}} = \frac{S_{k, l}}{\sqrt{S_{k}^2} \sqrt{S_{l}^2}}$$

tal que siempre $-1 \leq r_{k, l} \leq 1$.

&nbsp;

--

De esta forma la **matriz de correlaciones** se puede expresar como

$$R := \left(r_{k, l} \right)_{k,l=1,\ldots,p} = D^{-1/2} S D^{-1/2}, \quad D = diag(S) = \begin{pmatrix} S_{1,1}^2 & \ldots & 0 \\ \vdots  & \ddots & \vdots \\  0 & \ldots & S_{p, p}^2 \end{pmatrix}$$


---


name: teoria-PCA
class: center, middle

# Teor√≠a: an√°lisis de componentes principales

---

# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

El **objetivo ¬´mainstream¬ª** del **an√°lisis de componentes principales** (PCA en ingl√©s) suele ser el de **reducir la dimensi√≥n** de nuestros datos: pasar de un conjunto de $n$ individuos y $p$ variables a otro de $k < p$ variables (para los mismos $n$ individuos).

&nbsp;

Esta reducci√≥n de la dimensi√≥n se suele hacer con **3 objetivos** principalmente:

* **Mejora computacional** de los algoritmos al tener un dataset m√°s reducido.

* **Permitir la visualizaci√≥n** en 2 o 3 dimensiones de conjuntos $n$-dimensionales.

* **¬´Reflotar¬ª patrones** subyacentes en los datos.

]

.pull-right[

```{r echo = FALSE,  out.width = "100%", fig.align = "center", fig.cap = "Extra√≠da de https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa"}

knitr::include_graphics("https://miro.medium.com/max/959/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg")
``` 

]

---


# Objetivo: ¬øreducir dimensi√≥n?

.pull-left[

¬øEntonces? ¬øNo tiene sentido aplicar componentes principales o t√©cnicas de reducci√≥n de la dimensi√≥n en **datos bidimensionales**?

&nbsp;

Empecemos por un sencillo ejemplo, visualizando la **longitud y anchura de p√©talo** del famoso conjunto de datos `iris`

&nbsp;

**¬øCu√°les podr√≠an ser los objetivos?** ¬øTiene sentido en este ejemplo aplicar **t√©cnicas de reducci√≥n de la dimensi√≥n** como las componentes principales?

]

.pull-right[

```{r echo = FALSE, out.width = "100%"}
library(tidyverse)
ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).") +
  theme_minimal()
```

]

---

# Objetivo: maximizar la informaci√≥n


```{r echo = FALSE,  out.width = "80%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

Como veremos, el **objetivo real** ser√° **maximizar la informaci√≥n obtenido al menor coste posible**, y eso hace que siga siendo √∫til, aunque no reduzcamos dimensiones, hacerlo en el caso bidimensional: una **clave** de las componentes principales es que las **componentes resultantes** ser√°n **ortogonales** (perpendiculares), es decir, **linealmente independientes**.

&nbsp;

Las **componentes principales** pueden ser una herramienta muy √∫til para atajar problemas de **colinealidad** (variables altamente correladas entre s√≠, interfiriendo entre ellas)


---


# Idea principal

La **idea subyacente** tras el c√°lculo de las componentes principales se puede resumir de forma **geom√©trica**: para un conjunto de puntos $p$-dimensionales, encontrar un **nuevo sistema de coordenadas** de dimensi√≥n $k \leq p$ en el que expresar los datos, de forma que las **nuevas variables sean linealmente independientes**.

En el **caso bidimensional**, el resultado de aplicar componentes principales ser√° una especie de ¬´rotaci√≥n¬ª de los datos


```{r echo = FALSE, out.width = "75%", fig.align = "center"}
knitr::include_graphics("https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png")
``` 

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>


---

# Idea principal: caso bidimensional


En el **caso bidimensional**, la idea ser√° buscar esa **elipse** en torno a la cual tenemos los datos, de forma que la direcci√≥n que marca el **eje mayor** ser√° la **primera componente** (la que tiene mayor rango --> mayor varianza) y la direcci√≥n que marca el **eje menor** ser√° la **segunda componente**.


.pull-left[

```{r echo = FALSE,  out.width = "77%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_1.jpg")
```

]


.pull-right[

```{r echo = FALSE,  out.width = "58%", fig.align = "center", fig.cap = "Gr√°fica extra√≠da de Herv√© and Lynne (2010)"}
knitr::include_graphics("./img/pca_words_23.jpg")
```

]

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

---

# Caso inicial bidimensional


.pull-left[

Vamos a empezar por un **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del p√©talo.

```{r echo = FALSE}
iris_bi <-
  tibble(iris) %>%
  select(contains("Petal"))
```

```{r echo = FALSE}
iris_bi
```

]

.pull-right[

```{r echo = FALSE}
ggplot(iris_bi, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 3) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).") +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 23),
        axis.text.x = element_text(size = 15),
        axis.title.y = element_text(size = 23),
        axis.text.y = element_text(size = 15),
        plot.caption = element_text(size = 15))
```

]

---

# Caso bidimensional

.pull-left[

1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.

$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$


]

.pull-right[

```{r echo = FALSE,  out.width = "85%", fig.align = "center", fig.cap = "Direcciones de m√°xima varianza"}
knitr::include_graphics("./img/pca_iris_1.jpg")
```

]

---

# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---


# Caso bidimensional

.pull-left[


1. Encontrar las **direcci√≥nes de m√°xima varianza**. Dichas direcciones vendr√°n determinadas por **dos vectores** $\left\lbrace \Phi_1, \Phi_2 \right\rbrace$ perpendiculares entre s√≠ y que ser√°n **combinaci√≥n lineal de las variables** originales.
$$\Phi_1 = z_{1, 1} * \boldsymbol{x}_1 + z_{2, 1} * \boldsymbol{x}_2, \quad \Phi_2 = z_{1, 2} * \boldsymbol{x}_1 + z_{2, 2} * \boldsymbol{x}_2$$

2. Dado un registro $\boldsymbol{x}_i = \left(x_{i, 1}, x_{i, 2} \right)$ (que puede entenderse como un vector $\overline{\boldsymbol{x}}_i := \boldsymbol{x}_i$), lo que haremos ser√° obtener las **nuevas coordenadas** **proyectando ortogonalmente** el vector sobre las nuevas direcciones:
$$x_{i, 1}' =\left| \boldsymbol{x}_i \right| cos (\alpha)  =  \frac{\langle \boldsymbol{x}_i, \Phi_1 \rangle}{ \left| \Phi_1 \right|}, \quad x_{i, 2}' =  \frac{\langle \boldsymbol{x}_i, \Phi_2 \rangle}{ \left| \Phi_2 \right|}$$

3. Las **nuevas direcciones** las seleccionaremos  **ortonormales** (m√≥dulo unitario):
$$x_{i, 1}'  =  \langle \boldsymbol{x}_i, \Phi_1 \rangle =  \left(x_{i, 1}, x_{i, 2} \right) \left(z_{1, 1}, z_{2, 1} \right)^{T} = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{1}^{T}, \quad x_{i, 2}' = \langle \boldsymbol{x}_i, \Phi_2 \rangle = \boldsymbol{x}_{i} \boldsymbol{\Phi}_{2}^{T}$$

]

.pull-right[

```{r echo = FALSE,  out.width = "78%", fig.align = "center", fig.cap = "Proyecci√≥n ortogonal"}
knitr::include_graphics("./img/pca_iris_2.jpg")
```

]

---

# Idea general


Nuestros datos originales $\boldsymbol{X}$  (dimensiones $n \times p$) ser√°n reconvertidos en un conjunto $\boldsymbol{X}'$ de dimensiones $n \times k$, con $k \leq p$, tal que 

$$\boldsymbol{X}' = \boldsymbol{X} \boldsymbol{\Phi}^{T}$$

--

tal que $\boldsymbol{\Phi}^{T}$ es una matriz $p \times k$ que contiene por columnas las $k$ **direcciones principales**

$$\boldsymbol{\Phi}^{T} = \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2} \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix}$$

--

bajo la condici√≥n de que sean **direcciones ortonormales**

$$\Phi \Phi^{T} = \begin{pmatrix} 1 & \ldots & 0 \\  \vdots &  \ddots & \vdots \\ 0  & \ldots & 1 \end{pmatrix}$$

tal que dichas direcciones **maximicen la varianza**.

---

# Primera componente

Por ejemplo, para la **primera componente** el objetivo es encontrar, de entre todas las direcciones  $\boldsymbol{u}_1$ posibles, la direcci√≥n $\boldsymbol{\Phi}_1$ que **maximice la varianza de nuestros datos cuando los proyectamos sobre dicha direcci√≥n**

$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ z_{1,2} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ x_{2,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$


--

&nbsp;

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

---

# Primera componente

Dicha direcci√≥n por tanto saldr√° de un proceso de **optimizaci√≥n**

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{x}_{1}'  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)$$

--

Si **centramos los datos** (restamos su media para tener media nula)


$$\begin{eqnarray}\boldsymbol{\Phi}_1 &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} {\rm Var} \left( \boldsymbol{X}\boldsymbol{u}_{1}^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left({\rm E} \left[\left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right)^{T}\left( \boldsymbol{X} \boldsymbol{u}_{1}^{T} \right)\right] \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_{1} = 1} \left({\rm E} \left[ \boldsymbol{u}_{1} \boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{u}_1^{T} \right] \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 {\rm E} \left[\boldsymbol{X}^{T} \boldsymbol{X} \right] \boldsymbol{u}_{1}^{T}  \right) \\ &=& \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_{1}^{T}  \right)\end{eqnarray}$$

--

Si **estandarizamos los datos** (restamos su media y dividimos entre su desviaci√≥n t√≠pica, teniendo **datos con media cero y varianza unitaria** para que todos los datos ponderen por igual)

$$\boldsymbol{\Phi}_1 = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} {\rm Var} \left( \boldsymbol{X} \boldsymbol{u}_1^{T} \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 S \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right)$$

---

# Primera componente

Para **encontrar esa direcci√≥n $\boldsymbol{u}_1$** que nos maximiza la varianza de los proyectados en ella, sujeto a la restrcci√≥n de que $\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1$, se puede usar la t√©cnica de los **multiplicadores de Lagrange** que nos dice que

$$\boldsymbol{\Phi}_1 =  \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  \right) = \arg \max_{\boldsymbol{u}_{1}^{T} \boldsymbol{u}_1 = 1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right), \quad \lambda \in \mathbb{R}$$

--

Eso es equivalente a encontrar el valor que nos **iguale la derivada a cero**

$$\frac{\partial}{\partial \boldsymbol{u}_1} \left( \boldsymbol{u}_1 R \boldsymbol{u}_1^{T}  - \lambda \left(\boldsymbol{u}_1^{T} \boldsymbol{u}_1  - 1\right) \right) =   R \boldsymbol{u}_1  - \lambda \boldsymbol{u}_1^{T} = \left(R - \lambda \boldsymbol{Id}_{p} \right) \boldsymbol{u}_1^{T}  =  0$$
--

Esto es lo mismo que decir que $R \boldsymbol{u}_1^{T}  = \lambda \boldsymbol{u}_1^{T}$, es decir, la direcci√≥n que buscamos $\boldsymbol{u}_{1}^{T}$ es un **autovector de la matriz de covarianzas** (tras **estandarizar** los datos).

---

# Par√©ntesis: autovectores y autovalores

En √°lgebra matricial, dada una matriz $\boldsymbol{A}$ cuadrada de tama√±o $p \times p$, decimos que $v$ es su **autovector** y $\lambda$ su **autovalor asociado** si y solo s√≠

$$A v= \lambda v, \quad v = \left(v_1, \ldots, v_p \right) \neq 0$$

Esto es equivalente a decir que 

$$A v - \lambda v = 0 \rightarrow (A - \lambda I_{p}) v = 0$$

donde $I_p$ es la matriz identidad de tama√±o $p \times p$. Dicha ecuaci√≥n tiene soluci√≥n si y solo s√≠

$$\left| A - \lambda I_{p} \right| = 0$$

Adem√°s, por el **Teorema Fundamental del Algebra** sabemos que dicho determinante puede expresarse como un polinomio de grado $p$ (conocido como **polinomio caracter√≠stico**)

$$\left| A - \lambda I_{p} \right| = \left(\lambda_1 - \lambda \right)\left(\lambda_2 - \lambda \right) \ldots \left(\lambda_p - \lambda \right) = p (\lambda)$$

Adem√°s el determinante $\left| A \right|$ ser√° el producto de todos sus autovalores.

---

# Primera componente

Recapitulando, para obtener la **primera componente** $\boldsymbol{\Phi}_1$, debemos de 

* **Estandarizar** nuestros datos
* Calcular la **matriz de (cuasi)covarianzas** $\boldsymbol{S}$
* Calcula sus **autovectores** tal que $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$ (normalizados a m√≥dulo 1).

--

Adem√°s si es un autovector de la matriz de covarianzas tenemos entonces que la **varianza maximizada**, la **proporci√≥n de informaci√≥n** que **explica dicha componente**, ser√°

$$\boldsymbol{\Phi}_{1} \left(  S \boldsymbol{\Phi}_{1}^{T} \right) = \boldsymbol{\Phi}_{1} \left( R \boldsymbol{\Phi}_{1}^{T} \right) =\boldsymbol{\Phi}_{1}\left(  \lambda_1 \boldsymbol{\Phi}_{1}^{T} \right) =  \lambda_1 \boldsymbol{\Phi}_{1} \boldsymbol{\Phi}_{1}^{T} =_{\text{ortonormales}} \lambda_1$$ 

--

As√≠ que obtener la direcci√≥n (de todos los autovalores) que mayor informaci√≥n captura nos fijaremos en aquella que tenga **asociada el autovalor m√°s grande**.


$$\boldsymbol{x}_{1}' = \boldsymbol{X} \boldsymbol{\Phi}_{1}^{T} = \begin{pmatrix} x_{1,1} & \ldots & x_{1, p}  \\ \vdots  & \ddots & \vdots
\\ x_{n,1}  & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} \\ \vdots \\ z_{1,p} \end{pmatrix} =  \begin{pmatrix} x_{1,1}^{'} \\ \vdots \\ x_{n, 1}^{'} \end{pmatrix}$$

donde $S \boldsymbol{\Phi}_{1}^{T} = \lambda_1 \boldsymbol{\Phi}_{1}^{T}$, siendo $\lambda_1$ el mayor de los autovalores de la matriz de (cuasi)covarianzas $S$, y $\boldsymbol{\Phi}_{1}^{T}$ su autovector asociado. El **resto de las componentes** se obtendr√°n de forma similar, siendo ortogonales a cada una de las direcciones obtenidas.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Dados unos datos $\boldsymbol{X}$ de $n$ individuos y $p$ variables, el objetivo es encontrar nuevas **direcciones ortonormales** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$, con $1 \leq k \leq p$, como combinaci√≥n lineal de las variables originales.

* Los **datos son estandarizados**  tal que

$$\begin{pmatrix} \frac{x_{1,1} - \overline{x}_1}{S_{1}} & \frac{x_{1,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{1,p} - \overline{x}_p}{S_{p}} \\  \frac{x_{2,1} - \overline{x}_1}{S_{1}} & \frac{x_{2,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{2,p} - \overline{x}_p}{S_{p}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{x_{n,1} - \overline{x}_1}{S_{1}} & \frac{x_{n,2} - \overline{x}_2}{S_{2}} & \ldots & \frac{x_{n,p} - \overline{x}_p}{S_{p}} \end{pmatrix}$$

* Calcular la **matriz $S$ de (cuasi)covarianzas** de dichos datos estandarizados.

---

# Idea general paso a paso 

El proceso completo es el siguiente:

* Calculamos los $p$ **autovectores** $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_p \right\rbrace$, y sus **autovalores asociados** $\left\lbrace \lambda_1, \ldots, \lambda_p \right\rbrace$, de la matriz $S$, tal que $S  \boldsymbol{\Phi}_k = \lambda_k  \boldsymbol{\Phi}_k$.

* Seleccionamos las primeras $k \leq p$ componentes $\left\lbrace \boldsymbol{\Phi}_1, \ldots, \boldsymbol{\Phi}_k \right\rbrace$ asociadas a los primeros $\left\lbrace \lambda_1, \ldots, \lambda_k \right\rbrace$ autovalores.

* La **varianza (informaci√≥n) capturada** por la direcci√≥n $k$-√©sima ser√° igual a $\lambda_k$.

* Las nuevas coordenadas ser√°n

$$\boldsymbol{X}^{'} = \boldsymbol{X} \boldsymbol{\Phi}^{T} = \begin{pmatrix} x_{1,1} & x_{1, 2} & \ldots & x_{1, p} \\ x_{2,1} & x_{2, 1} & \ldots & x_{2, p} \\ \vdots & \vdots & \ddots & \vdots
\\ x_{n,1} & x_{n, 2} & \ldots & x_{n, p}\end{pmatrix} \begin{pmatrix} z_{1,1} & z_{2,1} & \ldots & z_{k,1} \\ z_{1,2} & z_{2,2} & \ldots & z_{k,2}  \\ \vdots & \vdots & \ddots & \vdots \\ z_{1,p} & z_{2,p} & \ldots & z_{k,p} \end{pmatrix} = \begin{pmatrix} \boldsymbol{x}_1 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_1 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_1 \boldsymbol{\Phi}_k^{T} \\ \boldsymbol{x}_2 \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_2 \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_2 \boldsymbol{\Phi}_k^{T} \\ \vdots & \vdots & \ddots & \vdots \\ \boldsymbol{x}_n \boldsymbol{\Phi}_1^{T} & \boldsymbol{x}_n \boldsymbol{\Phi}_2^{T} & \ldots & \boldsymbol{x}_n \boldsymbol{\Phi}_k^{T}\end{pmatrix}$$

---

# Glosario 

* **Autovectores**: nos indican la **direcci√≥n de la componente**

* **Loadings**: ser√°n los **coeficientes** de dichos autovectores (los que generan la combinaci√≥n lineal de las variables originales) nos indican el **peso que tiene cada variable original en dicha componente**. Si por ejemplo $\boldsymbol{\Phi}_1 = \left(0.95, 0.15, 0.273 \right)$, significa que la primera componente (la que m√°s varianza captura) ser√° $0.95* \boldsymbol{X}_1 + 0.15 * \boldsymbol{X}_2 + 0.273 * \boldsymbol{X}_3$, estando dominada por la variable original $\boldsymbol{X}_1$ (un peso de 0.95).


* **Signo de los loadings**: nos indica el **sentido de la relaci√≥n entre la componente y la variable original** (correlaci√≥n positiva/negativa). Si alguno de ellos fuese 0 significar√≠a que la nueva componente est√° incorrelada respecto a dicha variable original.


* **Scores**: para cada observaci√≥n $i$, las nuevas coordenadas $\left(x_{i, 1}^{'}, \ldots, x_{i, k}^{'} \right)$, calculadas tras **proyectar la observaci√≥n original en las direcciones principales** $\boldsymbol{x}_{i} \boldsymbol{\Phi}^{T}$.


* **Truncamiento o n√∫mero de componentes**: para seleccionar el n√∫mero $k$ de componentes a seleccionar el m√©todo m√°s sencillo es **fijar de antemano** un **umbral varianza explicada** que queremos conservar (por ejemplo, $95%$), de forma que nos quedemos con el primer n√∫mero $k$ tal que $\sum_{j=1}^{k} \lambda_k > 0.95$ (**varianza explicada acumulada**, teniendo los autovalores ordenados de mayor a menor).


---


name: practica-PCA
class: center, middle

# Pr√°ctica: PCA en R

---

# PCA en R: caso ¬´manual¬ª


Volvemos a nuestro **ejemplo sencillo (bidimensional)** tomando de `{iris}` solo las variables del p√©talo.

```{r}
iris_bi <- 
  tibble(iris) %>%
  select(contains("Petal"))
iris_bi
```

---

# PCA en R: caso ¬´manual¬ª


**Primer paso**: **estandarizar** los datos.

```{r}
iris_bi_std <-
  iris_bi %>%
  mutate(Petal.Length = (Petal.Length - mean(Petal.Length)) /  sd(Petal.Length),
         Petal.Width = (Petal.Width - mean(Petal.Width)) / sd(Petal.Width))

iris_bi_std
```

---

# PCA en R: caso ¬´manual¬ª

.pull-left[
```{r echo = FALSE}
ggplot(iris_bi, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5) +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "Datos originales") +
  theme_minimal()
```

]

.pull-right[
```{r echo = FALSE}
ggplot(iris_bi_std, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5, color = "darkolivegreen") +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "Datos estandarizados") +
  theme_minimal()
```

]

---


# PCA en R: caso ¬´manual¬ª

**Segundo paso**: calcular la **matriz de covarianzas**

```{r}
cov_mat <- cov(iris_bi_std)
cov_mat
```

Al estar estandarizados los datos, es equivalente a calcular la matriz de correlaciones

```{r}
library(corrr)
iris_bi_std %>% correlate(diagonal = 1)
```

---

# PCA en R: caso ¬´manual¬ª

**Tercer paso**: calcular los **autovalores y autovectores** de la matriz de covarianzas

```{r}
autoelementos <- eigen(cov_mat)
autoelementos
```

**IMPORTANTE** al tener las **variables estandarizadas**, la **suma de los autovalores** es $p$ (ya que ser√° la suma de las varianzas de las variables que tenemos).

---

# PCA en R: caso ¬´manual¬ª

**Cuarto paso**: **ordenar** autovectores segun autovalores (de mayor a menor)

```{r}
order_lambda <-
  order(autoelementos$values, decreasing = TRUE)
lambda <- autoelementos$values[order_lambda]
PC <- autoelementos$vectors[, order_lambda]
lambda # autovalores ordenadores
```

La **varianza capturada** por $\boldsymbol{\Phi}_1$ es $1.963$ y $0.037$ para la segunda componente $\boldsymbol{\Phi}_2$.

```{r}
PC # autovectores asociados --> direcciones principales
```

---

# PCA en R: caso ¬´manual¬ª


**Quinto paso**: calcular la **varianza explicada acumulada** por cada componente (una vez ordenadas)

```{r}
cumsum(lambda) / sum(lambda)
```

La **primera componente captura el 98.14% de la informaci√≥n (de la varianza)** y la segunda el 1.86% restante.

---

# PCA en R: caso ¬´manual¬ª

* **Sexto paso**: proyectar en las nuevas componentes para obtener las **nuevas coordenadas** (¬°respecto a la nueva base, a las nuevas componentes!)
 

```{r}
iris_pca <- iris_bi_std * t(PC)
names(iris_pca) <- c("PC_1", "PC_2")
iris_pca
```

---

# PCA en R: con prcomp

Dentro de los paquete b√°sicos cargados por `R` tenemos `prcomp` que nos permite realizar los c√°lculos anteriores de manera autom√°tica (`scale. = TRUE` debe ser indicado si los han datos no entran estandarizados previamente).

```{r}
pca <- prcomp(iris_bi, scale. = TRUE)
pca
```

* **Rotation**: la matriz cuyas columnas son las componentes principales $\boldsymbol{\Phi}_1, \boldsymbol{\Phi}_2$ (recuerda que dijimos que est√°bamos ¬´rotando¬ª los datos).

* **Standard deviations**: dado que cada $\lambda_j = {\rm Var} \left(\boldsymbol{\Phi}_j \right)$ representa la varianza de las componentes principales, lo que nos proporciona la salida es $\sqrt{\lambda_j}$, para cada $j=1,\ldots,p$

```{r}
pca$sdev^2 # autovalores
```

---

# PCA en R: con prcomp

```{r}
pca <- prcomp(iris_bi, scale. = TRUE)
pca
```

La **primera componente** viene definida como

$$\boldsymbol{\Phi}_1 = 0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$ 

La **segunda componente** viene definida como 

$$\boldsymbol{\Phi}_2 = -0.7071068 * Petal.Length^* +  0.7071068 * Petal.Width^*$$


---

# PCA en R: con prcomp

.pull-left[

En `pca$x` quedan guardados los **scores** o nuevas coordenadas de nuestros datos

```{r}
as_tibble(pca$x)
```

]

.pull-right[

Tambi√©n podemos calcularlas nosotros mismos **proyectando los datos en las nuevas componentes**

```{r}
as_tibble(as.matrix(iris_bi_std) %*%
            pca$rotation)
```

]

---

# Visualizando la transformaci√≥n

.pull-left[

```{r echo = FALSE}
ggplot(iris_bi_std, aes(x = Petal.Width, y = Petal.Length)) +
  geom_point(size = 5, color = "darkolivegreen") +
  labs(x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS ESTANDARIZADOS") +
  theme_minimal()
```

]

.pull-right[


```{r echo = FALSE}
ggplot(as_tibble(pca$x), aes(x = PC1, y = PC2)) +
  geom_point(size = 5, color = "pink") +
  labs(x = "PC 1", y = "PC 2",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS TRANSFORMADOS") +
  theme_minimal()
```

]

---

# Visualizando la transformaci√≥n


Si ahora pintamos los datos **codificando el color en funci√≥n de la especie** podemos darnos cuenta de por qu√© la primera componente es la que captura pr√°cticamente toda la informaci√≥n.

.pull-left[

```{r echo = FALSE}
ggplot(tibble(iris_bi_std, Species = iris$Species),
       aes(x = Petal.Width, y = Petal.Length,
           color = Species)) +
  geom_point(size = 5) +
  labs(color = "Especies",
       x = "Anchura p√©talo", y = "Longitud p√©talo",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS ESTANDARIZADOS") +
  theme_minimal()
```

]

.pull-right[


```{r echo = FALSE}
ggplot(tibble(as_tibble(pca$x), Species = iris$Species),
       aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 5) +
  labs(color = "Especies", x = "PC 1", y = "PC 2",
       caption = "Iris dataset extra√≠do de Fisher (1936) y Anderson (1935).",
       title = "DATOS TRANSFORMADOS") +
  theme_minimal()
```

]


---

# PCA en R: con factominer y factoextra

Ahora que controlamos un poco c√≥mo se calculan y qu√© significan, vamos a ampliar al dataset entero de iris `{iris}` con sus **4 variables num√©ricas**

```{r}
iris_full <- iris %>% select(-Species)

# Covarianza y correlaci√≥n sin estandarizar antes
library(corrr)
cov(iris_full)
iris_full %>% correlate(diagonal = 1) %>% fashion()
```

---

# PCA en R: con factominer y factoextra

.pull-left[

Las correlaciones tambi√©n podemos **visualizarlas** con el paquete `{corrplot}`

Las variables con mayor correlaci√≥n (positiva adem√°s) es entre la longitud y la anchura del p√©talo.

]

.pull-right[

```{r}
library(corrplot)
corrplot(cor(iris_full), type = "upper", tl.col = "black")
```
]

---


# PCA en R: con factominer y factoextra

Con `{FactoMineR}` podemos calcular con `PCA()` de forma muy sencilla, indic√°ndole que de momento no queremos gr√°ficos, que queremos tantas componentes como variables (luego ya decidiremos con cual nos quedamos) y que estandarice los datos (`scale.unit = TRUE`).

```{r}
library(FactoMineR)
library(factoextra)
pca_fit <-
  PCA(iris_full, scale.unit = TRUE,
      ncp = ncol(iris_full), graph = FALSE)
```

--

Para mostrar los autovalores basta con `pca_fit$eig` (ya nos los da ordenados y con la varianza explicada, tanto componente a componente como acumulada). Tambi√©n se obtienen con `get_eig(pca_fit)`

```{r}
pca_fit$eig # Alternativa: get_eig(pca_fit)
```

---

# PCA en R: con factominer y factoextra

En `pca_fit$svd$V` se guardan los **autovectores o componentes principales (de nuevo por columnas)** asociados a los autovalores que ya tenemos ordenados

```{r}
pca_fit$svd$V
```

Adem√°s con `pca_fit$var$contrib` nos muestra en **porcentaje lo que aporta cada variable** a la varianza explicada por cada componente (la suma de cada columna es el 100%)

```{r}
pca_fit$var$contrib
```

---

# PCA en R: con factominer y factoextra


As√≠ que dar√≠an expresadas las **nuevas componentes principales** en funci√≥n de las variables originales (estandarizadas$^*$)

```{r echo = FALSE}
knitr::kable(pca_fit$svd$V,
             col.names = c("Phi_1", "Phi_2",
                           "Phi_3", "Phi_4"),
             digits = 3)
```


$$\boldsymbol{\Phi}_1 = 0.521 * Sepal.Length^* - 0.269 * Sepal.Width^* + 0.580 * Petal.Length^*  + 0.565 * Petal.Width^*$$

$$\boldsymbol{\Phi}_2 = 0.377 * Sepal.Length^*  + 0.923 * Sepal.Width^*  + 0.024 * Petal.Length^*  + 0.067 * Petal.Width^*$$

$$\boldsymbol{\Phi}_3 = -0.719 * Sepal.Length^*  + 0.244 * Sepal.Width^*  + 0.142 * Petal.Length^*  + 0.634 * Petal.Width^*$$

$$\boldsymbol{\Phi}_4 = -0.261 * Sepal.Length^*  + 0.124 * Sepal.Width^*  + 0.801 * Petal.Length^*  - 0.524 * Petal.Width^*$$

---


# PCA en R: con factominer y factoextra

En `pca_fit$ind$coord` tenemos guardados los **scores**, las **nuevas coordenadas de los datos** (los **datos proyectados** en las nuevas direcciones).

```{r}
pca_scores <- as_tibble(pca_fit$ind$coord)
names(pca_scores) <- c("PC_1", "PC_2", "PC_3", "PC_4")
pca_scores # Nuevas coordenadas
```

---

# PCA en R: con factominer y factoextra


Podemos tambi√©n calcular las **covarianzas entre cada componente principal y las variables originales**, tal que ${\rm Cov}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \lambda_i z_{i,j}$, donde $z_{i,j}$ es el coeficiente $j$-√©simo de la componente $i$-√©sima (el peso de la variable $\boldsymbol{x}_j$ en la componente $\boldsymbol{\Phi}_i$)

La correlaci√≥n ser√° calculada como ${\rm Cor}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j ) = \frac{{\rm Cov}(\boldsymbol{\Phi}_i, \boldsymbol{x}_j )}{s_{\boldsymbol{\Phi}_i} s_{\boldsymbol{x}_j}} = \frac{\lambda_i z_{i,j}}{\sqrt{\lambda_i} s_{\boldsymbol{x}_j}} = \frac{\sqrt{\lambda_i}}{s_{x_j}} z_{i,j}$

Dichas correlaciones las tenemos guardadas en `pca_fit$var$cor` y representan las **coordenadas de cada variable en cada componente** 

```{r}
pca_fit$var$cor
```

---


# PCA en R: con factominer y factoextra

.pull-left[

En `pca_fit$var$cos2` tenemos las **correlaciones al cuadrado**, que expresan la **proporci√≥n de varianza de cada variable explicada por cada componente**

```{r}
round(pca_fit$var$cos2, 3)
```

Con `fviz_pca_var` podemos **visualizar de forma bidimensional** como se relacionan las variables originales con las dos componentes que mayor cantidad de varianza capturan.

La **primera componente captura** sobre todo las **dos variables del p√©talo** (dichas variables pr√°cticamente est√°n sobre la horizontal de la primera componente). La **segunda componente** captura ligeramente el s√©palo, aunque longitud del s√©palo es la peor variable representada de todas.

]

.pull-right[

```{r out.width = "93%"}
col <- c("#00AFBB", "#E7B800", "#FC4E07")
fviz_pca_var(pca_fit, col.var = "cos2",
             gradient.cols = col,
             repel = TRUE) +
  theme_minimal() + 
  labs(title = "Coordenadas de las variables",
       color = "Prop. var. explicada")
```
]



---


# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_cos2()` podemos mostrar el **porcentaje de la varianza de las variables que es explicada** por las componentes que le indiquemos en `axes`

As√≠ podemos apreciar que todas las 
**dos primeras componentes ya son capaces de capturar** al menos el 75% de la varianza de todas y cada una de las variables, rozando el 100% en las variables `Sepal.Width` y `Petal.Length`

]

.pull-right[

```{r out.width = "93%"}
fviz_cos2(pca_fit, choice = "var",
          axes = 1:2)
```
]

---

# PCA en R: con factominer y factoextra

.pull-left[

Con `fviz_eig()` podemos visualizar la varianza explicada por cada componente

```{r eval = FALSE}
fviz_eig(pca_fit,
         barfill = "darkolivegreen",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

]

.pull-right[

```{r echo = FALSE}
fviz_eig(pca_fit,
         barfill = "darkolivegreen",
         addlabels = TRUE) +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "Porcentaje de varianza explicada")
```

]

---

# PCA en R: con factominer y factoextra



.pull-left[

Tambi√©n podemos visualizar la **varianza acumulada** de forma manual

```{r eval = FALSE}
cumvar <- as_tibble(pca_fit$eig)
names(cumvar) <- c("lambda", "var", "cumvar")

ggplot(cumvar, aes(x = 1:4, y = cumvar)) +
  geom_col(fill = "pink") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

]

.pull-right[

```{r echo = FALSE}
cumvar <- as_tibble(pca_fit$eig)
names(cumvar) <- c("autovalor", "var", "cumvar")

ggplot(cumvar, aes(x = 1:4, y = cumvar)) +
  geom_col(fill = "pink") +
  geom_hline(yintercept = 90,
             linetype = "dashed") +
  theme_minimal() +
  labs(x = "Componente", 
       y = "% varianza explicada",
       title = "% varianza acumulada")
```

]

---

# PCA en R: con factominer y factoextra

.pull-left[

Por √∫ltimo `fviz_pca_biplot()` nos permite visualizar en las dos dimensiones que m√°s varianza capturan, e incluso nos permite **visualizar cl√∫sters** de observaciones con las elipses definidas por las matrices de covarianza de cada uno de los grupos.

```{r eval = FALSE}
fviz_pca_biplot(pca_fit,
                col.ind = iris$Species,
                palette = "jco",
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE,
                legend.title = "Especies")
```

Observamos de nuevo como la componente determinante es la primera, que nos discrimina perfectamente la especie de Setosa.

]

.pull-right[

```{r echo = FALSE}
fviz_pca_biplot(pca_fit, col.ind = iris$Species, palette = "jco",
                addEllipses = TRUE, label = "var", col.var = "black", repel = TRUE,
legend.title = "Especie")
```

]

---

hacerlo con tidymodels el iris y luego hacer ejemplos: notas, conjunto de tidymodels

---

class: inverse center middle

# BLOQUE II. An√°lisis cl√∫ster

&nbsp;


### [¬øQu√© es el an√°lisis cl√∫ster?](#intro-cluster)

### [M√©tricas](#metricas)

### [Algoritmos jer√°rquicos](#jerarquicos)

### [Algoritmos no jer√°rquicos](#no-jerarquicos)

### [Determinaci√≥n del n√∫mero de grupos](#n-grupos)


---

# Recursos y bibliograf√≠a

&nbsp;

### **Leyenda de los recursos**

&nbsp;

&nbsp;


#### üìö **Art√≠culos o libros** cient√≠ficos que han sido sometidos a revisi√≥n por pares.

&nbsp;

#### üîó **Recursos online** recomendados

&nbsp;

#### üíª Recursos para la **programaci√≥n en R**

---

# Bibliograf√≠a general

üíª **Tidy Data Tutor**: para visualizar la mec√°nica interna de `{tidyverse}`. <https://tidydatatutor.com/>

üîó Web con recursos para la **introducci√≥n a la estad√≠stica y Machine Learning en R** <https://artofstat.com/>

üìö **¬´An Introduction to Multivariate Statistical Analysis¬ª**. Anderson (1958) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/introduction_mva_anderson_2003.pdf>

üìö **¬´A New Measure of Rank Correlation¬ª**. Kendall (1938) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/correlation_kendall_1938.pdf>

üìö **¬´The generalised product moment distribution in samples from a normal multivariate population¬ª**. Wishart (1928) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/multivariate_normal_wishart_1928.pdf>

üìö **¬´On lines and planes of closest fit to systems of points in space¬ª**. Pearson (1901) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/fit_pearson_1901.pdf>


---

# Recursos dataviz

### Dataviz

üìö **¬´Gram√°tica de las gr√°ficas: pistas para mejorar las representaciones de datos¬ª**. Sevilla (2005) <http://academica-e.unavarra.es/bitstream/handle/2454/15785/Gram%C3%A1tica.pdf>

üìö **¬´Quantitative Graphics in Statistics: A Brief History¬ª**. Beniger and Robyn <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/graphics_beniger_robin_1978.pdf>
 
 
üíª **¬´Analizando datos, visualizando informaci√≥n, contando historias¬ª** (curso de dataviz en R). √Ålvarez-Li√©bana y Valverde-Castilla (2022) <https://dadosdelaplace.github.io/curso-dataviz-ECI-2022>

---

# Bibliograf√≠a componentes principales

üíª **Componentes principales** en `{tidymodels}`. <https://www.tmwr.org/dimensionality.html#beans>


üìö **¬´Principal Component Analysis¬ª**. Jolliffe (2002) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/pca_jolliffe_2002.pdf>

üìö **¬´Principal Component Analysis¬ª**. Herv√© and Lynne (2010) <http://staff.ustc.edu.cn/~zwp/teach/MVA/abdi-awPCA2010.pdf>

üìö **¬´Principal Component Analysis: a review and recent developments¬ª**. Jolliffe and Cadima (2016) <https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202>

üîó **¬´The Mathematics Behind Principal Component Analysis¬ª**. Dubey (2018).  <https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643>


üîó **¬´A One-Stop Shop for Principal Component Analysis¬ª**. Brems (2017). <https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c>

üìö **¬´On the number of principal components: a test of dimensionality based on measurements of similarity between matrices¬ª**. Dray (2008) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/numer_pca_dray_2008.pdf>


---

# Bibliograf√≠a an√°lisis cl√∫ster

Multiclass classification of dry beans using computer vision and machine learning techniques
https://www.sciencedirect.com/science/article/abs/pii/S0168169919311573

https://rpubs.com/Joaquin_AR/310338

https://www.tidymodels.org/learn/statistics/k-means/

https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/unsupervised-learning.html#kmeans-clustering

Algorithm AS 136: A K-Means Clustering Algorithm Author(s): J. A. Hartigan and M. A. Wong Reviewed work(s): Source: Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 28, No. 1 (1979), pp. 100-108 Published by: Wiley-Blackwell for the Royal Statistical Society Stable URL: http://www.jstor.org/stable/2346830 .
kmeans_hartigan_wong_1979

https://cimentadaj.github.io/ml_socsci/unsupervised-methods.html

---

# Recursos y bibliograf√≠a

### Otras t√©cnicas de reducci√≥n de la dimensi√≥n

üîó Sobre **PCA y PLS**. Amat (2017). <https://www.cienciadedatos.net/documentos/35_principal_component_analysis#Introducci%C3%B3n>

üìö **¬´On the early history of the singular value decomposition¬ª**. Stewart (1993) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/svd_stewart_1993.pdf>

üìö **¬´UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction¬ª**. McInnes, healy and Melville (2020) <https://github.com/dadosdelaplace/teaching/blob/main/bdba-pca-clustering-2022/biblio/umap_mcinnesetal_2020.pdf>


---


